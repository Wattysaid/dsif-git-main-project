{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELVTR Data Science Main Project\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "01 Git Repository\n",
    "\n",
    "Include all project code with a README file containing a high-level project description.\n",
    "\n",
    "Example README guide: [Make a README](link-to-readme-guide)\n",
    "\n",
    "Report\n",
    "\n",
    "* Methodology, approach, and model selection rationale.\n",
    "* Advantages and limitations of the chosen model.\n",
    "* Architecture of the final solution.\n",
    "* Considerations on deployment and scalability of the solution - i.e., how will the model be used in BAU by the business?\n",
    "* Estimated impact/ROI of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science in Finance: Lending Club Loan Analysis\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Lending Club has tasked us with preparing a loan application dataset for analysis and predictive modeling. \n",
    "\n",
    "The key tasks include data cleaning, exploratory data analysis, and building a predictive model for loan classification. An optional component involves building a real-time scoring application.\n",
    "\n",
    "**Project Objectives**:\n",
    "- Clean and preprocess the data.\n",
    "- Perform exploratory data analysis (EDA) to gain insights.\n",
    "- Develop a predictive model for loan application approval.\n",
    "- (Optional) Build a real-time scoring application.\n",
    "\n",
    "**Dataset Description**:\n",
    "The dataset consists of loan application records, including various financial metrics and the application status. The data dictionary is provided for understanding the attributes.\n",
    "\n",
    "**Dataset Path**:\n",
    "- CSV: `data/1-raw/lending-club-2007-2020Q3/Loan_status_2007-2020Q3-100ksample.csv`\n",
    "- Data Dictionary: `data/1-raw/lending-club-2007-2020Q3/LCDataDictionary.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\GitHub\\\\dsif-git-main-project\\\\elvtr_main_project\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data xlsx file as a dataframe\n",
    "df = pd.read_csv(\"c:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\GitHub\\\\dsif-git-main-project\\\\elvtr_main_project\\\\data\\\\raw\\\\Loan_status_2007-2020Q3\\\\Loan_status_2007-2020Q3-100k-Full-Data.csv\")\n",
    "\n",
    "# Clean headers in the existing DataFrame 'df'\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Display cleaned headers\n",
    "print(\"Cleaned headers:\", df.columns.tolist())\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data dictionary CSV file as a dataframe\n",
    "df_data_dict = pd.read_excel(\"c:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\GitHub\\\\dsif-git-main-project\\\\elvtr_main_project\\\\data\\\\raw\\\\Loan_status_2007-2020Q3\\\\LCDataDictionary.xlsx\")\n",
    "\n",
    "# Clean headers in the existing DataFrame 'df'\n",
    "df_data_dict.columns = df_data_dict.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Remove trailing whitespaces in all string columns of df_data_dict\n",
    "df_data_dict = df_data_dict.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Display cleaned headers\n",
    "print(\"Cleaned headers:\", df_data_dict.columns.tolist())\n",
    "\n",
    "df_data_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies the columns and descriptions from the data dictionary into a data frame for future recall.\n",
    "# Initialize empty lists for LoanStatNew and Description\n",
    "loanstatnew = []\n",
    "description = []\n",
    "\n",
    "# Iterate through each row in the DataFrame and populate lists\n",
    "for _, row in df_data_dict.iterrows():\n",
    "    loanstatnew.append(row['loanstatnew'])\n",
    "    description.append(row['description'])\n",
    "\n",
    "# Apply left-aligned styling to both headers and data cells\n",
    "styled_df_data_dict = df_data_dict.style.set_properties(\n",
    "    **{'text-align': 'left', 'white-space': 'nowrap'}\n",
    ").set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('text-align', 'left')]}]\n",
    ")\n",
    "\n",
    "# Display styled DataFrame\n",
    "styled_df_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing our data dictionary it is possible to class our columns into figurative categories to better organise our analysis. \n",
    "\n",
    "These categories are, for now:\n",
    "\n",
    "- Credit history, \n",
    "- Current Debt and Payment behaviours, \n",
    "- Employement, \n",
    "- Credit inquiries\n",
    "- Loan Application information\n",
    "- Hardship and Settlement Information\n",
    "- Co-Borrower Information\n",
    "- Loan Performance\n",
    "\n",
    "Let's create a table for reference. We'll add these manually so that we can tweak the data within each group as we discover more about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Basic Data Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the information within our data frame (df) looking at our initial feature set (pre_hardship_fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains 143 columns and 99999 rows of data. It is comprised of numerical (float, int) and categorical data (object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've ran into an error (KeyError: \"['desc', 'member_id', 'verified_status_joint'] not in index\"), for the time being we'll remove the missing fields from df_data_dict. These are most likely naming issues i.e. member_id is most likely id but considering we won't be using this data for now we can remove it and correct later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract column headers from df and the first column of df_data_dict\n",
    "df_columns = set(df.columns)\n",
    "df_data_dict_columns = set(df_data_dict.iloc[:, 0])  # First column of df_data_dict\n",
    "\n",
    "# Find columns in df that are missing in df_data_dict\n",
    "missing_in_data_dict = df_columns - df_data_dict_columns\n",
    "\n",
    "# Find columns in df_data_dict that are missing in df\n",
    "missing_in_df = df_data_dict_columns - df_columns\n",
    "\n",
    "# Output the results\n",
    "print(\"Columns in df that are missing in df_data_dict:\")\n",
    "print(missing_in_data_dict)\n",
    "\n",
    "print(\"\\nColumns in df_data_dict that are missing in df:\")\n",
    "print(missing_in_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the values that are missing in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list was created to run analysis later on. I've opted to select the pre_hardship_fields as my feature selection draft\n",
    "# Subjective groupings created for post ML analysis. Due to the transformation these lists can only be used with df\n",
    "groups = {\n",
    "    \"Credit History\": [\n",
    "        'earliest_cr_line', 'fico_range_high', 'fico_range_low', 'last_fico_range_high',\n",
    "        'last_fico_range_low', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'num_accts_ever_120_pd',\n",
    "        'num_tl_120dpd_2m', 'pub_rec', 'pub_rec_bankruptcies'\n",
    "    ],\n",
    "    \"Current Debt and Payment Behaviors\": [\n",
    "        'acc_now_delinq', 'all_util', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths',\n",
    "        'collections_12_mths_ex_med', 'delinq_2yrs', 'delinq_amnt', 'max_bal_bc', 'mths_since_last_delinq',\n",
    "        'num_rev_accts', 'num_rev_tl_bal_gt_0', 'percent_bc_gt_75', 'revol_bal', 'revol_util',\n",
    "        'tot_coll_amt', 'tot_cur_bal', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit'\n",
    "    ],\n",
    "    \"Employment\": [\n",
    "        'emp_length', 'emp_title', 'annual_inc', 'annual_inc_joint'\n",
    "    ],\n",
    "    \"Credit Inquiries\": [\n",
    "        'inq_fi', 'inq_last_12m', 'inq_last_6mths', 'num_tl_op_past_12m'\n",
    "    ],\n",
    "    \"Loan Application Information\": [\n",
    "        'loan_amnt', 'term', 'int_rate', 'application_type', 'grade', 'sub_grade', 'purpose',\n",
    "        'issue_d', 'home_ownership', 'zip_code', 'addr_state', 'title', 'desc', 'url'\n",
    "    ],\n",
    "    \"Hardship and Settlement Information\": [\n",
    "        'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'hardship_start_date',\n",
    "        'hardship_end_date', 'hardship_amount', 'hardship_length', 'settlement_status', 'settlement_date',\n",
    "        'settlement_amount', 'settlement_percentage', 'settlement_term'\n",
    "    ],\n",
    "    \"Co-Borrower Information\": [\n",
    "        'annual_inc_joint', 'dti_joint', 'verified_status_joint', 'revol_bal_joint',\n",
    "        'sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_earliest_cr_line',\n",
    "        'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_revol_util'\n",
    "    ],\n",
    "    \"Loan Performance\": [\n",
    "        'funded_amnt', 'funded_amnt_inv', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
    "        'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee', 'total_rec_prncp', 'recoveries',\n",
    "        'collection_recovery_fee', 'last_pymnt_amnt', 'last_pymnt_d', 'next_pymnt_d'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define pre and post hardship fields\n",
    "pre_hardship_fields = [\n",
    "    'acc_now_delinq', 'acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc', \n",
    "    'annual_inc_joint', 'application_type', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', \n",
    "    'chargeoff_within_12_mths', 'collections_12_mths_ex_med', 'delinq_2yrs', 'delinq_amnt', \n",
    "    'dti', 'dti_joint', 'earliest_cr_line', 'emp_length', 'emp_title', \n",
    "    'fico_range_high', 'fico_range_low', 'funded_amnt', 'funded_amnt_inv', 'grade', \n",
    "    'home_ownership', 'il_util', 'initial_list_status', 'inq_fi', 'inq_last_12m', \n",
    "    'inq_last_6mths', 'installment', 'int_rate', 'issue_d', 'loan_amnt', 'loan_status', \n",
    "    'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', \n",
    "    'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_last_delinq', \n",
    "    'mths_since_last_major_derog', 'mths_since_last_record', 'mths_since_rcnt_il', \n",
    "    'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq', \n",
    "    'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', \n",
    "    'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', \n",
    "    'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', \n",
    "    'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'open_acc', 'open_acc_6m', 'open_il_12m', \n",
    "    'open_il_24m', 'open_act_il', 'open_rv_12m', 'open_rv_24m', 'out_prncp', \n",
    "    'out_prncp_inv', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'policy_code', 'pub_rec', \n",
    "    'pub_rec_bankruptcies', 'purpose', 'pymnt_plan', 'revol_bal', 'revol_util', \n",
    "    'sub_grade', 'tax_liens', 'term', 'title', 'tot_coll_amt', 'tot_cur_bal', \n",
    "    'tot_hi_cred_lim', 'total_acc', 'total_bal_ex_mort', 'total_bal_il', 'total_bc_limit', \n",
    "    'total_cu_tl', 'total_il_high_credit_limit', 'total_pymnt', 'total_pymnt_inv', \n",
    "    'total_rec_int', 'total_rec_late_fee', 'total_rec_prncp', 'total_rev_hi_lim', \n",
    "    'verification_status', 'zip_code'\n",
    "]\n",
    "\n",
    "post_hardship_fields = [\n",
    "    'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', \n",
    "    'hardship_start_date', 'hardship_end_date', 'hardship_amount', 'hardship_length', \n",
    "    'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', \n",
    "    'hardship_payoff_balance_amount', 'hardship_last_payment_amount', 'disbursement_method', \n",
    "    'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date', \n",
    "    'settlement_amount', 'settlement_percentage', 'settlement_term'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pre_hardship_fields to only include values that exist as columns in df\n",
    "pre_hardship_fields_clean_kn = [field for field in pre_hardship_fields if field in df.columns]\n",
    "\n",
    "# Display the filtered list\n",
    "print(\"Filtered pre_hardship_fields:\", pre_hardship_fields_clean_kn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[pre_hardship_fields_clean_kn].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[pre_hardship_fields_clean_kn].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to collect cross-tab data\n",
    "cross_tab_data = []\n",
    "\n",
    "# Iterate over each group and each field to determine availability\n",
    "for group, fields in groups.items():\n",
    "    for field in fields:\n",
    "        pre = 'Yes' if field in pre_hardship_fields else 'No'\n",
    "        post = 'Yes' if field in post_hardship_fields else 'No'\n",
    "        cross_tab_data.append([group, field, pre, post])\n",
    "\n",
    "# Create DataFrame for cross-tab\n",
    "cross_tab_df = pd.DataFrame(cross_tab_data, columns=[\"Group\", \"Field\", \"Pre-Hardship\", \"Post-Hardship\"])\n",
    "\n",
    "# Apply styling to left-align specific columns and set table header alignment\n",
    "cross_tab_df_styled = cross_tab_df.style.set_properties(\n",
    "    subset=['Group', 'Field'],\n",
    "    **{'text-align': 'left'}\n",
    ").set_table_styles(\n",
    "    [{'selector': 'th.col_heading.level0', 'props': [('text-align', 'left')]}]\n",
    ")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "cross_tab_df_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because are scoring is supposed to identify good and bad payers we'll leverage Pre-Hardship feature list for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the employtment mapping CSV file as a dataframe\n",
    "df_emp_title = pd.read_csv(\"c:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\GitHub\\\\dsif-git-main-project\\\\elvtr_main_project\\\\data\\\\raw\\\\emp_title_mapping.csv\")\n",
    "\n",
    "# Clean headers in the existing DataFrame\n",
    "df_emp_title.columns = df_emp_title.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Display cleaned headers\n",
    "print(\"Cleaned headers:\", df_emp_title.columns.tolist())\n",
    "\n",
    "df_emp_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for LoanStatNew and Description\n",
    "jobtitle = []\n",
    "job_category = []\n",
    "\n",
    "# Iterate through each row in the DataFrame and populate lists\n",
    "for _, row in df_emp_title.iterrows():\n",
    "    jobtitle.append(row['job_title'])\n",
    "    job_category.append(row['category'])\n",
    "\n",
    "# Apply left-aligned styling to both headers and data cells\n",
    "styled_df_emp_title = df_emp_title.style.set_properties(\n",
    "    **{'text-align': 'left', 'white-space': 'nowrap'}\n",
    ").set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('text-align', 'left')]}]\n",
    ")\n",
    "\n",
    "# Display styled DataFrame\n",
    "styled_df_emp_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross table on 'category' in df_emp_title\n",
    "category_crosstab = pd.crosstab(index=df_emp_title['category'], columns='count').sort_values(by='count', ascending=False)\n",
    "\n",
    "# Display the crosstab\n",
    "category_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our libraries and configure any paramaters for charting later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for data manipulation, statistics, and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest, shapiro, anderson, kstest, skew\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt  # For standard plotting\n",
    "import seaborn as sns  # For static plots with themes\n",
    "import plotly.express as px  # For interactive plots\n",
    "import missingno as msno  # For missing data visualization\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import train_test_split, cross_val_score  # Data splitting and cross-validation\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor\n",
    ")\n",
    "from sklearn.svm import SVC, SVR  # Support Vector Machines for classification and regression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB  # Naive Bayes Classifier\n",
    "from sklearn.cluster import KMeans  # K-Means clustering\n",
    "from sklearn.decomposition import PCA  # Dimensionality reduction\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.feature_selection import RFE  # Recursive Feature Elimination\n",
    "\n",
    "# Additional machine learning models\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "\n",
    "# Utility libraries\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# Pandas display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Plot settings for consistent figure size (A4 landscape top half)\n",
    "FIG_WIDTH = 11.69  # Width\n",
    "FIG_HEIGHT = 4.14  # Height\n",
    "\n",
    "# Set the theme for Seaborn plots\n",
    "sns.set_theme(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Overview\n",
    "\n",
    "Before performing any analysis, we will explore the structure of the dataset to understand the nature of the available data. This includes checking the number of rows and columns, the data types of each feature, and identifying any missing values. Understanding these characteristics is essential for guiding data cleaning and feature engineering steps later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[pre_hardship_fields].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data frame contains 100k rows, 104 columns.\n",
    "\n",
    "Our Data set contains int, float, and string objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some basic statistics against all int and float data in our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistics only for int and float columns\n",
    "df[pre_hardship_fields].describe(include=['int', 'float'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ignore the id columns, and will drop these as part of our basic data cleaning.\n",
    "\n",
    "### High level analysis\n",
    "\n",
    "Scrolling from left to right we can make the following observations:\n",
    "\n",
    "- The Standard deviation for `annual_inc` is > 87k suggesting large disparity in numbers (we'll check this later on when pulling our distribution plots)\n",
    "- The average `open_acc` is equal to 11 with a high of up to 86. For UK standards this can be considered extremely high. Worth taking this into account as a feature for our deeper analysis.\n",
    "- Our delinquency fields show that we have a low average in the `delinq_2yrs` column and an average of 35 months since the last delinquency (`mths_since_last_delinq`) these could be a great indicators. \n",
    "\n",
    "Althought there are more lets continue our analysis and feature selection for our machine learning excercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "### Checking for Missing Values\n",
    "To ensure data integrity, we check for missing values in the dataset. The `isnull()` function is used to identify null entries, and the results are sorted by the number of missing values per column. This provides insight into the columns with the most missing data, which could impact our analysis and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = df[pre_hardship_fields].isnull().sum().sort_values(ascending=False)\n",
    "print(f\"There is a total of: {len(missing_values)} columns that are missing data\\n\")\n",
    "# print(\"\\nMissing values in each column:\\n\") \n",
    "# print(missing_values[missing_values > 90000]) # Display only columns with missing values\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data Strategy\n",
    "Instead of dropping columns with a high number of missing values, we may want to retain them for our analysis. Following the logic of Abraham Wald's famous airplane and bullet holes approach, it could be beneficial to analyse the data we don't have rather than discard potentially useful columns. \n",
    "\n",
    "This is especially relevant for improving loan default predictions as the absence of data in itself is indicative of the possible risk to default on loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirming DataFrame Shape\n",
    "\n",
    "After exploring the missing values, we validate the shape of our dataframe to ensure that the dataset remains unchanged. \n",
    "\n",
    "This step helps confirm that we are still working with the full set of features and rows taking into account we've already stripped the first 3 columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the full DataFrame\n",
    "print(\"Shape of the full DataFrame df:\", df.shape)\n",
    "\n",
    "# Print the shape of the DataFrame subset with pre_hardship_fields columns\n",
    "print(\"Shape of df with only pre_hardship_fields columns:\", df[pre_hardship_fields].shape)\n",
    "\n",
    "# Print the number of columns in pre_hardship_fields\n",
    "print(\"Number of columns in pre_hardship_fields:\", len(pre_hardship_fields))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the `missingno` library to analyze the missing data. This will help us understand how missing data is distributed across the dataset and the correlation between missing values in different columns.\n",
    "\n",
    "The `missing_values` variable has been defined earlier in our workflow to quantify the total number of missing entries in each column. \n",
    "\n",
    "Now, we'll leverage this list to visualize the missing data using  missingno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for columns with missing values greater than 0\n",
    "missing_values_graph = df[missing_values[missing_values > 0].index]\n",
    "\n",
    "print(\"Categorical Data Missing Values\\n\")\n",
    "\n",
    "# Visualize the missing data using the missingno library\n",
    "msno.matrix(missing_values_graph)\n",
    "msno.bar(missing_values_graph)\n",
    "msno.heatmap(missing_values_graph)\n",
    "# msno.dendrogram(missing_values_graph) #removed for the final anlysis to avoid cluttering the document with the same data but a different way to show it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of our missing data findings\n",
    "\n",
    "#### 1. Missing Data Matrix Plot:\n",
    "- The matrix plot visualizes the distribution of missing data across the dataset columns.\n",
    "- Some columns have no missing data, but a few are significantly affected.\n",
    "- Several columns have a substantial percentage of missing data, some exceeding 75%.\n",
    "- Key columns with high missing data include:\n",
    "  - `hardship_*` related fields, `payment_plan_start_date`, `sec_app_*` fields, etc.\n",
    "\n",
    "#### 2. Missing Data Heatmap:\n",
    "- The heatmap shows correlations between columns with missing data.\n",
    "- Higher intensity colors indicate stronger correlations.\n",
    "- Examples include `emp_title`, `mths_since_last_major_derog`, and `revol_util` showing linked missingness.\n",
    "- Strong correlations exist between certain groups of columns, suggesting shared patterns in their missing values.\n",
    "- We can clearly distinguish 4 groups. The three largest relating to `hardship_`, `sec_`, and `Acc` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Missing Value Indicators\n",
    "\n",
    "Opposed to solomly removing values and using collected data to predict loan defaults I will create indicator variables that flag whether a value was missing for a given feature. \n",
    "\n",
    "This allows us to retain missing values while also capturing information about whether a data point was reported or not, which could enhance our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new list by selecting specific groups from the logical mapping we created earlier\n",
    "selected_groups = [\"Credit History\", \"Employment\", \"Credit Inquiries\"] # created after reading the data dictionary\n",
    "missing_value_indicator = sum([groups[group] for group in selected_groups], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(missing_value_indicator)) # collection of columns from our grouping whilst reading the data dictionary\n",
    "print(len(pre_hardship_fields)) # the columns that are most likely pre any loan defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our draft list of features (`draft_features`) now includes 122 columns identified during the analysis of the missing data, and the basic statistics from `df` along with the information within the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of initial features based on our previous findings and the descriptions within the data dictionary\n",
    "new_features = []\n",
    "new_features.extend(pre_hardship_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expand `df` with new columns to keep a record of the missingness values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store the new column names\n",
    "new_missing_columns = []\n",
    "\n",
    "# Iterating through the list to create missing value indicator columns\n",
    "for col in missing_value_indicator:\n",
    "    indicator_col_name = f\"{col}_missing_clean_kn\"  # Create a new column name for the missing indicator\n",
    "    df[indicator_col_name] = df[col].isnull().astype(int)  # 1 for missing, 0 for not missing\n",
    "    \n",
    "    # Append the new column name to the list\n",
    "    new_missing_columns.append(indicator_col_name)\n",
    "\n",
    "# Display the list of new column names\n",
    "print(\"New missing indicator columns:\", new_missing_columns)\n",
    "print(\"Count of New Missing Columns created\", len(new_missing_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to verify the new columns\n",
    "df[new_missing_columns].head()  # Display the first few rows to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new items to the existing list\n",
    "new_features.extend(new_missing_columns)\n",
    "len(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for uniqueness in our feature list.\n",
    "\n",
    "Here we're looking for variability i.e. which features contain the most variability. The below code will provide us with a list of unique value counts for each feature within our data set, ordered in decending order, and with a % that reflects the uniqueness i.e. if we have 100k unique values (id field) then the % uniqueness will be 100%. This little variability will not add value in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_uniqueness(df):\n",
    "    \"\"\"\n",
    "    Analyzes the number of unique values in all columns within a DataFrame, \n",
    "    regardless of their data type, and helps identify suitable candidates for visualization.\n",
    "\n",
    "    Args:\n",
    "        df: The Pandas DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame with columns, unique value counts, and percentages.\n",
    "        Also, displays a bar graph for the unique counts sorted from highest to lowest.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Calculate unique counts and percentages and store them in a DataFrame\n",
    "    uniqueness_df = pd.DataFrame({\n",
    "        \"Unique Count\": df.nunique(),\n",
    "        \"Unique Percentage\": df.nunique() / total_rows * 100\n",
    "    }).sort_values(by=\"Unique Count\", ascending=False)\n",
    "\n",
    "    # Print sorted output with unique counts and percentages\n",
    "    print(uniqueness_df)\n",
    "\n",
    "    # Create a bar graph to visualize the number of unique values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(uniqueness_df.index, uniqueness_df[\"Unique Count\"])\n",
    "    plt.xlabel(\"Columns\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Unique Values\", fontsize=14)\n",
    "    plt.title(\"Unique Value Counts per Column (Sorted)\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)  # Rotate x-axis labels for readability\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return uniqueness_df\n",
    "\n",
    "# Example usage with pre-selected columns\n",
    "# Assuming df and pre_hardship_fields are defined and valid\n",
    "#try:\n",
    "#    unique_counts_df = analyze_uniqueness(df[pre_hardship_fields])\n",
    "#except ValueError as e:\n",
    "#    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    unique_counts_df = analyze_uniqueness(df[new_features])\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see columns with 100% and 0% variability i.e. every row is unique or contains the same number. This is typical of unique identifiers and in our case policy codes. We can remove these as they won't be of any use to us for further analysis.\n",
    "\n",
    "To simplify our data we'll remove 100%, 0%, and anythin less than 5%:\n",
    "\n",
    "- 100% of values are unique = `id`, `url`\n",
    "- 0% of values are unique = `policy_code`, `pymnt_plan`\n",
    "\n",
    "We'll also take this opportunity to remove `unnamed:_0.1`, `unnamed:_0` too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['policy_code', 'pymnt_plan']\n",
    "\n",
    "# Remove specified columns from pre_hardship_fields_clean\n",
    "new_features = [col for col in new_features if col not in columns_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature list has been updated with the new columns. This means that for each column with missing values, a new binary indicator column is created, where `1` represents a missing value and `0` indicates that a value was present. \n",
    "\n",
    "This approach ensures that we retain as much information as possible from the original dataset while also capturing the fact that missing values themselves may provide valuable insight. For instance, missing income information could be an indicator of a higher default risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining our target variable and applying simplify the `loan_status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logical groupings for 'loan_status'\n",
    "loan_status_groupings = {\n",
    "    'Fully Paid': 'Paid Loan',\n",
    "    'Current': 'Active Loan',\n",
    "    'Charged Off': 'Defaulted Loan',\n",
    "    'Late (31-120 days)': 'Late Loan',\n",
    "    'In Grace Period': 'Late Loan',\n",
    "    'Late (16-30 days)': 'Late Loan',\n",
    "    'Does not meet the credit policy. Status:Fully Paid': 'Paid Loan',\n",
    "    'Issued': 'Active Loan',\n",
    "    'Does not meet the credit policy. Status:Charged Off': 'Defaulted Loan',\n",
    "    'Default': 'Defaulted Loan'\n",
    "}\n",
    "\n",
    "# Apply the grouping to the 'loan_status' column\n",
    "df['loan_status_grouped_kn'] = df['loan_status'].replace(loan_status_groupings)\n",
    "\n",
    "# Verify the groupings\n",
    "print(df['loan_status_grouped_kn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['loan_status_grouped_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dependant variable\n",
    "\n",
    "Our dependant veriable is `loan_status_grouped_kn` further work is to understand how best to predict good and bad loan applicants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status_grouped_kn'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = ['loan_status_grouped_kn']\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"Both columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df[new_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our data frame to see what we're working with. Note our most recent data frame is now df_dropped and no longer df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) on Missing Values\n",
    "\n",
    "To understand the impact of missing values on our target variable (`loan_status`), we perform an exploratory analysis. \n",
    "\n",
    "This compares the distribution of loan status between rows where key variables are missing and where they are not. By doing this, we hope to detect which missing data is associated with loan outcomes. \n",
    "\n",
    "This helps us understand how big a influence on the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that have missing values in df_dropped\n",
    "missing_cols = df_dropped.columns[df_dropped.isnull().any()]\n",
    "\n",
    "# Store results for plotting\n",
    "missing_dict = {}\n",
    "not_missing_dict = {}\n",
    "\n",
    "# Function to collect percentages for missing and non-missing data\n",
    "def missing_value_analysis(column, target_column='loan_status_grouped_kn'):\n",
    "    missing = df_dropped[df_dropped[column].isnull()][target_column].value_counts(normalize=True) * 100\n",
    "    not_missing = df_dropped[df_dropped[column].notnull()][target_column].value_counts(normalize=True) * 100\n",
    "    missing_dict[column] = missing\n",
    "    not_missing_dict[column] = not_missing\n",
    "\n",
    "# Apply the function for all columns with missing data\n",
    "for col in missing_cols:\n",
    "    missing_value_analysis(col)\n",
    "\n",
    "# Create DataFrames for heatmaps\n",
    "missing_df = pd.DataFrame(missing_dict).fillna(0)  # Fill NaN with 0 to ensure proper heatmap display\n",
    "not_missing_df = pd.DataFrame(not_missing_dict).fillna(0)\n",
    "\n",
    "# Plotting heatmaps one below the other\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 16), gridspec_kw={'height_ratios': [1, 1]})  # Adjust aspect ratio\n",
    "\n",
    "# Heatmap for missing data\n",
    "sns.heatmap(missing_df, annot=False, cmap=\"Blues\", ax=ax[0], cbar_kws={\"shrink\": .75})\n",
    "ax[0].set_title('Percentage of Loan Status for Missing Data')\n",
    "ax[0].tick_params(axis='x', rotation=90, labelsize=10)  # Rotate x-axis labels for readability\n",
    "ax[0].tick_params(axis='y', labelsize=10)  # Adjust y-axis label size\n",
    "\n",
    "# Heatmap for non-missing data\n",
    "sns.heatmap(not_missing_df, annot=False, cmap=\"Greens\", ax=ax[1], cbar_kws={\"shrink\": .75})\n",
    "ax[1].set_title('Percentage of Loan Status for Non-Missing Data')\n",
    "ax[1].tick_params(axis='x', rotation=90, labelsize=10)  # Rotate x-axis labels for readability\n",
    "ax[1].tick_params(axis='y', labelsize=10)  # Adjust y-axis label size\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Missing and Non-Missing Data by Loan Status\n",
    "\n",
    "#### 1. Percentage of Missing Data (First Image)\n",
    "- **Paid Loan**: Shows a high percentage of missing data across numerous columns, particularly towards the end of the list (indicated by darker shades of blue).\n",
    "- **Active Loan** and **Late Loan**: Generally have less missing data, though some specific columns still contain notable missing values.\n",
    "- **Defaulted Loan**: Contains fewer missing values compared to `Paid Loan`, but some columns still show missing data.\n",
    "\n",
    "#### 2. Percentage of Non-Missing Data (Second Image)\n",
    "- **Paid Loan**: Displays lower percentages of non-missing data across various columns, aligning with the missing data heatmap (darker greens represent more missing data).\n",
    "- **Active Loan** and **Late Loan**: Feature higher percentages of non-missing data in most columns, shown by lighter green shades, suggesting more complete data.\n",
    "- **Defaulted Loan**: Generally has a higher percentage of non-missing data than `Paid Loan`, but less than `Active Loan` and `Late Loan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Cleaning\n",
    "\n",
    "Perform thorough data cleaning on the provided dataset, including but not limited to the following steps:\n",
    "\n",
    "* Handling missing values (imputation or removal)\n",
    "* Converting data types to appropriate formats\n",
    "* Removing duplicate records\n",
    "* Detecting and handling outliers\n",
    "* Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross table of data types in df\n",
    "dtype_crosstab = df_dropped.dtypes.value_counts().reset_index()\n",
    "dtype_crosstab.columns = ['Data Type', 'Count']\n",
    "\n",
    "# Display the cross table\n",
    "dtype_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct references to 'completed' to 'complete' in the 'hardship_status' column\n",
    "df_dropped['hardship_status_kn'] = df['hardship_status'].replace('COMPLETED', 'COMPLETE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['hardship_status_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Feature Selection\n",
    "\n",
    "Based on our initial quick analysis of the data and the data dictionary, we'll now work on the shortlisted features for further exploration in our machine learning model. These features were selected based on their relevant at the time of taking out a loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and list columns with 'object' data types from our selected features\n",
    "# This helps us identify which columns require encoding or conversions before modeling\n",
    "df_dropped[new_features].select_dtypes(include=['object']).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[new_features].select_dtypes(include=['object']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List split\n",
    "\n",
    "Let's take our list of features we'd like to review for machine learning and split them into 3 seperate lists (boolean, numerical, and categorical) for further analysis.\n",
    "\n",
    "In this next phase I want to explore the feature selection for multicollinearity and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_frame(features_list, df):\n",
    "    \"\"\"\n",
    "    Splits the provided DataFrame into three lists containing Boolean, Numerical, and Categorical column names.\n",
    "    Converts floats with trailing zeros into integers and replaces NaN values with 0 for integers, 0.00 for floats.\n",
    "\n",
    "    Parameters:\n",
    "    features_list (list): List of column names to be checked.\n",
    "    df (pd.DataFrame): The input DataFrame to split.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing three lists (boolean_cols, numerical_cols, categorical_cols).\n",
    "    \"\"\"\n",
    "    boolean_cols = []\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "\n",
    "    # Define acceptable boolean values\n",
    "    acceptable_boolean_values = {0, 1, True, False, 0.0, 1.0}\n",
    "\n",
    "    for col in features_list:\n",
    "        # Treat each column explicitly as a Series\n",
    "        column_series = df[col]\n",
    "\n",
    "        # Handle cases where columns might be interpreted incorrectly\n",
    "        if pd.api.types.is_bool_dtype(column_series) or all(column_series.dropna().isin(acceptable_boolean_values)):\n",
    "            boolean_cols.append(col)\n",
    "        elif pd.api.types.is_numeric_dtype(column_series):\n",
    "            # Check for floats with trailing zeros\n",
    "            if column_series.dtype == 'float64':\n",
    "                # Check if all float values are equivalent to integers\n",
    "                if all(column_series.dropna() == column_series.dropna().astype(int)):\n",
    "                    df[col] = column_series.fillna(0).astype(int)  # Replace NaNs with 0 and convert to int\n",
    "                else:\n",
    "                    df[col] = column_series.fillna(0.00)  # Replace NaNs with 0.00 for floats\n",
    "                numerical_cols.append(col)\n",
    "            else:\n",
    "                df[col] = column_series.fillna(0)  # Replace NaNs with 0 for integers\n",
    "                numerical_cols.append(col)\n",
    "        else:\n",
    "            categorical_cols.append(col)\n",
    "    \n",
    "    # Print a summary of the count of columns in each list\n",
    "    print(f\"Summary of column counts:\")\n",
    "    print(f\"boolean_list contains {len(boolean_cols)} values\")\n",
    "    print(f\"numerical_list contains {len(numerical_cols)} values\")\n",
    "    print(f\"categorical_list contains {len(categorical_cols)} values\")\n",
    "    print(f\"The feature list we'll be working with contains {df_dropped[new_features].shape} rows and columns.\")\n",
    "\n",
    "    return boolean_cols, numerical_cols, categorical_cols\n",
    "\n",
    "# Instructions:\n",
    "# this calls the split_data_frame function above create three lists to capture the sorting outputs in. \n",
    "# These will later be used to pull some graphs to evaluate the data and what possible transformations we've missed.\n",
    "# boolean_list, numerical_list, categorical_list = split_data_frame(new_features, df_dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns are in df_dropped\n",
    "missing_columns = [col for col in new_features if col not in df_dropped.columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this calls the split_data_frame function above create three lists to capture the sorting outputs in. \n",
    "# These will later be used to pull some graphs to evaluate the data and what possible transformations we've missed.\n",
    "# Our function parameters are list and the latest version of our data frame in this caes new_features and df_dropped accordingly.\n",
    "boolean_list, numerical_list, categorical_list = split_data_frame(new_features, df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[categorical_list].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[numerical_list].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[boolean_list].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections are the code snippets we'll use to analyse our data (Numerical, Boolean, Categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_numeric_columns(numeric_cols, dataframe):\n",
    "    \"\"\"\n",
    "    Analyze and visualize numeric columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    numeric_cols (list): List of numeric column names to analyze.\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    for column in numeric_cols:\n",
    "        print(f\"\\nSummary Statistics and Analysis for Numeric Column: {column}\")\n",
    "\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if column not in dataframe.columns:\n",
    "            print(f\" '{column}' is not found in the DataFrame. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Ensure the column is converted to numeric\n",
    "        dataframe.loc[:, column] = pd.to_numeric(dataframe[column], errors='coerce')\n",
    "\n",
    "        # Drop NaN values to ensure we have numeric data for analysis\n",
    "        numeric_data = dataframe[column].dropna()\n",
    "\n",
    "        if numeric_data.empty:\n",
    "            print(f\"No valid numeric data available for column: {column}. Skipping...\")\n",
    "            continue  # Skip the column if there's no valid data\n",
    "\n",
    "        # Calculate z-scores\n",
    "        z_scores = (numeric_data - numeric_data.mean()) / numeric_data.std()\n",
    "\n",
    "        # Calculate IQR\n",
    "        Q1 = numeric_data.quantile(0.25)\n",
    "        Q3 = numeric_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Identify outliers based on z-scores (z > 3 or z < -3)\n",
    "        outliers_z = numeric_data[(z_scores > 3) | (z_scores < -3)]\n",
    "\n",
    "        # Identify outliers based on IQR\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers_iqr = numeric_data[(numeric_data < lower_bound) | (numeric_data > upper_bound)]\n",
    "\n",
    "        # Create a figure for the distribution and box plot\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "\n",
    "        # Distribution plot (histogram)\n",
    "        sns.histplot(numeric_data, kde=True, bins=10, ax=axs[0])\n",
    "        axs[0].set_title(f'Distribution of {column}')\n",
    "        axs[0].set_xlabel(column)\n",
    "        axs[0].set_ylabel('Frequency')\n",
    "\n",
    "        # Box plot for outlier detection\n",
    "        sns.boxplot(x=numeric_data, ax=axs[1])\n",
    "        axs[1].set_title(f'Box Plot for {column} (Outlier Detection)')\n",
    "        axs[1].set_xlabel(column)\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Summary Statistics\n",
    "        print(f\"\\nSummary Statistics for Numeric  '{column}':\")\n",
    "        print(numeric_data.describe())\n",
    "\n",
    "        # Interquartile Range (IQR)\n",
    "        print(f\"\\nInterquartile Range (IQR): {IQR:.4f}\")\n",
    "        print(f\"Lower Bound for Outliers (IQR method): {lower_bound:.4f}\")\n",
    "        print(f\"Upper Bound for Outliers (IQR method): {upper_bound:.4f}\")\n",
    "        print(f\"Number of Outliers (IQR method): {len(outliers_iqr)}\")\n",
    "\n",
    "        # Display outliers based on IQR\n",
    "        #if not outliers_iqr.empty:\n",
    "        #    print(\"\\nOutliers detected using the IQR method:\")\n",
    "        #    print(outliers_iqr)\n",
    "        #else:\n",
    "        #    print(\"\\nNo outliers detected using the IQR method.\")\n",
    "\n",
    "        # Z-scores\n",
    "        print(\"\\nZ-score Summary:\")\n",
    "        print(z_scores.describe())\n",
    "\n",
    "        print(f\"\\nNumber of Outliers (Z-score method): {len(outliers_z)}\")\n",
    "\n",
    "        # Display outliers based on Z-scores\n",
    "        #if not outliers_z.empty:\n",
    "        #    print(\"\\nOutliers detected using the Z-score method:\")\n",
    "        #    print(outliers_z)\n",
    "        #else:\n",
    "        #    print(\"\\nNo outliers detected using the Z-score method.\")\n",
    "\n",
    "        # Skewness\n",
    "        skewness_value = skew(numeric_data)\n",
    "        print(f\"\\nSkewness: {skewness_value:.4f}\")\n",
    "\n",
    "        # Normality Tests\n",
    "        print(\"\\nNormality Tests:\")\n",
    "        # D'Agostino's K^2 Test\n",
    "        k2_stat, k2_p = normaltest(numeric_data)\n",
    "        print(f\"D'Agostino's K^2 Test: Statistic={k2_stat:.4f}, p-value={k2_p:.4f}\")\n",
    "\n",
    "        # Shapiro-Wilk Test\n",
    "        shapiro_stat, shapiro_p = shapiro(numeric_data)\n",
    "        print(f\"Shapiro-Wilk Test: Statistic={shapiro_stat:.4f}, p-value={shapiro_p:.4f}\")\n",
    "\n",
    "        # Anderson-Darling Test\n",
    "        anderson_result = anderson(numeric_data)\n",
    "        print(f\"Anderson-Darling Test: Statistic={anderson_result.statistic:.4f}\")\n",
    "        #for i in range(len(anderson_result.critical_values)):\n",
    "        #    sl, cv = anderson_result.significance_level[i], anderson_result.critical_values[i]\n",
    "        #    if anderson_result.statistic < cv:\n",
    "        #        result = \"Accept\"\n",
    "        #    else:\n",
    "        #        result = \"Reject\"\n",
    "        #    print(f\"At {sl}% significance level, critical value: {cv:.4f}, {result} the null hypothesis of normality\")\n",
    "\n",
    "        # Kolmogorov-Smirnov Test against normal distribution\n",
    "        # ks_stat, ks_p = kstest(numeric_data, 'norm', args=(numeric_data.mean(), numeric_data.std()))\n",
    "        # print(f\"Kolmogorov-Smirnov Test: Statistic={ks_stat:.4f}, p-value={ks_p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boolean Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_boolean_columns(boolean_cols, dataframe):\n",
    "    \"\"\"\n",
    "    Analyze and visualize boolean columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    boolean_cols (list): List of boolean column names to analyze.\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    for column in boolean_cols:\n",
    "        print(f\"\\nSummary Statistics and Analysis for Boolean Column: {column}\")\n",
    "\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if column not in dataframe.columns:\n",
    "            print(f\"'{column}' is not found in the DataFrame. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Cast the column to boolean in case it contains 1/0 or other non-boolean values\n",
    "        dataframe[column] = dataframe[column].astype(bool)\n",
    "\n",
    "        # Prepare boolean counts\n",
    "        boolean_counts = dataframe[column].value_counts()\n",
    "\n",
    "        # Create a figure with two subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(FIG_WIDTH, FIG_HEIGHT)) # Configured at the beginning of the file for image consistancy\n",
    "\n",
    "        # Bar plot using 'x' parameter\n",
    "        sns.countplot(\n",
    "            x=column,\n",
    "            data=dataframe,\n",
    "            ax=axs[0]\n",
    "        )\n",
    "        axs[0].set_title(f'Boolean Distribution for {column}')\n",
    "        axs[0].set_xlabel(column)\n",
    "        axs[0].set_ylabel('Count')\n",
    "\n",
    "        # Rotate x-axis labels\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # Pie chart on the right subplot\n",
    "        boolean_counts.plot.pie(\n",
    "            autopct='%1.1f%%',\n",
    "            ax=axs[1],\n",
    "            startangle=90\n",
    "        )\n",
    "        axs[1].set_title(f'Proportion of True/False for {column}')\n",
    "        axs[1].set_ylabel('')\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Display summary statistics for boolean data\n",
    "        print(f\"\\nSummary for Boolean '{column}':\")\n",
    "        true_count = boolean_counts.get(True, 0)\n",
    "        false_count = boolean_counts.get(False, 0)\n",
    "        total_count = true_count + false_count\n",
    "        true_percentage = (true_count / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "        print(f\"Count of True: {true_count}\")\n",
    "        print(f\"Count of False: {false_count}\")\n",
    "        print(f\"Percentage of True: {true_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_columns(categorical_cols, dataframe):\n",
    "    \"\"\"\n",
    "    Analyze and visualize categorical columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    categorical_cols (list): List of categorical column names to analyze.\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    for column in categorical_cols:\n",
    "        print(f\"\\nSummary Statistics and Analysis for Categorical Column: {column}\")\n",
    "\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if column not in dataframe.columns:\n",
    "            print(f\" '{column}' is not found in the DataFrame. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Prepare category counts and percentages\n",
    "        category_counts = dataframe[column].value_counts()\n",
    "        category_percentages = dataframe[column].value_counts(normalize=True) * 100\n",
    "\n",
    "        # Display bar plot and pie chart\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "\n",
    "        # Bar plot using 'x' parameter\n",
    "        sns.countplot(\n",
    "            x=column,\n",
    "            data=dataframe,\n",
    "            ax=axs[0]\n",
    "        )\n",
    "        axs[0].set_title(f'Frequency Distribution for {column} (Categorical Data)')\n",
    "        axs[0].set_xlabel(column)\n",
    "        axs[0].set_ylabel('Frequency')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Pie chart on the right subplot\n",
    "        category_counts.plot.pie(\n",
    "            autopct='%1.1f%%',\n",
    "            ax=axs[1],\n",
    "            title=f'Proportion of Categories for {column}',\n",
    "            startangle=90\n",
    "        )\n",
    "        axs[1].set_ylabel('')\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nFrequency Table for '{column}':\")\n",
    "        freq_table = pd.DataFrame({'Count': category_counts, 'Percentage': category_percentages})\n",
    "        print(freq_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_features) # issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with our analysis of the Categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List to store features that contain any numerical values (even within mixed text)\n",
    "categorical_list_with_numerical_values = []\n",
    "\n",
    "# Iterate through each column in categorical_list\n",
    "for column in categorical_list:\n",
    "    # Check if the column contains any values with numerical characters\n",
    "    if df_dropped[column].apply(lambda x: bool(re.search(r'\\d', str(x)))).any():\n",
    "        categorical_list_with_numerical_values.append(column)\n",
    "        print(f\" '{column}' contains numerical values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[categorical_list_with_numerical_values].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore our data to avoid creating graphs for rows that contain unique values. To do this we'll run a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_uniqueness(df, columns):\n",
    "    \"\"\"\n",
    "    Analyzes the number of unique values in categorical columns and \n",
    "    helps identify suitable candidates for visualization.\n",
    "\n",
    "    Args:\n",
    "        df: The Pandas DataFrame.\n",
    "        columns: A list of column names to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are column names and values are \n",
    "        the number of unique values in each column. Also, \n",
    "        prints a summary to assist visualizing the results via bar graphs.\n",
    "    \"\"\"\n",
    "    uniqueness_counts = {}\n",
    "    for col in columns:\n",
    "        if not pd.api.types.is_object_dtype(df[col]):  # checks for object type; adjust if needed\n",
    "            continue  # Skip analysis for non-categorical columns\n",
    "        \n",
    "        unique_count = df[col].nunique()  # use nunique for direct count of unique values\n",
    "        uniqueness_counts[col] = unique_count\n",
    "\n",
    "    # Sort the dictionary by unique counts in descending order\n",
    "    sorted_uniqueness_counts = dict(sorted(uniqueness_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Print sorted output with unique counts\n",
    "    print(\"Sorted Unique Value Counts:\")\n",
    "    for col, unique_count in sorted_uniqueness_counts.items():\n",
    "        print(f\"Number of unique values in '{col}': {unique_count}\")\n",
    "\n",
    "    # Create a bar graph to visualize the number of unique values, sorted from high to low\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(sorted_uniqueness_counts.keys(), sorted_uniqueness_counts.values())\n",
    "    plt.xlabel(\"Categorical Columns\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Unique Values\", fontsize=14)\n",
    "    plt.title(\"Unique Value Counts per Categorical Column (Sorted)\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)  # Rotate x-axis labels for readability\n",
    "    plt.tight_layout()  # Adjust subplot parameters for a tight layout\n",
    "    plt.show()\n",
    "\n",
    "    return sorted_uniqueness_counts\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#categorical_uniqueness = analyze_categorical_uniqueness(df_dropped, categorical_list_with_numerical_values)\n",
    "#columns_for_visualization = [col for col, count in categorical_uniqueness.items() if count <= 15] # Example: Use only below or equal to 10\n",
    "#print(f\"\\nSuggested columns for visualization (<= 15 unique values): {columns_for_visualization}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_uniqueness = analyze_categorical_uniqueness(df, categorical_list_with_numerical_values)\n",
    "\n",
    "columns_for_visualization = [col for col, count in categorical_uniqueness.items() if count <= 10] # Example: Use only below or equal to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straight away we can see `int_rate` (584),`emp_title` (40049), `issue_d` (159),`title` (3454), `revol_util` (1088), `last_pymnt_d` (147), `earliest_cr_line` (667), `sec_app_earliest_cr_line` (506), `last_credit_pull` (137). `zip_code` (878) stand out, we'll remove these as graphically representing these won't produce any meaningful insight. However, we'll see if we can reduce them into logical groupings later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = ['int_rate', 'zip_code', 'emp_title', 'issue_d', 'title', 'revol_util', 'last_pymnt_d', 'earliest_cr_line', 'sec_app_earliest_cr_line', 'last_credit_pull_d']\n",
    "\n",
    "# Filter the categorical columns to exclude specified ones\n",
    "filtered_categorical_columns = [col for col in categorical_list_with_numerical_values if col not in exclude_list]\n",
    "\n",
    "# Analyze the filtered categorical columns\n",
    "analyze_categorical_columns(filtered_categorical_columns, df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Object and Float Features\n",
    "\n",
    "We have identified the following columns that require conversion or encoding:\n",
    "\n",
    "#### Actions Post-Analysis\n",
    "\n",
    "##### Convert String to Integer\n",
    "- **term**: Extract numerical part and convert to integer (keep `36` and `60`).\n",
    "- **emp_length**: Extract numerical years and convert to integer.\n",
    "\n",
    "##### Convert String to Float\n",
    "- **int_rate**: Convert to float after removing any non-numeric characters.\n",
    "- **revol_util**: Convert to float after removing the \"%\" symbol.\n",
    "\n",
    "##### Encode Categorical Values\n",
    "- **sub_grade**: Use as is or encode if necessary; consider dropping **grade** if redundant.\n",
    "- **loan_status**: Group or encode based on loan status levels.\n",
    "- **hardship_loan_status**: Analyze and group similar hardship statuses if logical.\n",
    "\n",
    "##### Convert to Date/Time Format\n",
    "- **issue_d**: Convert to date/time for chronological analysis.\n",
    "- **earliest_cr_line**: Convert to date/time to track the earliest credit history.\n",
    "- **last_pymnt_d**: Convert to date/time; create separate year and month columns.\n",
    "- **next_pymnt_d**: Convert to date/time; add year and month columns.\n",
    "- **last_credit_pull_d**: Convert to date/time for recent credit activity insights.\n",
    "- **sec_app_earliest_cr_line**: Convert to date/time for secondary applicants’ credit history.\n",
    "- **hardship_start_date**: Convert to date/time; add year and month columns.\n",
    "- **hardship_end_date**: Convert to date/time; add year and month columns.\n",
    "- **payment_plan_start_date**: Convert to date/time; add year and month columns.\n",
    "\n",
    "##### Remove Non-Analytical or Irrelevant Columns\n",
    "- **emp_title**: Not relevant for numerical analysis; remove.\n",
    "- **url**: Non-analytical; remove as it doesn’t contribute to analysis.\n",
    "\n",
    "##### Evaluate for Categorical Consistency\n",
    "- **zip_code**: Analyze the first few digits if relevant to extract location-based insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_feature_list(original_features, new_features):\n",
    "    \"\"\"\n",
    "    Checks for the existence of original features in a list, \n",
    "    removes them, and adds new features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_features : list\n",
    "        List of original feature names to be removed.\n",
    "    new_features : list\n",
    "        List of new feature names to be added.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Updated list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    updated_features = original_features.copy()  # Create a copy to avoid modifying the original list\n",
    "\n",
    "    for feature in original_features:\n",
    "        if feature in updated_features:\n",
    "            updated_features.remove(feature)\n",
    "\n",
    "    updated_features.extend(new_features)\n",
    "    return updated_features\n",
    "\n",
    "# Example usage:\n",
    "#original_features = ['int_rate', 'revol_util'] \n",
    "#new_features = ['int_rate_kn', 'revol_util_kn']\n",
    "\n",
    "#updated_list = update_feature_list(original_features, new_features)\n",
    "#print(updated_list)  # Output: ['int_rate_kn', 'revol_util_kn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Columns that need to be converted from string to integer\n",
    "string_columns_to_convert_int = ['term', 'emp_length']  # features to replace with int values\n",
    "\n",
    "# Convert each specified column to an integer in a new column\n",
    "for column in string_columns_to_convert_int:\n",
    "    # Extract numerical part and convert to integer\n",
    "    df_dropped[f\"{column}_kn\"] = df_dropped[column].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[['term_kn','emp_length_kn']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = ['term', 'emp_length']\n",
    "\n",
    "# Remove specified columns from new_features if they exist\n",
    "new_features = [col for col in new_features if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['term_kn', 'emp_length_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = string_columns_to_convert_int\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"Both columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the columns that need to be converted from string to float\n",
    "string_columns_to_convert_float = ['int_rate', 'revol_util']  # features to replace with float values\n",
    "\n",
    "# Convert each specified column to a float in a new column as percentage (e.g., 80% becomes 0.80)\n",
    "for column in string_columns_to_convert_float:\n",
    "    # Extract numerical part, convert to float, and divide by 100\n",
    "    df_dropped[f\"{column}_kn\"] = df_dropped[column].apply(\n",
    "        lambda x: float(re.search(r'\\d+', str(x)).group()) / 100 if pd.notnull(x) else None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['int_rate_kn'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['int_rate_kn', 'revol_util_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = ['int_rate', 'revol_util']\n",
    "\n",
    "# Remove specified columns from new_features if they exist\n",
    "new_features = [col for col in new_features if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = ['int_rate_kn', 'revol_util_kn']\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"Both columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert date time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[['issue_d', 'earliest_cr_line']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of date columns to convert\n",
    "date_columns = ['issue_d', 'earliest_cr_line']\n",
    "\n",
    "# Convert the columns to datetime format\n",
    "for col in date_columns:\n",
    "    # Convert to datetime using the format '%b-%y', with errors coerced to NaT\n",
    "    df_dropped[col] = pd.to_datetime(df_dropped[col], format='%b-%y', errors='coerce')\n",
    "\n",
    "# Extracting date features for each date column\n",
    "for col in date_columns:\n",
    "    # Define new column names for year and month\n",
    "    year_col = f'{col}_year_kn'\n",
    "    month_col = f'{col}_month_kn'\n",
    "       \n",
    "    # Extract year and month, convert to integer with support for NaNs\n",
    "    df_dropped[year_col] = df_dropped[col].dt.year.astype('Int64').fillna(0)  # Extract year\n",
    "    df_dropped[month_col] = df_dropped[col].dt.month.astype('Int64').fillna(0)  # Extract month\n",
    "    \n",
    "    # Add the new column names to the existing 'new_features' list\n",
    "    new_features.extend([year_col, month_col])\n",
    "    \n",
    "    # Print the names of the newly created columns\n",
    "    print(f\"New columns created: {year_col}, {month_col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[['issue_d', 'earliest_cr_line', 'issue_d_year_kn', 'issue_d_month_kn', 'earliest_cr_line_year_kn', 'earliest_cr_line_month_kn']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['issue_d_year_kn', 'issue_d_month_kn','earliest_cr_line_year_kn', 'earliest_cr_line_month_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = ['issue_d', 'earliest_cr_line']\n",
    "\n",
    "# Remove specified columns from new_features if they exist\n",
    "new_features = [col for col in new_features if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = ['issue_d_year_kn', 'issue_d_month_kn','earliest_cr_line_year_kn', 'earliest_cr_line_month_kn']\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"All columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of columns in df_dropped\n",
    "df_dropped_columns = set(df_dropped.columns)\n",
    "\n",
    "# Convert new_features to a set for comparison\n",
    "new_features_set = set(new_features)\n",
    "\n",
    "# Find columns in new_features that are missing in df_dropped\n",
    "missing_in_df_dropped = new_features_set - df_dropped_columns\n",
    "\n",
    "# Find columns in df_dropped that are not in new_features (if relevant)\n",
    "extra_in_df_dropped = df_dropped_columns - new_features_set\n",
    "\n",
    "# Print the results\n",
    "print(\"Features in new_features missing from df_dropped:\", missing_in_df_dropped)\n",
    "print(\"Columns in df_dropped not listed in new_features:\", extra_in_df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Non Analytical or Irrelevent columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm tempted to leave these in as it could be a good indicator of good or bad payment trends but won't have the time to do it for now we'll remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = ['zip_code', 'emp_title']\n",
    "\n",
    "# Remove specified columns from new_features if they exist\n",
    "new_features = [col for col in new_features if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sense check\n",
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating logical groups "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **hardship_reason**: Analyse and group to simplify analysis (import from df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logical groupings for 'hardship_reason'\n",
    "hardship_reason_groupings = {\n",
    "    'INCOMECURT': 'Income Loss',\n",
    "    'UNEMPLOYED': 'Income Loss',\n",
    "    'UNEMPLOYMENT': 'Income Loss',\n",
    "    'INCOME_CURTAILMENT': 'Income Loss',\n",
    "    'REDCDHOURS': 'Income Loss',\n",
    "    'REDUCED_HOURS': 'Income Loss',\n",
    "    'FURLOUGH': 'Income Loss',\n",
    "    'MEDICAL': 'Health Issues',\n",
    "    'DISABILITY': 'Health Issues',\n",
    "    'NATURAL_DISASTER': 'External Events',\n",
    "    'NATDISAST': 'External Events',\n",
    "    'FINANCIAL': 'Financial Strain',\n",
    "    'EXCESSIVE_OBLIGATIONS': 'Financial Strain',\n",
    "    'EXCESSOBLI': 'Financial Strain',\n",
    "    'DIVORCE': 'Family Circumstances',\n",
    "    'FAMILY_DEATH': 'Family Circumstances',\n",
    "    'DEATH': 'Family Circumstances'\n",
    "}\n",
    "\n",
    "# Apply the grouping to the 'hardship_reason' column\n",
    "df_dropped['hardship_reason_grouped_kn'] = df['hardship_reason'].replace(hardship_reason_groupings) # adding this back into our data set\n",
    "\n",
    "# Verify the groupings\n",
    "print(df_dropped['hardship_reason_grouped_kn'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logical groupings for 'loan_status'\n",
    "loan_status_groupings = {\n",
    "    'Fully Paid': 'Completed',\n",
    "    'Current': 'In Progress',\n",
    "    'Charged Off': 'Defaulted',\n",
    "    'Late (31-120 days)': 'Late',\n",
    "    'In Grace Period': 'Late',\n",
    "    'Late (16-30 days)': 'Late',\n",
    "    'Does not meet the credit policy. Status:Fully Paid': 'Completed',\n",
    "    'Issued': 'In Progress',\n",
    "    'Does not meet the credit policy. Status:Charged Off': 'Defaulted',\n",
    "    'Default': 'Defaulted'\n",
    "}\n",
    "\n",
    "# Apply the grouping to the 'loan_status' column\n",
    "df_dropped['loan_status_grouped_kn'] = df['loan_status'].replace(loan_status_groupings)\n",
    "\n",
    "# Verify the groupings\n",
    "print(df_dropped['loan_status_grouped_kn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Any', 'Other', and 'none' with a unified value 'Other'\n",
    "df_dropped['home_ownership_grouped_kn'] = df_dropped['home_ownership'].replace(['ANY', 'OTHER', 'NONE'], 'OTHER')\n",
    "\n",
    "# Display the updated DataFrame to verify\n",
    "df_dropped['home_ownership_grouped_kn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['hardship_reason_grouped_kn', 'loan_status_grouped_kn','home_ownership_grouped_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of columns to be removed\n",
    "# these are all the columns we've altered. These can remain in the data frame but should be removed from our feature list\n",
    "columns_to_remove = ['hardship_reason', 'loan_status','home_ownership']\n",
    "\n",
    "# Remove specified columns from new_features if they exist\n",
    "new_features = [col for col in new_features if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = ['hardship_reason_grouped_kn', 'loan_status_grouped_kn','home_ownership_grouped_kn']\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"Both columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our list function again on the udpated data frame and scroll through to update our feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_boolean_columns(boolean_list, df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for missing values\n",
    "columns_to_check = [\n",
    "    'earliest_cr_line', 'fico_range_high', \n",
    "    'fico_range_low', 'last_fico_range_high', \n",
    "    'last_fico_range_low', 'pub_rec', \n",
    "    'pub_rec_bankruptcies', 'annual_inc', \n",
    "    'inq_last_6mths'\n",
    "]\n",
    "\n",
    "# Check for missing values in each specified column and display the count\n",
    "for column in columns_to_check:\n",
    "    missing_count = df[column].isnull().sum() # checking against original data if 0 we can remove these from our ML feature list\n",
    "    print(f\"Missing values in '{column}': {missing_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing our output I've decided to drop the following values as their representation within the data set is not meaningful enought to be any statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original list of columns to remove\n",
    "columns_to_remove = [\n",
    "    'earliest_cr_line_missing_clean_kn', 'fico_range_high_missing_clean_kn', \n",
    "    'fico_range_low_missing_clean_kn', 'last_fico_range_high_missing_clean_kn', \n",
    "    'last_fico_range_low_missing_clean_kn', 'pub_rec_missing_clean_kn', \n",
    "    'annual_inc_missing_clean_kn', 'inq_last_6mths_missing_clean_kn'\n",
    "]\n",
    "\n",
    "# Create a new list by excluding the columns in columns_to_remove\n",
    "new_features_updated = [col for col in new_features if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['loan_status_grouped_kn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_numeric_columns(numerical_list, df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finised our EDA let's check some of the categorical points with our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of purpose and loan_status\n",
    "comparison_loan_status_purpose = pd.crosstab(df_dropped['loan_status_grouped_kn'], df_dropped['purpose'])\n",
    "\n",
    "# Display the result\n",
    "comparison_loan_status_purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort data by total loan counts across all statuses for each purpose (descending order)\n",
    "sorted_data = comparison_loan_status_purpose.sum(axis=0).sort_values(ascending=False)\n",
    "sorted_columns = sorted_data.index\n",
    "comparison_loan_status_purpose_sorted = comparison_loan_status_purpose[sorted_columns]\n",
    "\n",
    "# Plot a stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot each loan status as a stacked segment\n",
    "comparison_loan_status_purpose_sorted.T.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Purpose')\n",
    "ax.set_ylabel('Number of Loans')\n",
    "ax.set_title('Distribution of Loan Statuses by Loan Purpose')\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Loan Status')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of purpose and loan_status\n",
    "comparison_loan_status_ver_status = pd.crosstab(df_dropped['loan_status_grouped_kn'], df_dropped['verification_status'])\n",
    "\n",
    "# Display the result\n",
    "comparison_loan_status_ver_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Transpose the DataFrame for easier plotting and plot each loan status as a stacked segment\n",
    "comparison_loan_status_ver_status.T.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Verification Status')\n",
    "ax.set_ylabel('Number of Loans')\n",
    "ax.set_title('Distribution of Loan Statuses by Verification Status')\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Loan Status')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting view, as the the Not Verified, and Source Verified represent both roughly 15-20% of the charged off loans for each status but Verified accounts for roughly 23% of the total. I was expecting a lot of the Charged Off in the Not Verified `verification_status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of purpose and loan_status\n",
    "comparison_loan_status_addr_state = pd.crosstab(df_dropped['loan_status_grouped_kn'], df_dropped['addr_state'])\n",
    "\n",
    "# Display the result\n",
    "comparison_loan_status_addr_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to only include the 'Defaulted' loan status\n",
    "defaulted_by_state = comparison_loan_status_addr_state.loc['Defaulted']\n",
    "\n",
    "# Sort the data by the number of defaulted loans in descending order\n",
    "defaulted_by_state_sorted = defaulted_by_state.sort_values(ascending=False)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "defaulted_by_state_sorted.plot(kind='bar', color='salmon', edgecolor='black')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Number of Defaulted Loans\")\n",
    "plt.title(\"Number of Defaulted Loans by State (Sorted)\")\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group data by 'loan_status' and 'addr_state' and count occurrences\n",
    "grouped_data = df_dropped.groupby(['addr_state', 'loan_status_grouped_kn']).size().unstack()\n",
    "\n",
    "# Sort the data by the total count of loan statuses in descending order\n",
    "grouped_data = grouped_data.loc[grouped_data.sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "# Plot the bar chart\n",
    "grouped_data.plot(kind='bar', stacked=True, figsize=(10, 7))\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Loan Status by Address State')\n",
    "plt.xlabel('Address State')\n",
    "plt.ylabel('Count of Loan Status')\n",
    "plt.legend(title='Loan Status')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of loans for each state\n",
    "total_loans_by_state = comparison_loan_status_addr_state.sum(axis=0)\n",
    "\n",
    "# Extract the number of defaulted loans for each state\n",
    "defaulted_loans_by_state = comparison_loan_status_addr_state.loc['Defaulted']\n",
    "\n",
    "# Calculate the percentage of defaulted loans\n",
    "defaulted_percentage_by_state = (defaulted_loans_by_state / total_loans_by_state) * 100\n",
    "\n",
    "# Combine into a DataFrame for easy viewing\n",
    "defaulted_percentage_df = pd.DataFrame({\n",
    "    'Total Loans': total_loans_by_state,\n",
    "    'Defaulted Loans': defaulted_loans_by_state,\n",
    "    '% Defaulted': defaulted_percentage_by_state\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "defaulted_percentage_df = defaulted_percentage_df.sort_values(by='% Defaulted', ascending=False)\n",
    "defaulted_percentage_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Observations\n",
    "- **Top States**: The states with the highest loan counts include **CA (California)**, **TX (Texas)**, **NY (New York)**, and **FL (Florida)**. These states exhibit a high volume of loans, likely due to their larger populations and economic activities.\n",
    "- **Completed Loans**: The **Completed** loan status (blue segment) constitutes a significant portion in most states, suggesting a high rate of loan completion across regions.\n",
    "- **In Progress Loans**: The **In Progress** loan status (green segment) appears prominently in states with higher loan volumes, indicating ongoing loan activities.\n",
    "- **Defaulted and Late Loans**: **Defaulted** (orange) and **Late** (red) loans make up smaller portions of the overall loan distribution. However, states with higher loan counts (e.g., CA, TX, NY) also show relatively higher counts in these categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep `addr_state` in our feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post EDA Analysis data transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call our list function with the new_features selected. Then sense check the results for duplicates or missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this calls the split_data_frame function above create three lists to capture the sorting outputs in. \n",
    "# These will later be used to pull some graphs to evaluate the data and what possible transformations we've missed.\n",
    "# Our function parameters are list and the latest version of our data frame in this caes new_features and df_dropped accordingly.\n",
    "boolean_list, numerical_list, categorical_list = split_data_frame(new_features, df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns by datetime dtype\n",
    "datetime_headers = df_dropped.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "# Print the list of datetime headers\n",
    "print(\"Datetime headers in data frame:\", datetime_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing data time columns as we aren't running a time series.\n",
    "# List of datetime headers\n",
    "datetime_headers = df_dropped.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "# Function to remove datetime headers from a given list\n",
    "def remove_datetime_headers(feature_list, datetime_headers):\n",
    "    return [feature for feature in feature_list if feature not in datetime_headers]\n",
    "\n",
    "# Removing datetime columns from each feature list\n",
    "boolean_list = remove_datetime_headers(boolean_list, datetime_headers)\n",
    "numerical_list = remove_datetime_headers(numerical_list, datetime_headers)\n",
    "categorical_list = remove_datetime_headers(categorical_list, datetime_headers)\n",
    "\n",
    "# Print updated lists to verify\n",
    "print(\"Updated boolean_list:\", boolean_list)\n",
    "print(\"Updated numerical_list:\", numerical_list)\n",
    "print(\"Updated categorical_list:\", categorical_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_duplicates(*lists):\n",
    "    \"\"\"\n",
    "    Checks for duplicate headers in each list and prints the duplicates if any are found.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    *lists : list\n",
    "        Lists of headers to check for duplicates.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    for i, header_list in enumerate(lists, start=1):\n",
    "        duplicates = [item for item in set(header_list) if header_list.count(item) > 1]\n",
    "        if duplicates:\n",
    "            print(f\"Duplicates found in list {i}: {duplicates}\")\n",
    "        else:\n",
    "            print(f\"No duplicates found in list {i}.\")\n",
    "\n",
    "check_for_duplicates(boolean_list, numerical_list, categorical_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(*lists):\n",
    "    \"\"\"\n",
    "    Removes duplicates from each list and returns lists with unique headers.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    *lists : list\n",
    "        Lists of headers to remove duplicates from.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    unique_lists : list\n",
    "        A list of lists, each with duplicates removed.\n",
    "    \"\"\"\n",
    "    unique_lists = [list(set(header_list)) for header_list in lists]\n",
    "    \n",
    "    return unique_lists\n",
    "\n",
    "boolean_list, numerical_list, categorical_list = remove_duplicates(boolean_list, numerical_list, categorical_list)\n",
    "\n",
    "print(\"Boolean list without duplicates:\", boolean_list)\n",
    "print(\"Numerical list without duplicates:\", numerical_list)\n",
    "print(\"Categorical list without duplicates:\", categorical_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lists_against_dataframe(dataframe, *lists):\n",
    "    \"\"\"\n",
    "    Checks each provided list for missing and duplicate columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        The DataFrame to check against.\n",
    "    *lists : list\n",
    "        Lists of column names to check.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Iterate over each list and check for missing columns and duplicates\n",
    "    for i, header_list in enumerate(lists, start=1):\n",
    "        # Convert list to a set to check for missing items and duplicates\n",
    "        list_set = set(header_list)\n",
    "        \n",
    "        # Find columns in the list that are not in the DataFrame\n",
    "        missing_in_df = list_set - set(dataframe.columns)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nList {i} Results:\")\n",
    "        if missing_in_df:\n",
    "            print(f\"Columns missing in DataFrame: {missing_in_df}\")\n",
    "        else:\n",
    "            print(\"All columns are present in DataFrame.\")\n",
    "\n",
    "# Run the function with each list\n",
    "check_lists_against_dataframe(df_dropped, boolean_list, numerical_list, categorical_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = df_dropped[new_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if two DataFrames are exactly the same\n",
    "are_identical = df_dropped[new_features].equals(df_transformed)\n",
    "print(\"DataFrames are identical:\", are_identical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in df_dropped but not in df_transformed:\", set(df_dropped[new_features].columns) - set(df_transformed.columns))\n",
    "print(\"Columns in df_transformed but not in df_dropped:\", set(df_transformed.columns) - set(df_dropped.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed[categorical_list].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def transform_features(dataframe, bool_features, num_features, cat_features):\n",
    "    \"\"\"\n",
    "    Transforms features in the dataframe based on their data type and specified feature lists.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        The DataFrame containing the data to be transformed.\n",
    "    bool_features : list\n",
    "        A list of column names representing boolean features.\n",
    "    num_features : list\n",
    "        A list of column names representing numerical features.\n",
    "    cat_features : list\n",
    "        A list of column names representing categorical features.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_transformed : pd.DataFrame\n",
    "        The DataFrame with transformed features.\n",
    "    \"\"\"\n",
    "    # Copy the original DataFrame to avoid modifying it\n",
    "    df_transformed = dataframe.copy()\n",
    "\n",
    "    # Process boolean features (convert to integers, skip scaling)\n",
    "    for feature in bool_features:\n",
    "        if feature in df_transformed.columns:\n",
    "            print(f\"Processing boolean feature: {feature}\")\n",
    "            df_transformed[feature] = df_transformed[feature].astype(bool).astype(int)\n",
    "        else:\n",
    "            print(f\"Boolean feature '{feature}' not found in DataFrame. Skipping...\")\n",
    "\n",
    "    # Helper function to handle skewness and log transformation\n",
    "    def handle_skewness(series):\n",
    "        skewness = series.skew()\n",
    "        if skewness > 1 or skewness < -1:\n",
    "            # Apply log transformation to reduce skewness\n",
    "            feature_min = series.min()\n",
    "            if feature_min <= 0:\n",
    "                return np.log1p(series - feature_min + 1)\n",
    "            else:\n",
    "                return np.log1p(series)\n",
    "        return series\n",
    "\n",
    "    # Process numerical features (apply transformations and scale at the end)\n",
    "    num_features_to_scale = []\n",
    "    for feature in num_features:\n",
    "        if feature in df_transformed.columns:\n",
    "            print(f\"Processing numerical feature: {feature}\")\n",
    "            df_transformed[feature] = df_transformed[feature].fillna(0)  # Impute missing values\n",
    "            df_transformed[feature] = handle_skewness(df_transformed[feature])  # Apply skewness handling\n",
    "            num_features_to_scale.append(feature)  # Collect features for batch scaling\n",
    "        else:\n",
    "            print(f\"Numerical feature '{feature}' not found in DataFrame. Skipping...\")\n",
    "\n",
    "    # Scale all numerical features in a single step\n",
    "    scaler = StandardScaler()\n",
    "    df_transformed[num_features_to_scale] = scaler.fit_transform(df_transformed[num_features_to_scale])\n",
    "\n",
    "    # Process categorical features (One-Hot Encoding, ensure no duplicate encoding later)\n",
    "    cat_features = [col for col in cat_features if col in df_transformed.columns]\n",
    "    if cat_features:\n",
    "        print(f\"Processing categorical features: {cat_features}\")\n",
    "        dummies = pd.get_dummies(df_transformed[cat_features], prefix=cat_features)\n",
    "        df_transformed = pd.concat([df_transformed, dummies], axis=1)\n",
    "        df_transformed.drop(columns=cat_features, inplace=True)\n",
    "\n",
    "    print(\"Transformation complete\")\n",
    "    return df_transformed\n",
    "\n",
    "# Example usage (ensure boolean_list, numerical_list, and categorical_list are defined)\n",
    "# df_transformed = transform_features(df_dropped, boolean_list, numerical_list, categorical_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = transform_features(df_dropped, boolean_list, numerical_list, categorical_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the VIF calculation function\n",
    "def calculate_vif(df):\n",
    "    \"\"\"\n",
    "    Calculate VIF (Variance Inflation Factor) for each numeric feature in the given DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the features to check for multicollinearity.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    vif_data : pd.DataFrame\n",
    "        DataFrame with features and their VIF scores, sorted from highest to lowest.\n",
    "    \"\"\"\n",
    "    # Select only numeric columns, as VIF is applicable to numeric data\n",
    "    X = df.select_dtypes(include=[float, int]).dropna()\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    tqdm.pandas(desc=\"Calculating VIF\")\n",
    "\n",
    "    # Calculate VIF for each feature with progress tracking\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in tqdm(range(X.shape[1]), total=X.shape[1])]\n",
    "\n",
    "    # Sort VIF data from highest to lowest\n",
    "    vif_data = vif_data.sort_values(by=\"VIF\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Print features with high VIF scores\n",
    "    high_vif = vif_data[vif_data[\"VIF\"] > 5]\n",
    "    print(\"\\nFeatures with high VIF scores (indicating multicollinearity):\")\n",
    "    print(high_vif)\n",
    "    \n",
    "    return vif_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our VIF on our data frame (df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply VIF calculation to all numeric columns in df_transformed\n",
    "vif_data_all = calculate_vif(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = [\n",
    "    'mo_sin_old_rev_tl_op_missing_clean_kn'] \n",
    "\n",
    "\"\"\"\n",
    "    'num_accts_ever_120_pd_missing_clean_kn',\n",
    "    'fico_range_high',\n",
    "    'fico_range_low',\n",
    "    'out_prncp_inv',\n",
    "    'out_prncp',\n",
    "    'loan_amnt',\n",
    "    'funded_amnt',\n",
    "    'funded_amnt_inv',\n",
    "    'tot_cur_bal',\n",
    "    'total_pymnt',\n",
    "    'total_pymnt_inv',\n",
    "    'avg_cur_bal',\n",
    "    'total_acc',\n",
    "    'total_bal_il',\n",
    "    'num_sats',\n",
    "    'total_bal_ex_mort',\n",
    "    'num_rev_tl_bal_gt_0',\n",
    "    'num_actv_rev_tl',\n",
    "    'tot_hi_cred_lim',\n",
    "    'total_bc_limit',\n",
    "    'total_rev_hi_lim',\n",
    "    'num_op_rev_tl',\n",
    "    'dti_joint',\n",
    "    'annual_inc_joint',\n",
    "    'num_rev_accts',\n",
    "    'total_rec_prncp',\n",
    "    'pct_tl_nvr_dlq',\n",
    "    'acc_open_past_24mths',\n",
    "    'installment',\n",
    "    'issue_d_year_kn',\n",
    "    'bc_util',\n",
    "    'open_acc',\n",
    "    'open_act_il',\n",
    "    'all_util',\n",
    "    'open_rv_24m',\n",
    "    'revol_util_kn',\n",
    "    'revol_bal',\n",
    "    'bc_open_to_buy',\n",
    "    'num_bc_sats',\n",
    "    'max_bal_bc',\n",
    "    'pub_rec',\n",
    "    'num_actv_bc_tl',\n",
    "    'num_bc_tl',\n",
    "    'num_il_tl',\n",
    "    'il_util',\n",
    "    'open_il_24m',\n",
    "    'open_il_12m',\n",
    "    'pub_rec_bankruptcies',\n",
    "    'num_tl_op_past_12m',\n",
    "    'mths_since_last_record',\n",
    "    'inq_last_12m_missing_clean_kn',\n",
    "    'inq_fi_missing_clean_kn'\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "# Removing specified columns from df_transformed\n",
    "df_transformed = df_transformed.drop(columns=columns_to_remove, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply VIF calculation to all numeric columns in df_transformed\n",
    "vif_data_all = calculate_vif(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor (VIF) Analysis\n",
    "\n",
    "VIF analysis helps us detect multicollinearity among features, where high VIF values indicate strong correlations that can destabilize regression models and reduce interpretability. Reducing multicollinearity improves model robustness, decreases overfitting, and ensures reliable coefficient estimates.\n",
    "\n",
    "#### Why Run VIF?\n",
    "By identifying features with high VIF, we can remove those causing redundancy, leading to a more stable model. Features with VIF scores over 10 are typically problematic and indicate a strong correlation with other features.\n",
    "\n",
    "#### Next Steps\n",
    "\n",
    "Remove High VIF Features: Begin with the feature with the highest VIF score, as it contributes most to multicollinearity.\n",
    "Recalculate VIF: After each removal, recheck VIF to assess the impact on remaining features.\n",
    "Repeat as Needed: Continue until all VIF scores are within an acceptable range, typically below 10.\n",
    "\n",
    "**Interpreting VIF Scores**\n",
    "\n",
    "#### VIF values are generally interpreted as follows:\n",
    "\n",
    "- VIF = 1: No correlation with other variables.\n",
    "- 1 < VIF < 5: Low to moderate correlation.\n",
    "- VIF > 5: High correlation, potentially problematic.\n",
    "- VIF > 10: Very high correlation, indicating strong multicollinearity that should be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns by datetime dtype\n",
    "df_object = df_transformed.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "# Print the list of datetime headers\n",
    "print(\"Datetime headers in data frame:\", df_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = ['earliest_cr_line', 'issue_d', 'emp_length', 'emp_title', 'home_ownership', 'int_rate', 'loan_status', 'revol_util', 'term', 'zip_code']\n",
    "\n",
    "# Removing specified columns from df_transformed\n",
    "df_transformed = df_transformed.drop(columns=columns_to_remove, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting the new_features list. During the encoding process I forgot to remove the original columns. This caused string to float conversion errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns by dtype\n",
    "df_object = df_transformed.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Print the list of headers\n",
    "print(\"Datetime headers in data frame:\", df_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elements from datetime_headers that are also in new_features\n",
    "in_new_features = [item for item in df_object if item in new_features]\n",
    "# Elements from datetime_headers that are not in new_features\n",
    "not_in_new_features = [item for item in df_object if item not in new_features]\n",
    "\n",
    "print(\"Elements in df that are in new_features:\", in_new_features)\n",
    "print(\"Elements in df that are NOT in new_features:\", not_in_new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns by dtype\n",
    "datetime_headers = df_transformed.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "# Print the list of headers\n",
    "print(\"Datetime headers in data frame:\", datetime_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scanning df_transformed for the value \"< 1 year\" and identifying columns that contain it\n",
    "\n",
    "# Check if the value \"< 1 year\" exists in each column\n",
    "#matching_columns = df_transformed.apply(lambda col: col.astype(str).str.contains(r\"1\", na=False)).any()\n",
    "\n",
    "# Filter to get columns that contain the value \"< 1 year\"\n",
    "#columns_with_value = matching_columns[matching_columns].index.tolist()\n",
    "\n",
    "# Output the result\n",
    "#print(\"Columns containing your word:\", columns_with_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce our loan_status_grouped_kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['loan_status_grouped_kn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df_dropped\n",
    "\n",
    "def group_loan_status(status):\n",
    "    if status in (\"Completed\", \"In Progress\"):\n",
    "        return \"NoDefault\"\n",
    "    elif status in (\"Defaulted\", \"Late\"):\n",
    "        return \"Defaulted\"\n",
    "    else:\n",
    "        return status\n",
    "\n",
    "df_dropped['loan_status_grouped2_kn'] = df_dropped['loan_status_grouped_kn'].apply(group_loan_status)\n",
    "\n",
    "# Print the value counts of the new field\n",
    "print(df_dropped['loan_status_grouped2_kn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[['loan_status_grouped2_kn','loan_status_grouped_kn']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Scikit-learn classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Additional classifiers from external libraries\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Single Classification - boolean Y value\n",
    "# Configured for binary classification\n",
    "models_single = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=50),\n",
    "    #'Random Forest': RandomForestClassifier(random_state=50, n_estimators=100),\n",
    "    #'Gradient Boosting': GradientBoostingClassifier(random_state=50, n_estimators=100),\n",
    "    #'SVM': SVC(probability=True, random_state=50),\n",
    "    #'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    #'Naive Bayes': GaussianNB(),\n",
    "}\n",
    "\n",
    "# Multi Classification - Multiple Y values\n",
    "# Configured for multi-class classification\n",
    "models_multi = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, solver=\"lbfgs\"),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=50),\n",
    "    'Random Forest': RandomForestClassifier(random_state=50, n_estimators=100),\n",
    "    #'Gradient Boosting': GradientBoostingClassifier(random_state=50, n_estimators=100),\n",
    "    #'SVM': SVC(probability=True, random_state=50, decision_function_shape='ovr'),\n",
    "    #'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    #'Naive Bayes': GaussianNB(),\n",
    "    #'XGBoost': XGBClassifier(objective=\"multi:softmax\", random_state=50, n_estimators=100),\n",
    "    #'LightGBM': LGBMClassifier(objective=\"multiclass\", random_state=50, n_estimators=100),\n",
    "    #'CatBoost': CatBoostClassifier(loss_function=\"MultiClass\", random_state=50, iterations=100),\n",
    "    #'Neural Network': MLPClassifier(random_state=50, max_iter=300)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Define the save path as a constant for the ML results (*.pkl)\n",
    "SAVE_PATH = \"c:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\GitHub\\\\dsif-git-main-project\\\\elvtr_main_project\\\\models\\\\\"\n",
    "\n",
    "# Assign your dataframes\n",
    "X = df_transformed  # Use the output from our encoded data\n",
    "Y = df_dropped['loan_status_grouped2_kn']\n",
    "\n",
    "# Optional: Encode target variable if necessary\n",
    "if Y.dtype == 'object' or len(Y.unique()) > 2:\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y = label_encoder.fit_transform(Y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.5, random_state=50\n",
    ")\n",
    "\n",
    "# Model selection prompt\n",
    "def get_model_choice():\n",
    "    model_type = input(\"Select model type to run ('single' for models_single or 'multi' for models_multi): \").strip().lower()\n",
    "    if model_type == 'single':\n",
    "        print(\"Selected: Single Classification Model\")\n",
    "        return models_single, None  # No multi-class strategy needed for single\n",
    "    elif model_type == 'multi':\n",
    "        multi_class_strategy = input(\"Select multi-class strategy ('ovo' for one-vs-one or 'ovr' for one-vs-rest): \").strip().lower()\n",
    "        print(f\"Selected: Multi Classification Model with '{multi_class_strategy}' strategy\")\n",
    "        return models_multi, multi_class_strategy\n",
    "    else:\n",
    "        print(\"Invalid selection. Please choose 'single' or 'multi'.\")\n",
    "        return get_model_choice()  # Re-prompt until a valid choice is made\n",
    "\n",
    "models, multi_class_strategy = get_model_choice()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "# Function to wrap model in appropriate multi-class strategy\n",
    "def get_wrapped_model(model, strategy):\n",
    "    \"\"\"\n",
    "    Wraps the provided model in a multi-class strategy if specified.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : estimator object\n",
    "        The machine learning model to be wrapped in a multi-class strategy.\n",
    "    strategy : str or None\n",
    "        Multi-class strategy, either 'ovo' (one-vs-one) or 'ovr' (one-vs-rest).\n",
    "        If None or unsupported, returns the model unwrapped.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    estimator object\n",
    "        Model wrapped in the specified multi-class strategy, or the original model.\n",
    "    \"\"\"\n",
    "    if strategy == 'ovo':\n",
    "        return OneVsOneClassifier(model)\n",
    "    elif strategy == 'ovr':\n",
    "        return OneVsRestClassifier(model)\n",
    "    elif strategy is None:\n",
    "        return model  # No wrapping needed for single classification\n",
    "    else:\n",
    "        print(f\"Warning: Unsupported strategy '{strategy}'. Returning base model.\")\n",
    "        return model  # Fallback to base model for unsupported strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate binary classification models\n",
    "def evaluate_model_single(name, model, X_train, X_test, y_train, y_test, save_path=SAVE_PATH):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Check if model provides probability estimates\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.title(f'ROC Curve - {name}')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Precision-Recall Curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, label=f'Average Precision = {avg_precision:.2f}')\n",
    "        plt.title(f'Precision-Recall Curve - {name}')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        roc_auc = None\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Cross-Validation Mean Accuracy\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean_accuracy = cv_scores.mean()\n",
    "\n",
    "    # Print metrics in desired format\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\" if roc_auc is not None else \"ROC-AUC: N/A\")\n",
    "    print(f\"Cross-Validation Mean Accuracy: {cv_mean_accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Display Confusion Matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Save model and results\n",
    "    results = {\n",
    "        'Model': name, 'Accuracy': accuracy, 'Precision': precision,\n",
    "        'Recall': recall, 'F1-Score': f1, 'ROC-AUC': roc_auc,\n",
    "        'Cross-Validation Mean Accuracy': cv_mean_accuracy\n",
    "    }\n",
    "    if save_path:\n",
    "        joblib.dump(model, f\"{save_path}/{name}_model.pkl\")\n",
    "        joblib.dump(results, f\"{save_path}/{name}_results.pkl\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class classification evaluation function\n",
    "def evaluate_model_multi(name, model, X_train, X_test, y_train, y_test, save_path=SAVE_PATH, multi_class_strategy='ovr'):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model = get_wrapped_model(model, multi_class_strategy)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # ROC and PR Curves for each class\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        y_test_binarized = label_binarize(y_test, classes=range(len(set(y_test))))\n",
    "        n_classes = y_test_binarized.shape[1]\n",
    "\n",
    "        # ROC Curves for each class\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "            roc_auc = roc_auc_score(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.title(f'Multi-Class ROC Curve - {name}')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # PR Curves for each class\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for i in range(n_classes):\n",
    "            precision, recall, _ = precision_recall_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "            avg_precision = average_precision_score(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "            plt.plot(recall, precision, label=f'Class {i} (AP = {avg_precision:.2f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'Multi-Class Precision-Recall Curve - {name}')\n",
    "        plt.legend(loc='lower left')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Cross-Validation Mean Accuracy\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean_accuracy = cv_scores.mean()\n",
    "\n",
    "    # Print metrics in desired format\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\" if roc_auc is not None else \"ROC-AUC: N/A\")\n",
    "    print(f\"Cross-Validation Mean Accuracy: {cv_mean_accuracy:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Display Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Save model and results\n",
    "    results = {\n",
    "        'Model': name, 'Accuracy': accuracy, 'Precision': precision,\n",
    "        'Recall': recall, 'F1-Score': f1, 'ROC-AUC': roc_auc,\n",
    "        'Cross-Validation Mean Accuracy': cv_mean_accuracy\n",
    "    }\n",
    "    if save_path:\n",
    "        joblib.dump(model, f\"{save_path}/{name}_model.pkl\")\n",
    "        joblib.dump(results, f\"{save_path}/{name}_results.pkl\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Train and Test split integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overlap between train and test indices (should be empty)\n",
    "train_test_overlap = set(X_train.index).intersection(set(X_test.index))\n",
    "\n",
    "print(\"Overlap between train and test sets (should be empty):\")\n",
    "print(train_test_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify features are derived from target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple check to see if target-like columns are present in X\n",
    "target_like_columns = [col for col in X_train.columns if 'loan_status_grouped2_kn' in col or y_train.name in col]\n",
    "\n",
    "print(\"Columns potentially derived from target:\")\n",
    "print(target_like_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure correct handling of categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Fit TargetEncoder only on training data\n",
    "encoder = TargetEncoder()\n",
    "X_train_encoded = encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "# Apply to test data without refitting\n",
    "X_test_encoded = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect cross validation for leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Example pipeline for cross-validation\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Apply cross-validation using the pipeline, which prevents data leakage\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validation scores:\", cv_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've extracted, created, cleaned, and encoded our features. Time to run the machine learning models. I've tweaked the code to run several machine learning models since we're focusing on regression type models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluating models based on user selection with progress bar\n",
    "results = []\n",
    "if models is not None:\n",
    "    for name, model in tqdm(models.items(), desc=\"Evaluating models\"):\n",
    "        # Choose the correct evaluation function based on model type\n",
    "        if multi_class_strategy:\n",
    "            result = evaluate_model_multi(\n",
    "                name, model, X_train, X_test, y_train, y_test,\n",
    "                save_path=SAVE_PATH,\n",
    "                multi_class_strategy=multi_class_strategy  # Pass strategy to evaluation function\n",
    "            )\n",
    "        else:\n",
    "            result = evaluate_model_single(\n",
    "                name, model, X_train, X_test, y_train, y_test,\n",
    "                save_path=SAVE_PATH\n",
    "            )\n",
    "            \n",
    "        results.append(result)\n",
    "else:\n",
    "    print(\"Model evaluation was not performed due to invalid selection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model Run Conclusion:\n",
    "\n",
    "## Summary of Results:\n",
    "\n",
    "- **Best Recall**: Naive Bayes, though its low precision and high false-positive make it my third choice.\n",
    "- **Balanced Models**: Random Forest (#1) and Gradient Boosting (#2) offer the best balance between precision, recall, and AUC. I'd suggest working with the features to improve the scoring of these models.\n",
    "\n",
    "### Detail:\n",
    "\n",
    "#### 1. Logistic Regression:\n",
    "\n",
    "The Logistic Regression model achieves a good accuracy and AUC score, indicating some predictive capability. \n",
    "\n",
    "However, the low recall indicates it struggles to capture true positive loan defaults.\n",
    "\n",
    "#### 2. Decision Tree:\n",
    "\n",
    "The Decision Tree model performs lower than Logistic Regression in terms of AUC.\n",
    "\n",
    "But it has a better recall, implying it is better at identifying loan defaults compared to Logistic Regression.\n",
    "\n",
    "#### 3. Random Forest:\n",
    "\n",
    "Random Forest achieves the highest accuracy and AUC score, but with a lower recall. \n",
    "\n",
    "It is more balanced than Logistic Regression (both Precision and Accuracy in the 80s) but scores worse than the Decision Tree in identifying true positives (45 DT compared to 27 RF).\n",
    "\n",
    "#### 4. Gradient Boosting:\n",
    "\n",
    "Similar to Random Forest, Gradient Boosting has high accuracy and AUC but lower recall. \n",
    "\n",
    "#### 5. K-Nearest Neighbors (KNN):\n",
    "\n",
    "KNN has lower performance compared to the previous models, particularly struggling with recall and AUC.\n",
    "\n",
    "We can safely elminate this model.\n",
    "\n",
    "#### 6. Naive Bayes:\n",
    "\n",
    "Naive Bayes has a high recall, but very poor accuracy and precision. \n",
    "\n",
    "It identifies nearly all defaults, but with many false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing imbalance to improve our model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "We noticed earlier that the data set was largely skewed. Leading to an imbalence in our data set. Let's try and account for this using SMOTE and see if this improves our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Reduce k_neighbors to handle the small minority class\n",
    "smote = SMOTE(random_state=50, k_neighbors=3)\n",
    "\n",
    "# Resample the training data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Evaluating models based on user selection with progress bar\n",
    "results = []\n",
    "if models is not None:\n",
    "    for name, model in tqdm(models.items(), desc=\"Evaluating models\"):\n",
    "        # Choose the correct evaluation function based on model type\n",
    "        if multi_class_strategy:\n",
    "            result = evaluate_model_multi(\n",
    "                name, model, X_resampled, X_test, y_resampled, y_test,\n",
    "                save_path=SAVE_PATH,\n",
    "                multi_class_strategy=multi_class_strategy  # Pass strategy to evaluation function\n",
    "            )\n",
    "        else:\n",
    "            result = evaluate_model_single(\n",
    "                name, model, X_resampled, X_test, y_resampled, y_test,\n",
    "                save_path=SAVE_PATH\n",
    "            )\n",
    "            \n",
    "        results.append(result)\n",
    "else:\n",
    "    print(\"Model evaluation was not performed due to invalid selection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model Run Conclusion:\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "Overall reduced precision resulted in drastically better ROC results at the sacrifice of precision (avg. 0.20 pts).\n",
    "\n",
    "After applying SMOTE, the performance of most models improved in terms of recall, particularly:\n",
    "- Random Forest: improvements overall besides Precision which dropped a few points. ROC score improved from 0.27 to 0.43 equal to a 60% improvement.\n",
    "- Gradient Boosting: improvements overall besides Precision, Accuracy, and as a consequence ROC-AUC. ROC score improved from 0.28 to 0.49 equal to a 75% improvement.\n",
    "and Logistic Regression: improvements overall but suffered by a decrease in accuracy, and precision. This model improved its ROC score the most going from 0.09 to 0.63 after SMOTE equal to a 600% improvement.\n",
    "\n",
    "### Recommendations:\n",
    "1. **Random Forest** remains a top choice due to its overall balanced performance, strong ROC-AUC (0.8412), and improved recall after SMOTE.\n",
    "2. **Gradient Boosting** is another strong option, with a good F1-score and high ROC-AUC (0.8155), performing well across metrics.\n",
    "\n",
    "These two models are the ones I'd leverage be the primary algorithms selected for predicting loan defaults after applying SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cost of being wrong\n",
    "\n",
    "In attempt to understand the cost of error I've used a sample number of 1000 predictions. The question I'm asking myself is the following:\n",
    "\n",
    "How much is the cost of the time spent researching, execution, and closing false negatives, and false positives?\n",
    "\n",
    "I'll use Precision and Recall to calculate this. We'll also substitue a few numbers to simulate cost in minutes.\n",
    "\n",
    "We have the following information:\n",
    "\n",
    "- **Gradient Boosting:**\n",
    "    - Precsion (after SMOTE) 0.5523\n",
    "    - Recall (after SMOTE) 0.4991\n",
    "\n",
    "- **Random Forest:**\n",
    "    - Precsion (after SMOTE) 0.6833\n",
    "    - Recall (after SMOTE) 0.4325\n",
    "\n",
    "Expected number of defaults = Sum of defaults / Total data set =  12431 / 63689 = 0.1951 * 100 = 20%\n",
    "\n",
    "Using 1000 applicants or current loans as a base figure.\n",
    "\n",
    "1000 * 0.20 = 800 estimated non defaults, leaving us with 200 actual defaults estimated.\n",
    "\n",
    "- **Gradient Boosting:**\n",
    "    - False Positives = Precsion (after SMOTE) = (1 - 0.5523) * 800 =  358\n",
    "    - False Negatives = Recall (after SMOTE) = (1 - 0.4991) * 200 = 100\n",
    "    - Total of incorrect predictions 458\n",
    "\n",
    "- **Random Forest:**\n",
    "    - False Positives = Precsion (after SMOTE) = (1 - 0.6833) * 800 =  253\n",
    "    - False Negatives = Recall (after SMOTE) = (1 - 0.4325) * 200 = 114\n",
    "    - Total of incorrect predictions 367\n",
    "\n",
    "The cost of for each approach can be calculated by multiplying our results against the cost of a False Positive, and the cost of a False Negative.\n",
    "\n",
    "Let's assume the cost of a False Positive (revenue loss maybe denying a loan) equates to 2500, and the cost of a False Negtive (loss from a customer defaulting) equates to 5600.\n",
    "\n",
    "**Gradient Boosting** = (358 * 2500) + (100 * 5600) = 1,455,000\n",
    "**Random Forest** = (253 * 2500) + (114 * 5600) = 1,270,900\n",
    "\n",
    "In the above scenario with these very subjective numbers we'd be best opting for the Random Forest which has a lower cost of being wrong.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_infinity(df):\n",
    "    infinite_list = df.isin([-np.inf, np.inf]).sum()\n",
    "\n",
    "    if infinite_list.sum() == 0:\n",
    "        print(\"No column has infinite values\")\n",
    "    else:\n",
    "        print(\"Columns with infinite values:\")\n",
    "        print(infinite_list[infinite_list>0]).sort_values(ascending=False)\n",
    "\n",
    "check_infinity(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list = df_transformed.isna().sum()\n",
    "\n",
    "if nan_list.sum() == 0:\n",
    "    print(\"No column has NaN values\")\n",
    "else:\n",
    "    print(\"Columns with NaN values (sorted high to low):\")\n",
    "    print(nan_list[nan_list > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 'loan_default' is our target column\n",
    "X = df_transformed\n",
    "y = df_dropped['loan_status_grouped2_kn']\n",
    "X_columns = X.columns\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the target variable for both training and test sets\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=150, random_state=42) # sets the reproducability to 42\n",
    "rfe = RFE(estimator=rf, n_features_to_select=23, step=134, verbose=3) # here we're selecting 23 feautures and eleminate 3 features with every iteration of the RFE process.\n",
    "\n",
    "# Fit RFE on training data only to prevent data leakage\n",
    "rfe.fit(X_train, y_train)\n",
    "selected_features = X_train.columns[rfe.support_] # here we're creating a new list with the features the RFE process has captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've opted for a random forest classifier, but will evalute select kbest (from sklearn.feature_selection import SelectKBest, f_classif) at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top features\n",
    "#selected_features = X_train.columns[rfe.support_] # I've opted to keep this for memory purpose.\n",
    "selected_features_names = list(selected_features)\n",
    "\n",
    "print(\"Selected Features by RFE:\")\n",
    "print(f\"Index: {selected_features}\")\n",
    "print(f\"Column names: {selected_features_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list is the selected features based on the randomforestclassifier selected earlier. \n",
    "\n",
    "We've selected a total of **23 features** and created two new variables (**selected_features** and **selected_features_names**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rfe.support_)\n",
    "\n",
    "# Create a list of tuples with column name and boolean value\n",
    "feature_selection_results = list(zip(X_train.columns, rfe.support_))\n",
    "\n",
    "# outputs only true\n",
    "#print(\"Feature Selection Results:\")\n",
    "#for feature, selected in feature_selection_results:\n",
    "#    print(f\"{feature}: {selected}\")\n",
    "\n",
    "# outputs only false\n",
    "print(\"Features Not Selected:\")\n",
    "for feature, selected in feature_selection_results:\n",
    "    if not selected:  # Only print if 'selected' is False\n",
    "        print(f\"{feature}: {selected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see which columns were excluded by the randomForestClassifier. These are indicated by their boolean value True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running into a few errors I've inserted this point to check our data set lengths are the same. Essenitally flagging RFE setup errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train.columns))  # Number of columns in training data\n",
    "print(len(rfe.support_))  # Length of the boolean mask from RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(selected_features))\n",
    "print(len(selected_features_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a new test focusing only on the features selected in the RFE excercise. We're expecting better prcision and accuracy results and will tweak as needed using the RFE feature selection.\n",
    "\n",
    "In theory, these new features will generage better forecast capability, faster training through fewer features, and easier to interpret and trouble shoot models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our RFE selected features \"selected_features\" we'll rerun our randomforest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# Train a new Random Forest classifier on the selected features\n",
    "rf_selected = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "rf_selected.fit(X_train_selected, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next steps should now be focused on Evaluating performance, and Hyperparameter tuning. Let's start by verifying the results with the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "y_prob_selected = rf_selected.predict_proba(X_test_selected)[:, 1]\n",
    "\n",
    "# Print evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_selected)\n",
    "precision = precision_score(y_test, y_pred_selected)\n",
    "recall = recall_score(y_test, y_pred_selected)\n",
    "auc = roc_auc_score(y_test, y_prob_selected)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, AUC: {auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great results. Above we can see that our model predicts **86%** of the samples in our test data correctly. Our positive predictions are correct **82%** of the time. \n",
    "\n",
    "However, our Recall is still low with a score of **33%**.\n",
    "\n",
    "Our AUC scoring **85%** illustrating that it is capable of destinguising between positive and negative classes with a high level of certinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at these results with the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsif6utility import model_evaluation_report\n",
    "\n",
    "model_evaluation_report(X_test[selected_features], y_test, y_pred_selected, y_prob_selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main concern here is the low recall. The model seems to be too conservative. We have a few options here:\n",
    "\n",
    "- hypertuning our randomforest\n",
    "- try new models e.g. gradient boost, logistic regression.\n",
    "\n",
    "Let's circle back later on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "target = df_dropped['loan_status_grouped2_kn']\n",
    "flag_plot = False  # Don't show graphs\n",
    "\n",
    "for col in selected_features_names:\n",
    "    # Calculate correlation \n",
    "    correlation = df_transformed[[col, target]].corr().iloc[0, 1]\n",
    "        \n",
    "    print(f\"Correlation between {col} and {target}: {correlation:.2f}\")\n",
    "    \n",
    "    # Create scatter plot (if flag_plot is True)\n",
    "    if flag_plot:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        \n",
    "        # Use X_test_selected here\n",
    "        sns.scatterplot(x=X_test_selected[col], y=y_test)  \n",
    "\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(target)\n",
    "        plt.title(f\"Correlation: {correlation:.2f}\") \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data.\n",
    "\n",
    "**int_rate_int_clean_kn** stands out as being a lead contributor in the above results.\n",
    "\n",
    "We can observe modertate results from (<10pts - due to training size):\n",
    "\n",
    "**acc_open_past_24mths** implying that having more accounts opened could be condusive of a person or family taking on more debt and consequently more likely to default.\n",
    "\n",
    "**last_credit_pull_d_month_clean_kn** this implies that there is a possible linkage between timing of credit queries and default risk \n",
    "\n",
    "**dti** being our debt to load ratio is supprisingly low.\n",
    "\n",
    "At this piont it is important to remeber that correlation is not equal to causation and that our data set is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Value Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use SHAP to gain a better understanding of how our selected_features contribute to the machine learning outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretability and Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our data into a numpy array for faster calculation, and parallael calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = rf_selected.feature_importances_\n",
    "print(type(importances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print our numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the array there aren't any dominant or high features. The values, for the most part, are between 0.01 and 0.09. This implies that the model is dependant on multiple variables opposed to having a strong bias towards or smaller portion of variables.\n",
    "\n",
    "This said we can distinguish a handful of metrics contributing to higher weight (scores above 0.065). Let's visualise these against a bar chart to better understand their importance against each other. We'll sort the bar chart from hieghest to lowest.\n",
    "\n",
    "Plotting these value will help us determine which features to drop (e.g. lower than 0.02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the array in descending order of feature importance\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Ensure that col_labels matches the number of features being plotted\n",
    "col_labels = [selected_features[i] for i in indices[:X_test[selected_features].shape[1]]]  # Ensure the correct number of labels\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_test[selected_features].shape[1]), importances[indices[:X_test[selected_features].shape[1]]], align=\"center\")\n",
    "plt.xticks(ticks=range(X_test[selected_features].shape[1]), labels=col_labels, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, looking at the above diagram we notice that the last two features contribute the lowest to our model and that we could arguabgly remove these without negatively impacting our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a SHAP excercise to understand and visually reference how these features contribute to the overall predictive capabilities.\n",
    "\n",
    "Let's start by training our date using the selected features we identified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Sample only 100 rows from X_test for faster computation\n",
    "X_test_sample = X_test.sample(1000) \n",
    "\n",
    "# Compute SHAP values for X_test_sample\n",
    "rf_explainer = shap.TreeExplainer(rf_selected) # rf_selected is the randonforestclassifier\n",
    "rf_shap_values = rf_explainer.shap_values(X_test_sample)\n",
    "\n",
    "# Print the shape of the SHAP values to verify\n",
    "print(f'SHAP values shape: {rf_shap_values[1].shape}')  # This takes the positive values only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've had to reduce the sample due to computing power. In the above case I've kept the sample to 1k. This portion of the code takes just under 20 min. to run.\n",
    "\n",
    "Here we have positive and negative output which matches our expectations in terms of postive or negative loan default prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the SHAP javascript code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SHAP values shape: {rf_shap_values[1].shape}')\n",
    "print(f'X_test_sample shape: {X_test_sample.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize JS visualization for interactive plots\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our features and the sample against a force plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_index = 7\n",
    "\n",
    "# Step 1 - calculate expected value of positive class\n",
    "# This is baseline or average prediction of the model for the positive class before any specific features for an observation are considered\n",
    "expected_value_positive_class = rf_explainer.expected_value[1]\n",
    "print(\"expected_value_positive_class:\", expected_value_positive_class)\n",
    "\n",
    "# Step 2 - SHAP plot for a specific observation (e.g., the first row in the test set)\n",
    "shap.force_plot(expected_value_positive_class\n",
    "                ,  rf_shap_values[observation_index][:, 1] # shap_values_positive_class\n",
    "                , X_test.iloc[observation_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our base value (**0.1945**) represents the average prediction for a positive class, our output value in this case **0.21**.\n",
    "\n",
    "The red arrows push our predictions higher, and the blue push them lower. The length of the arrows illustrate their relevent importance across the feature sets i.e. mths_since_last_major_derog_missing_clean_kn contributes the strongest to the a negative prediction. \n",
    "\n",
    "In other words, if there aren't recent bad ratings (over a 90-day period), then the model will reduce the likelyhood of a loan default. This cumulated with the other RFE features will determine the individual True or False score per row i.e. highly likely or not likely to default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create a function to update SHAP visualizations based on selected observation\n",
    "def update_shap_plot(observation_index):\n",
    "    # Clear the previous output to avoid stacking multiple force plots\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Re-display the slider (since clearing the output also clears the slider)\n",
    "    display(observation_slider)\n",
    "    \n",
    "    feature_names = col_labels\n",
    "    \n",
    "    # Display SHAP force plot for the selected observation\n",
    "    display(shap.force_plot(\n",
    "            expected_value_positive_class\n",
    "                , rf_shap_values[observation_index][:, 1] \n",
    "                , X_test.iloc[observation_index]\n",
    "                , feature_names = feature_names\n",
    "           ))\n",
    "\n",
    "# Create a slider to select the observation (0 to len(X_test)-1)\n",
    "observation_slider = widgets.IntSlider(\n",
    "    value=0, min=0, max=len(X_test_sample)-1, step=1, description='Observation:', continuous_update=True)\n",
    "\n",
    "# Use the interactive function to update the SHAP plot when the slider value changes\n",
    "observation_slider.observe(lambda change: update_shap_plot(change['new']), names='value')\n",
    "\n",
    "expected_value_positive_class = rf_explainer.expected_value[1]\n",
    "print(\"expected_value_positive_class:\", expected_value_positive_class)\n",
    "\n",
    "# Display the slider and force plot\n",
    "display(observation_slider)\n",
    "\n",
    "# Call update_shap_plot to display the initial SHAP plot\n",
    "update_shap_plot(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we've reduced the length to match our sample variable.\n",
    "\n",
    "As a reminder our basline/average is **0.1945** anything above or below this value is considered a True or False default prediction.\n",
    "\n",
    "If we move our observation slider (or enter a value) e.g. **825** our model outputs a value of **0.23** indicating that this row of data is likely to default, the higher the value the more likely this is to occur. If we move our slider to index **390** we can observe an output score of **0.07** suggesting that this individual is unlikely to default on their loan as this is much lower than our baseline of **0.1945**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run our machine learning neural network challenger model with the RFE selected features. Before assigning our x and y I want to check the feature list is accurate and reflective of our RFE results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign your dataframes\n",
    "X_neural_net = df_transformed[selected_features]  # saved file from previous excercise. [selected_features] are the features selected by the RFE excercise using our pre selected features from assingment 2. In hindsight I'd probably ignore those and start from scratch using RFE to validate my findings against the total collection of columns within the enhanced data frame.\n",
    "y_neural_net = df_transformed['loan_status_grouped2_kn']\n",
    "\n",
    "# Optional: Encode target variable if necessary\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_neural_net)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_neural_net, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Challenger Models\n",
    "\n",
    "### Model 1: A simple neural network with 16, 8, and 1 neurons in three layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model1a = Sequential([\n",
    "    Dense(16, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1a.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history1 = model1a.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history1.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: A neural network with an additional 32-neuron layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model1b = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1b.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history1 = model1b.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history1.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: A more complex network with layers of 64, 32, 16, and 8 neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model1c = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1c.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history1 = model1c.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history1.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history1.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of our Basic Neural Network Model\n",
    "\n",
    "We've ran three basic neural networks to evaluate differnet levels of complexity. Our second model performed the best. **needs more input** we'll use the second nn for future excercises.\n",
    "\n",
    "- **Training Accuracy**: 0.8590\n",
    "- **Validation Accuracy**: 0.8417\n",
    "- **Training Loss**: 0.3470\n",
    "- **Validation Loss**: 0.3799"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations:**\n",
    "\n",
    "**Accuracy:** Both the training and validation accuracy are important to measure how well the model is performing on both the training data and unseen data.\n",
    "\n",
    "The third model has the highest training accuracy at 85.89%, followed by the second model at 85.01%, and the first model at 83.77%.\n",
    "For validation accuracy, the second and third models are tied at 84.49%, which is higher than the first model's 83.33%.\n",
    "Validation Loss: This metric indicates how well the model generalizes to unseen data, where a lower value is better.\n",
    "\n",
    "The second model has the lowest validation loss at 0.3781, followed by the third model at 0.3813, and the first model has the highest validation loss at 0.3914.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "The second model seems to perform the best, given that it has:\n",
    "\n",
    "- High validation accuracy (84.49%).\n",
    "- The lowest validation loss (0.3781), indicating the best generalization to unseen data.\n",
    "\n",
    "While the third model has a slightly higher training accuracy, the minimal difference in validation accuracy and higher validation loss compared to the second model suggests the second model is better optimized and generalizes better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Word of warning, I've played around with model parameters. To save you the hassle Model2b performed best**\n",
    "\n",
    "Apolgies in advance for the hours of life I've taken away from you reading this.\n",
    "\n",
    "#### Brief recap (better summary towards the end)\n",
    "\n",
    "The introduction of dropout, early stopping, and L2 regularisation demonstrated improvements in the neural network models' ability to generalise, with Models 2b and 2e outperforming the baseline ensemble models. Despite these gains, recall remains a challenge, highlighting the need for continued work on class imbalance and model sensitivity. Moving forward, these insights provide a solid foundation for enhancing model robustness and achieving better predictive outcomes for loan default classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Adding Dropout and Early Stopping\n",
    "\n",
    "To prevent overfitting we'll use dropout and early stopping techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model with Dropout layers\n",
    "model2a = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),  # Dropout layer\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model2a.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history2 = model2a.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history2.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy (Early Stopping)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model with Dropout layers\n",
    "model2b = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2),  # Dropout layer\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model2b.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history2 = model2b.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history2.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy (Early Stopping)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model with Dropout layers\n",
    "model2c = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.5),  # Dropout layer\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model2c.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history2 = model2c.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history2.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy (Early Stopping)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model2d = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(16, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model2d.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history2 = model2d.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history2.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy (Early Stopping)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model with Dropout layers\n",
    "model2e = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2),  # Dropout layer\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model2e.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history2 = model2e.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history2.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy (Early Stopping)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with Dropout and Early Stopping\n",
    "- **Training Accuracy**: 0.8088\n",
    "- **Validation Accuracy**: 0.8080\n",
    "- **Training Loss**: 0.4380\n",
    "- **Validation Loss**: 0.4267\n",
    "- **ROC-AUC**: 0.76\n",
    "\n",
    "**Training and Validation Observations**:\n",
    "- Early stopping helped mitigate overfitting, keeping training and validation accuracy close.\n",
    "- However, the ROC-AUC dropped to 0.76, lower than Gradient Boosting and Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization with L2\n",
    "\n",
    "Add L2 regularization to penalize large weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Define model with L2 regularization\n",
    "model3a = Sequential([\n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(8, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model3a.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "\n",
    "# Train the model\n",
    "history3 = model3a.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history3.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history3.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy (L2 Regularization)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model3b = Sequential([\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model3b.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall', 'AUC'])\n",
    "\n",
    "# Train the model\n",
    "history3 = model3b.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Plotting training vs validation performance\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "plt.plot(history3.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history3.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training vs Validation Accuracy (L2 Regularization)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network with L2 Regularization\n",
    "- **Training Accuracy**: 0.8117\n",
    "- **Validation Accuracy**: 0.8130\n",
    "- **Training Loss**: 0.4380\n",
    "- **Validation Loss**: 0.4417\n",
    "- **ROC-AUC**: 0.77\n",
    "\n",
    "**Training and Validation Observations**:\n",
    "- L2 Regularization kept training and validation accuracies well-matched.\n",
    "- ROC-AUC was 0.77, which is also lower compared to Random Forest and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Comparison\n",
    "- **Accuracy**: The Random Forest model provided the best accuracy (0.8506), followed by Gradient Boosting and the Basic Neural Network model.\n",
    "- **ROC-AUC**: The Random Forest model achieved the highest ROC-AUC (0.8412), followed by Gradient Boosting (0.8155). The basic neural network model came close (0.82).\n",
    "- **Regularization Impact**: Adding dropout with early stopping and L2 regularization helped prevent overfitting but decreased ROC-AUC compared to the Gradient Boosting and Random Forest models.\n",
    "- **Validation**: Validation accuracy and training accuracy closely match for the regularized models, indicating reduced overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set\n",
    "\n",
    "Calculate metrics for each model on the test data ready to plot in a ROC Curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Helper function to evaluate model on test data\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred_probs = model.predict(X_test).ravel()\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_probs)\n",
    "    \n",
    "    return accuracy, precision, recall, auc\n",
    "\n",
    "# Evaluate models\n",
    "models = [model1a, model1b, model1c, model2a, model2b, model2c, model2d, model2e, model3a, model3b]\n",
    "model_names = [\"Model1a - Basic NN\", \"Model1b - Basic NN\", \"Model1c - Basic NN\", \"Model2a - Dropout + Early Stopping\", \"Model2b - Dropout + Early Stopping\", \"Model2c - Dropout + Early Stopping\", \"Model2d - Dropout + Early Stopping\", \"Model2e - Dropout + Early Stopping\", \"Model3a - L2 Regularization\", \"Model3b - L2 Regularization\"]\n",
    "\n",
    "for name, model in zip(model_names, models):\n",
    "    accuracy, precision, recall, auc = evaluate_model(model, X_test_scaled, y_test)\n",
    "    print(f\"{name} - Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, AUC: {auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curves\n",
    "\n",
    "Plot the ROC curves for all models to compare their ability to distinguish between positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "\n",
    "for name, model in zip(model_names, models):\n",
    "    y_pred_probs = model.predict(X_test_scaled).ravel()\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc_score(y_test, y_pred_probs):.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Model Performance - ROC Curves\n",
    "\n",
    "The plot above shows the **ROC curves** for all models evaluated on the RFE feature selected data frame.\n",
    "\n",
    "**AUC (Area Under the Curve)** scores provided for each model wherby the best performing modesl are Model2b and Model2e both scoring 0.84:\n",
    "\n",
    "1. **Model1a, Model1b, Model1c - Basic Neural Networks**:\n",
    "   - These models achieved **AUC scores** of **0.82 to 0.83**, showing consistent performance across variations. They performed well in distinguishing between positive and negative classes, with relatively high TPR (True Positive Rate) and low FPR (False Positive Rate).\n",
    "\n",
    "2. **Model2a to Model2e - Dropout + Early Stopping**:\n",
    "   - These models varied in performance, with **AUC scores ranging from 0.73 to 0.84**. The best-performing Dropout + Early Stopping model (Model2b and Model2e) both achieved an **AUC of 0.84**, indicating strong predictive power.\n",
    "\n",
    "3. **Model3a and Model3b - L2 Regularization**:\n",
    "   - The **L2 Regularization** models had **AUC scores of 0.77 and 0.79**. These models were generally more consistent than some of the dropout variants, showing moderate performance compared to the best-performing Dropout + Early Stopping models.\n",
    "\n",
    "### Key Observations\n",
    "- **Dropout with Early Stopping** (Model2b and Model2e) produced the best results among all models, achieving AUC scores of **0.84**.\n",
    "- **Basic Neural Networks** performed consistently well, with **AUC values between 0.82 and 0.83**.\n",
    "- **L2 Regularisation** models showed stable but slightly lower performance compared to the other methods. I've dragged this on longer than I should and will explore the possible tweaks for L2 regularisation on my own time.\n",
    "\n",
    "Overall, **Dropout with Early Stopping** appears to be the most effective strategy in this case, providing a good balance between generalization and avoiding overfitting. Future optimization could focus on fine-tuning dropout rates and the regularization parameter to further improve model performance.\n",
    "\n",
    "If I were to chose one at this point it would be Model2b which has the higher recall from the two models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our previous notebook results and compare them against the neural network.\n",
    "\n",
    "The code to export ML resulsta was added to the assignment 2 code after submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load pre-trained models\n",
    "gradient_boosting_model = joblib.load('models/smote/Gradient_Boosting_model.pkl')\n",
    "random_forest_model = joblib.load('models/smote/Random_Forest_model.pkl')\n",
    "\n",
    "# Load the saved results\n",
    "gradient_boosting_results = joblib.load('models/smote/Gradient_Boosting_results.pkl')\n",
    "random_forest_results = joblib.load('models/smote/Random_Forest_results.pkl')\n",
    "\n",
    "print(gradient_boosting_results)\n",
    "print(random_forest_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mindful the previous models were trained on 54 features, and more rows of data. I managed to upscale and compare but wasn't sure if the results can be trusted and decided to remove them in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this assignment, we developed several neural network models to serve as challengers to the baseline models from Assignment 2, which included Random Forest and Gradient Boosting. By incorporating various regularisation techniques, such as dropout, early stopping, and L2 regularisation, we aimed to improve model generalisation, reduce overfitting, and increase predictive performance.\n",
    "\n",
    "## Summary of Results:\n",
    "\n",
    "The basic neural network models (Models 1a, 1b, 1c) demonstrated consistent accuracy and ROC-AUC scores, with Model 1b performing slightly better (AUC = 0.83).\n",
    "Dropout with Early Stopping proved to be an effective regularisation technique. Models 2b and 2e achieved the best AUC scores (0.84), outperforming the Random Forest baseline (AUC = 0.8412) and Gradient Boosting (AUC = 0.8155).\n",
    "L2 Regularisation models (Models 3a and 3b) provided moderate improvement in training stability, but their overall AUC scores (0.77 and 0.79) were slightly lower than other models.\n",
    "Validation accuracy was generally consistent with training accuracy for the regularised models, indicating reduced overfitting.\n",
    "\n",
    "## Key Observations:\n",
    "\n",
    "**Precision and Recall Trade-off:** While the models showed strong precision and accuracy, recall remained relatively low across all models. This suggests that, although the models are confident in their predictions, they struggle with correctly identifying positive instances (loan defaults).\n",
    "\n",
    "Models 2b and 2e emerged as the most effective challenger models, offering a good balance between precision, recall, and AUC, with strong generalisation.\n",
    "The ROC curves illustrated that Models 2b and 2e performed the best in distinguishing between positive and negative classes, showing improved predictive power compared to other models.\n",
    "\n",
    "## Future Improvements:\n",
    "\n",
    "**Class Imbalance:** The low recall suggests that the model may be biased towards the majority class. This is mostly due to the skewed data but also due to the fact that the hardware limited running the entire dataset.\n",
    "\n",
    "**Ensemble Methods:** Combining multiple models (e.g., ensembling neural networks with Gradient Boosting) may yield a more robust solution. I haven't figured out how to make this work efficiently just yet and will try to do this in the next assignment.\n",
    "\n",
    "**Feature Engineering:** Overall I'm happy with the feature selection but believe that it is possible to leverage additional data points, maybe increase our feature count from 23 to X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
