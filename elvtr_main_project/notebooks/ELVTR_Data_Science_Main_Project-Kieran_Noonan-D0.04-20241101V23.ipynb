{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELVTR Data Science Main Project\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "01 Git Repository\n",
    "\n",
    "Include all project code with a README file containing a high-level project description.\n",
    "\n",
    "Example README guide: [Make a README](link-to-readme-guide)\n",
    "\n",
    "Report\n",
    "\n",
    "* Methodology, approach, and model selection rationale.\n",
    "* Advantages and limitations of the chosen model.\n",
    "* Architecture of the final solution.\n",
    "* Considerations on deployment and scalability of the solution - i.e., how will the model be used in BAU by the business?\n",
    "* Estimated impact/ROI of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science in Finance: Lending Club Loan Analysis\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Lending Club has tasked us with preparing a loan application dataset for analysis and predictive modeling. \n",
    "\n",
    "The key tasks include data cleaning, exploratory data analysis, and building a predictive model for loan classification. An optional component involves building a real-time scoring application.\n",
    "\n",
    "**Project Objectives**:\n",
    "- Clean and preprocess the data.\n",
    "- Perform exploratory data analysis (EDA) to gain insights.\n",
    "- Develop a predictive model for loan application approval.\n",
    "- (Optional) Build a real-time scoring application.\n",
    "\n",
    "**Dataset Description**:\n",
    "The dataset consists of loan application records, including various financial metrics and the application status. The data dictionary is provided for understanding the attributes.\n",
    "\n",
    "**Dataset Path**:\n",
    "- CSV: `data/1-raw/lending-club-2007-2020Q3/Loan_status_2007-2020Q3-100ksample.csv`\n",
    "- Data Dictionary: `data/1-raw/lending-club-2007-2020Q3/LCDataDictionary.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data xlsx file as a dataframe\n",
    "df = pd.read_csv(\"c:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\GitHub\\\\dsif-git-main-project\\\\elvtr_main_project\\\\data\\\\raw\\\\Loan_status_2007-2020Q3\\\\Loan_status_2007-2020Q3-100k-Full-Data.csv\")\n",
    "\n",
    "# Clean headers in the existing DataFrame 'df'\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Display cleaned headers\n",
    "print(\"Cleaned headers:\", df.columns.tolist())\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data dictionary CSV file as a dataframe\n",
    "df_data_dict = pd.read_excel(\"c:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\GitHub\\\\dsif-git-main-project\\\\elvtr_main_project\\\\data\\\\raw\\\\Loan_status_2007-2020Q3\\\\LCDataDictionary.xlsx\")\n",
    "\n",
    "# Clean headers in the existing DataFrame 'df'\n",
    "df_data_dict.columns = df_data_dict.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Remove trailing whitespaces in all string columns of df_data_dict\n",
    "df_data_dict = df_data_dict.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "# Display cleaned headers\n",
    "print(\"Cleaned headers:\", df_data_dict.columns.tolist())\n",
    "\n",
    "df_data_dict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies the columns and descriptions from the data dictionary into a data frame for future recall.\n",
    "# Initialize empty lists for LoanStatNew and Description\n",
    "loanstatnew = []\n",
    "description = []\n",
    "\n",
    "# Iterate through each row in the DataFrame and populate lists\n",
    "for _, row in df_data_dict.iterrows():\n",
    "    loanstatnew.append(row['loanstatnew'])\n",
    "    description.append(row['description'])\n",
    "\n",
    "# Apply left-aligned styling to both headers and data cells\n",
    "styled_df_data_dict = df_data_dict.style.set_properties(\n",
    "    **{'text-align': 'left', 'white-space': 'nowrap'}\n",
    ").set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('text-align', 'left')]}]\n",
    ")\n",
    "\n",
    "# Display styled DataFrame\n",
    "styled_df_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing our data dictionary it is possible to class our columns into figurative categories to better organise our analysis. \n",
    "\n",
    "These categories are, for now:\n",
    "\n",
    "- Credit history, \n",
    "- Current Debt and Payment behaviours, \n",
    "- Employement, \n",
    "- Credit inquiries\n",
    "- Loan Application information\n",
    "- Hardship and Settlement Information\n",
    "- Co-Borrower Information\n",
    "- Loan Performance\n",
    "\n",
    "Let's create a table for reference. We'll add these manually so that we can tweak the data within each group as we discover more about our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Basic Data Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the information within our data frame (df) looking at our initial feature set (pre_hardship_fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains 143 columns and 99999 rows of data. It is comprised of numerical (float, int) and categorical data (object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've ran into an error (KeyError: \"['desc', 'member_id', 'verified_status_joint'] not in index\"), for the time being we'll remove the missing fields from df_data_dict. These are most likely naming issues i.e. member_id is most likely id but considering we won't be using this data for now we can remove it and correct later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract column headers from df and the first column of df_data_dict\n",
    "df_columns = set(df.columns)\n",
    "df_data_dict_columns = set(df_data_dict.iloc[:, 0])  # First column of df_data_dict\n",
    "\n",
    "# Find columns in df that are missing in df_data_dict\n",
    "missing_in_data_dict = df_columns - df_data_dict_columns\n",
    "\n",
    "# Find columns in df_data_dict that are missing in df\n",
    "missing_in_df = df_data_dict_columns - df_columns\n",
    "\n",
    "# Output the results\n",
    "print(\"Columns in df that are missing in df_data_dict:\")\n",
    "print(missing_in_data_dict)\n",
    "\n",
    "print(\"\\nColumns in df_data_dict that are missing in df:\")\n",
    "print(missing_in_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the values that are missing in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list was created to run analysis later on. I've opted to select the pre_hardship_fields as my feature selection draft\n",
    "# Subjective groupings created for post ML analysis. Due to the transformation these lists can only be used with df\n",
    "groups = {\n",
    "    \"Credit History\": [\n",
    "        'earliest_cr_line', 'fico_range_high', 'fico_range_low', 'last_fico_range_high',\n",
    "        'last_fico_range_low', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'num_accts_ever_120_pd',\n",
    "        'num_tl_120dpd_2m', 'pub_rec', 'pub_rec_bankruptcies'\n",
    "    ],\n",
    "    \"Current Debt and Payment Behaviors\": [\n",
    "        'acc_now_delinq', 'all_util', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths',\n",
    "        'collections_12_mths_ex_med', 'delinq_2yrs', 'delinq_amnt', 'max_bal_bc', 'mths_since_last_delinq',\n",
    "        'num_rev_accts', 'num_rev_tl_bal_gt_0', 'percent_bc_gt_75', 'revol_bal', 'revol_util',\n",
    "        'tot_coll_amt', 'tot_cur_bal', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit'\n",
    "    ],\n",
    "    \"Employment\": [\n",
    "        'emp_length', 'emp_title', 'annual_inc', 'annual_inc_joint'\n",
    "    ],\n",
    "    \"Credit Inquiries\": [\n",
    "        'inq_fi', 'inq_last_12m', 'inq_last_6mths', 'num_tl_op_past_12m'\n",
    "    ],\n",
    "    \"Loan Application Information\": [\n",
    "        'loan_amnt', 'term', 'int_rate', 'application_type', 'grade', 'sub_grade', 'purpose',\n",
    "        'issue_d', 'home_ownership', 'zip_code', 'addr_state', 'title', 'desc', 'url'\n",
    "    ],\n",
    "    \"Hardship and Settlement Information\": [\n",
    "        'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', 'hardship_start_date',\n",
    "        'hardship_end_date', 'hardship_amount', 'hardship_length', 'settlement_status', 'settlement_date',\n",
    "        'settlement_amount', 'settlement_percentage', 'settlement_term'\n",
    "    ],\n",
    "    \"Co-Borrower Information\": [\n",
    "        'annual_inc_joint', 'dti_joint', 'verified_status_joint', 'revol_bal_joint',\n",
    "        'sec_app_fico_range_low', 'sec_app_fico_range_high', 'sec_app_earliest_cr_line',\n",
    "        'sec_app_mort_acc', 'sec_app_open_acc', 'sec_app_revol_util'\n",
    "    ],\n",
    "    \"Loan Performance\": [\n",
    "        'funded_amnt', 'funded_amnt_inv', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
    "        'total_pymnt_inv', 'total_rec_int', 'total_rec_late_fee', 'total_rec_prncp', 'recoveries',\n",
    "        'collection_recovery_fee', 'last_pymnt_amnt', 'last_pymnt_d', 'next_pymnt_d'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Define pre and post hardship fields\n",
    "pre_hardship_fields = [\n",
    "    'acc_now_delinq', 'acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc', \n",
    "    'annual_inc_joint', 'application_type', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', \n",
    "    'chargeoff_within_12_mths', 'collections_12_mths_ex_med', 'delinq_2yrs', 'delinq_amnt', \n",
    "    'dti', 'dti_joint', 'earliest_cr_line', 'emp_length', 'emp_title', \n",
    "    'fico_range_high', 'fico_range_low', 'funded_amnt', 'funded_amnt_inv', 'grade', \n",
    "    'home_ownership', 'il_util', 'initial_list_status', 'inq_fi', 'inq_last_12m', \n",
    "    'inq_last_6mths', 'installment', 'int_rate', 'issue_d', 'loan_amnt', 'loan_status', \n",
    "    'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', \n",
    "    'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_last_delinq', \n",
    "    'mths_since_last_major_derog', 'mths_since_last_record', 'mths_since_rcnt_il', \n",
    "    'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq', \n",
    "    'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', \n",
    "    'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', \n",
    "    'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', \n",
    "    'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'open_acc', 'open_acc_6m', 'open_il_12m', \n",
    "    'open_il_24m', 'open_act_il', 'open_rv_12m', 'open_rv_24m', 'out_prncp', \n",
    "    'out_prncp_inv', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'policy_code', 'pub_rec', \n",
    "    'pub_rec_bankruptcies', 'purpose', 'pymnt_plan', 'revol_bal', 'revol_util', \n",
    "    'sub_grade', 'tax_liens', 'term', 'title', 'tot_coll_amt', 'tot_cur_bal', \n",
    "    'tot_hi_cred_lim', 'total_acc', 'total_bal_ex_mort', 'total_bal_il', 'total_bc_limit', \n",
    "    'total_cu_tl', 'total_il_high_credit_limit', 'total_pymnt', 'total_pymnt_inv', \n",
    "    'total_rec_int', 'total_rec_late_fee', 'total_rec_prncp', 'total_rev_hi_lim', \n",
    "    'verification_status', 'zip_code'\n",
    "]\n",
    "\n",
    "post_hardship_fields = [\n",
    "    'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', \n",
    "    'hardship_start_date', 'hardship_end_date', 'hardship_amount', 'hardship_length', \n",
    "    'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', \n",
    "    'hardship_payoff_balance_amount', 'hardship_last_payment_amount', 'disbursement_method', \n",
    "    'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date', \n",
    "    'settlement_amount', 'settlement_percentage', 'settlement_term'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pre_hardship_fields to only include values that exist as columns in df\n",
    "pre_hardship_fields_clean_kn = [field for field in pre_hardship_fields if field in df.columns]\n",
    "\n",
    "# Display the filtered list\n",
    "print(\"Filtered pre_hardship_fields:\", pre_hardship_fields_clean_kn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[pre_hardship_fields_clean_kn].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[pre_hardship_fields_clean_kn].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to collect cross-tab data\n",
    "cross_tab_data = []\n",
    "\n",
    "# Iterate over each group and each field to determine availability\n",
    "for group, fields in groups.items():\n",
    "    for field in fields:\n",
    "        pre = 'Yes' if field in pre_hardship_fields else 'No'\n",
    "        post = 'Yes' if field in post_hardship_fields else 'No'\n",
    "        cross_tab_data.append([group, field, pre, post])\n",
    "\n",
    "# Create DataFrame for cross-tab\n",
    "cross_tab_df = pd.DataFrame(cross_tab_data, columns=[\"Group\", \"Field\", \"Pre-Hardship\", \"Post-Hardship\"])\n",
    "\n",
    "# Apply styling to left-align specific columns and set table header alignment\n",
    "cross_tab_df_styled = cross_tab_df.style.set_properties(\n",
    "    subset=['Group', 'Field'],\n",
    "    **{'text-align': 'left'}\n",
    ").set_table_styles(\n",
    "    [{'selector': 'th.col_heading.level0', 'props': [('text-align', 'left')]}]\n",
    ")\n",
    "\n",
    "# Display the styled DataFrame\n",
    "cross_tab_df_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because are scoring is supposed to identify good and bad payers we'll leverage Pre-Hardship feature list for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the employtment mapping CSV file as a dataframe\n",
    "df_emp_title = pd.read_csv(\"c:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\GitHub\\\\dsif-git-main-project\\\\elvtr_main_project\\\\data\\\\raw\\\\emp_title_mapping.csv\")\n",
    "\n",
    "# Clean headers in the existing DataFrame\n",
    "df_emp_title.columns = df_emp_title.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "# Display cleaned headers\n",
    "print(\"Cleaned headers:\", df_emp_title.columns.tolist())\n",
    "\n",
    "df_emp_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for LoanStatNew and Description\n",
    "jobtitle = []\n",
    "job_category = []\n",
    "\n",
    "# Iterate through each row in the DataFrame and populate lists\n",
    "for _, row in df_emp_title.iterrows():\n",
    "    jobtitle.append(row['job_title'])\n",
    "    job_category.append(row['category'])\n",
    "\n",
    "# Apply left-aligned styling to both headers and data cells\n",
    "styled_df_emp_title = df_emp_title.style.set_properties(\n",
    "    **{'text-align': 'left', 'white-space': 'nowrap'}\n",
    ").set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('text-align', 'left')]}]\n",
    ")\n",
    "\n",
    "# Display styled DataFrame\n",
    "styled_df_emp_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross table on 'category' in df_emp_title\n",
    "category_crosstab = pd.crosstab(index=df_emp_title['category'], columns='count').sort_values(by='count', ascending=False)\n",
    "\n",
    "# Display the crosstab\n",
    "category_crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our libraries and configure any paramaters for charting later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for data manipulation, statistics, and visualization\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest, shapiro, anderson, kstest, skew\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.express as plt  # For interactive plots\n",
    "import seaborn as sns  # For static plots with themes\n",
    "import matplotlib.pyplot as plt  # For standard plotting\n",
    "\n",
    "# Missing data visualization\n",
    "import missingno as msno\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression  # Logistic and Linear Regression models\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor  # Decision Tree Classifier and Regressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor  # Ensemble models\n",
    "from sklearn.svm import SVC, SVR  # Support Vector Machines (SVC for classification, SVR for regression)\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor  # K-Nearest Neighbors models\n",
    "from sklearn.naive_bayes import GaussianNB  # Naive Bayes Classifier\n",
    "from sklearn.cluster import KMeans  # K-Means clustering\n",
    "from sklearn.decomposition import PCA  # Principal Component Analysis for dimensionality reduction\n",
    "\n",
    "# Pandas display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Plot settings for consistent figure size (A4 landscape top half)\n",
    "FIG_WIDTH = 11.69  # Width\n",
    "FIG_HEIGHT = 4.14  # Height\n",
    "\n",
    "# Set the theme for Seaborn plots\n",
    "sns.set_theme(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Data Overview\n",
    "\n",
    "Before performing any analysis, we will explore the structure of the dataset to understand the nature of the available data. This includes checking the number of rows and columns, the data types of each feature, and identifying any missing values. Understanding these characteristics is essential for guiding data cleaning and feature engineering steps later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[pre_hardship_fields].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data frame contains 100k rows, 104 columns.\n",
    "\n",
    "Our Data set contains int, float, and string objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check some basic statistics against all int and float data in our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistics only for int and float columns\n",
    "df[pre_hardship_fields].describe(include=['int', 'float'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ignore the id columns, and will drop these as part of our basic data cleaning.\n",
    "\n",
    "### High level analysis\n",
    "\n",
    "Scrolling from left to right we can make the following observations:\n",
    "\n",
    "- The Standard deviation for `annual_inc` is > 87k suggesting large disparity in numbers (we'll check this later on when pulling our distribution plots)\n",
    "- The average `open_acc` is equal to 11 with a high of up to 86. For UK standards this can be considered extremely high. Worth taking this into account as a feature for our deeper analysis.\n",
    "- Our delinquency fields show that we have a low average in the `delinq_2yrs` column and an average of 35 months since the last delinquency (`mths_since_last_delinq`) these could be a great indicators. \n",
    "\n",
    "Althought there are more lets continue our analysis and feature selection for our machine learning excercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "### Checking for Missing Values\n",
    "To ensure data integrity, we check for missing values in the dataset. The `isnull()` function is used to identify null entries, and the results are sorted by the number of missing values per column. This provides insight into the columns with the most missing data, which could impact our analysis and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "missing_values = df[pre_hardship_fields].isnull().sum().sort_values(ascending=False)\n",
    "print(f\"There is a total of: {len(missing_values)} columns that are missing data\\n\")\n",
    "# print(\"\\nMissing values in each column:\\n\") \n",
    "# print(missing_values[missing_values > 90000]) # Display only columns with missing values\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data Strategy\n",
    "Instead of dropping columns with a high number of missing values, we may want to retain them for our analysis. Following the logic of Abraham Wald's famous airplane and bullet holes approach, it could be beneficial to analyse the data we don't have rather than discard potentially useful columns. \n",
    "\n",
    "This is especially relevant for improving loan default predictions as the absence of data in itself is indicative of the possible risk to default on loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirming DataFrame Shape\n",
    "\n",
    "After exploring the missing values, we validate the shape of our dataframe to ensure that the dataset remains unchanged. \n",
    "\n",
    "This step helps confirm that we are still working with the full set of features and rows taking into account we've already stripped the first 3 columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[pre_hardship_fields].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the `missingno` library to analyze the missing data. This will help us understand how missing data is distributed across the dataset and the correlation between missing values in different columns.\n",
    "\n",
    "The `missing_values` variable has been defined earlier in our workflow to quantify the total number of missing entries in each column. \n",
    "\n",
    "Now, we'll leverage this list to visualize the missing data using  missingno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for columns with missing values greater than 0\n",
    "missing_values_graph = df[missing_values[missing_values > 0].index]\n",
    "\n",
    "print(\"Categorical Data Missing Values\\n\")\n",
    "\n",
    "# Visualize the missing data using the missingno library\n",
    "msno.matrix(missing_values_graph)\n",
    "msno.bar(missing_values_graph)\n",
    "msno.heatmap(missing_values_graph)\n",
    "# msno.dendrogram(missing_values_graph) #removed for the final anlysis to avoid cluttering the document with the same data but a different way to show it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of our missing data findings\n",
    "\n",
    "#### 1. Missing Data Matrix Plot:\n",
    "- The matrix plot visualizes the distribution of missing data across the dataset columns.\n",
    "- Some columns have no missing data, but a few are significantly affected.\n",
    "- Several columns have a substantial percentage of missing data, some exceeding 75%.\n",
    "- Key columns with high missing data include:\n",
    "  - `hardship_*` related fields, `payment_plan_start_date`, `sec_app_*` fields, etc.\n",
    "\n",
    "#### 2. Missing Data Heatmap:\n",
    "- The heatmap shows correlations between columns with missing data.\n",
    "- Higher intensity colors indicate stronger correlations.\n",
    "- Examples include `emp_title`, `mths_since_last_major_derog`, and `revol_util` showing linked missingness.\n",
    "- Strong correlations exist between certain groups of columns, suggesting shared patterns in their missing values.\n",
    "- We can clearly distinguish 4 groups. The three largest relating to `hardship_`, `sec_`, and `Acc` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Missing Value Indicators\n",
    "\n",
    "Opposed to solomly removing values and using collected data to predict loan defaults I will create indicator variables that flag whether a value was missing for a given feature. \n",
    "\n",
    "This allows us to retain missing values while also capturing information about whether a data point was reported or not, which could enhance our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new list by selecting specific groups from the logical mapping we created earlier\n",
    "selected_groups = [\"Credit History\", \"Employment\", \"Credit Inquiries\"] # created after reading the data dictionary\n",
    "missing_value_indicator = sum([groups[group] for group in selected_groups], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(missing_value_indicator)) # collection of columns from our grouping whilst reading the data dictionary\n",
    "print(len(pre_hardship_fields)) # the columns that are most likely pre any loan defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our draft list of features (`draft_features`) now includes 122 columns identified during the analysis of the missing data, and the basic statistics from `df` along with the information within the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection of initial features based on our previous findings and the descriptions within the data dictionary\n",
    "new_features = []\n",
    "new_features.extend(pre_hardship_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expand `df` with new columns to keep a record of the missingness values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store the new column names\n",
    "new_missing_columns = []\n",
    "\n",
    "# Iterating through the list to create missing value indicator columns\n",
    "for col in missing_value_indicator:\n",
    "    indicator_col_name = f\"{col}_missing_clean_kn\"  # Create a new column name for the missing indicator\n",
    "    df[indicator_col_name] = df[col].isnull().astype(int)  # 1 for missing, 0 for not missing\n",
    "    \n",
    "    # Append the new column name to the list\n",
    "    new_missing_columns.append(indicator_col_name)\n",
    "\n",
    "# Display the list of new column names\n",
    "print(\"New missing indicator columns:\", new_missing_columns)\n",
    "print(\"Count of New Missing Columns created\", len(new_missing_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to verify the new columns\n",
    "df[new_missing_columns].head()  # Display the first few rows to verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the new items to the existing list\n",
    "new_features.extend(new_missing_columns)\n",
    "len(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for uniqueness in our feature list.\n",
    "\n",
    "Here we're looking for variability i.e. which features contain the most variability. The below code will provide us with a list of unique value counts for each feature within our data set, ordered in decending order, and with a % that reflects the uniqueness i.e. if we have 100k unique values (id field) then the % uniqueness will be 100%. This little variability will not add value in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_uniqueness(df):\n",
    "    \"\"\"\n",
    "    Analyzes the number of unique values in all columns within a DataFrame, \n",
    "    regardless of their data type, and helps identify suitable candidates for visualization.\n",
    "\n",
    "    Args:\n",
    "        df: The Pandas DataFrame to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame with columns, unique value counts, and percentages.\n",
    "        Also, displays a bar graph for the unique counts sorted from highest to lowest.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Calculate unique counts and percentages and store them in a DataFrame\n",
    "    uniqueness_df = pd.DataFrame({\n",
    "        \"Unique Count\": df.nunique(),\n",
    "        \"Unique Percentage\": df.nunique() / total_rows * 100\n",
    "    }).sort_values(by=\"Unique Count\", ascending=False)\n",
    "\n",
    "    # Print sorted output with unique counts and percentages\n",
    "    print(uniqueness_df)\n",
    "\n",
    "    # Create a bar graph to visualize the number of unique values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(uniqueness_df.index, uniqueness_df[\"Unique Count\"])\n",
    "    plt.xlabel(\"Columns\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Unique Values\", fontsize=14)\n",
    "    plt.title(\"Unique Value Counts per Column (Sorted)\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)  # Rotate x-axis labels for readability\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return uniqueness_df\n",
    "\n",
    "# Example usage with pre-selected columns\n",
    "# Assuming df and pre_hardship_fields are defined and valid\n",
    "#try:\n",
    "#    unique_counts_df = analyze_uniqueness(df[pre_hardship_fields])\n",
    "#except ValueError as e:\n",
    "#    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    unique_counts_df = analyze_uniqueness(df[new_features])\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see columns with 100% and 0% variability i.e. every row is unique or contains the same number. This is typical of unique identifiers and in our case policy codes. We can remove these as they won't be of any use to us for further analysis.\n",
    "\n",
    "To simplify our data we'll remove 100%, 0%, and anythin less than 5%:\n",
    "\n",
    "- 100% of values are unique = `id`, `url`\n",
    "- 0% of values are unique = `policy_code`, `pymnt_plan`\n",
    "\n",
    "We'll also take this opportunity to remove `unnamed:_0.1`, `unnamed:_0` too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['policy_code', 'pymnt_plan']\n",
    "\n",
    "# Remove specified columns from pre_hardship_fields_clean\n",
    "new_features = [col for col in new_features if col not in columns_to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our feature list has been updated with the new columns. This means that for each column with missing values, a new binary indicator column is created, where `1` represents a missing value and `0` indicates that a value was present. \n",
    "\n",
    "This approach ensures that we retain as much information as possible from the original dataset while also capturing the fact that missing values themselves may provide valuable insight. For instance, missing income information could be an indicator of a higher default risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining our target variable and applying simplify the `loan_status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logical groupings for 'loan_status'\n",
    "loan_status_groupings = {\n",
    "    'Fully Paid': 'Paid Loan',\n",
    "    'Current': 'Active Loan',\n",
    "    'Charged Off': 'Defaulted Loan',\n",
    "    'Late (31-120 days)': 'Late Loan',\n",
    "    'In Grace Period': 'Late Loan',\n",
    "    'Late (16-30 days)': 'Late Loan',\n",
    "    'Does not meet the credit policy. Status:Fully Paid': 'Paid Loan',\n",
    "    'Issued': 'Active Loan',\n",
    "    'Does not meet the credit policy. Status:Charged Off': 'Defaulted Loan',\n",
    "    'Default': 'Defaulted Loan'\n",
    "}\n",
    "\n",
    "# Apply the grouping to the 'loan_status' column\n",
    "df['loan_status_grouped_kn'] = df['loan_status'].replace(loan_status_groupings)\n",
    "\n",
    "# Verify the groupings\n",
    "print(df['loan_status_grouped_kn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['loan_status_grouped_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dependant variable\n",
    "\n",
    "Our dependant veriable is `loan_status_grouped_kn` further work is to understand how best to predict good and bad loan applicants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_status_grouped_kn'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = ['loan_status_grouped_kn']\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"Both columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df[new_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our data frame to see what we're working with. Note our most recent data frame is now df_dropped and no longer df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) on Missing Values\n",
    "\n",
    "To understand the impact of missing values on our target variable (`loan_status`), we perform an exploratory analysis. \n",
    "\n",
    "This compares the distribution of loan status between rows where key variables are missing and where they are not. By doing this, we hope to detect which missing data is associated with loan outcomes. \n",
    "\n",
    "This helps us understand how big a influence on the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that have missing values in df_dropped\n",
    "missing_cols = df_dropped.columns[df_dropped.isnull().any()]\n",
    "\n",
    "# Store results for plotting\n",
    "missing_dict = {}\n",
    "not_missing_dict = {}\n",
    "\n",
    "# Function to collect percentages for missing and non-missing data\n",
    "def missing_value_analysis(column, target_column='loan_status_grouped_kn'):\n",
    "    missing = df_dropped[df_dropped[column].isnull()][target_column].value_counts(normalize=True) * 100\n",
    "    not_missing = df_dropped[df_dropped[column].notnull()][target_column].value_counts(normalize=True) * 100\n",
    "    missing_dict[column] = missing\n",
    "    not_missing_dict[column] = not_missing\n",
    "\n",
    "# Apply the function for all columns with missing data\n",
    "for col in missing_cols:\n",
    "    missing_value_analysis(col)\n",
    "\n",
    "# Create DataFrames for heatmaps\n",
    "missing_df = pd.DataFrame(missing_dict).fillna(0)  # Fill NaN with 0 to ensure proper heatmap display\n",
    "not_missing_df = pd.DataFrame(not_missing_dict).fillna(0)\n",
    "\n",
    "# Plotting heatmaps one below the other\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 16), gridspec_kw={'height_ratios': [1, 1]})  # Adjust aspect ratio\n",
    "\n",
    "# Heatmap for missing data\n",
    "sns.heatmap(missing_df, annot=False, cmap=\"Blues\", ax=ax[0], cbar_kws={\"shrink\": .75})\n",
    "ax[0].set_title('Percentage of Loan Status for Missing Data')\n",
    "ax[0].tick_params(axis='x', rotation=90, labelsize=10)  # Rotate x-axis labels for readability\n",
    "ax[0].tick_params(axis='y', labelsize=10)  # Adjust y-axis label size\n",
    "\n",
    "# Heatmap for non-missing data\n",
    "sns.heatmap(not_missing_df, annot=False, cmap=\"Greens\", ax=ax[1], cbar_kws={\"shrink\": .75})\n",
    "ax[1].set_title('Percentage of Loan Status for Non-Missing Data')\n",
    "ax[1].tick_params(axis='x', rotation=90, labelsize=10)  # Rotate x-axis labels for readability\n",
    "ax[1].tick_params(axis='y', labelsize=10)  # Adjust y-axis label size\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Missing and Non-Missing Data by Loan Status\n",
    "\n",
    "#### 1. Percentage of Missing Data (First Image)\n",
    "- **Paid Loan**: Shows a high percentage of missing data across numerous columns, particularly towards the end of the list (indicated by darker shades of blue).\n",
    "- **Active Loan** and **Late Loan**: Generally have less missing data, though some specific columns still contain notable missing values.\n",
    "- **Defaulted Loan**: Contains fewer missing values compared to `Paid Loan`, but some columns still show missing data.\n",
    "\n",
    "#### 2. Percentage of Non-Missing Data (Second Image)\n",
    "- **Paid Loan**: Displays lower percentages of non-missing data across various columns, aligning with the missing data heatmap (darker greens represent more missing data).\n",
    "- **Active Loan** and **Late Loan**: Feature higher percentages of non-missing data in most columns, shown by lighter green shades, suggesting more complete data.\n",
    "- **Defaulted Loan**: Generally has a higher percentage of non-missing data than `Paid Loan`, but less than `Active Loan` and `Late Loan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Cleaning\n",
    "\n",
    "Perform thorough data cleaning on the provided dataset, including but not limited to the following steps:\n",
    "\n",
    "* Handling missing values (imputation or removal)\n",
    "* Converting data types to appropriate formats\n",
    "* Removing duplicate records\n",
    "* Detecting and handling outliers\n",
    "* Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross table of data types in df\n",
    "dtype_crosstab = df_dropped.dtypes.value_counts().reset_index()\n",
    "dtype_crosstab.columns = ['Data Type', 'Count']\n",
    "\n",
    "# Display the cross table\n",
    "dtype_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct references to 'completed' to 'complete' in the 'hardship_status' column\n",
    "df_dropped['hardship_status_kn'] = df['hardship_status'].replace('COMPLETED', 'COMPLETE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend('hardship_status_kn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Feature Selection\n",
    "\n",
    "Based on our initial quick analysis of the data and the data dictionary, we'll now work on the shortlisted features for further exploration in our machine learning model. These features were selected based on their relevant at the time of taking out a loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and list columns with 'object' data types from our selected features\n",
    "# This helps us identify which columns require encoding or conversions before modeling\n",
    "df_dropped[new_features].select_dtypes(include=['object']).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[new_features].select_dtypes(include=['object']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List split\n",
    "\n",
    "Let's take our list of features we'd like to review for machine learning and split them into 3 seperate lists (boolean, numerical, and categorical) for further analysis.\n",
    "\n",
    "In this next phase I want to explore the feature selection for multicollinearity and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_frame(features_list, df):\n",
    "    \"\"\"\n",
    "    Splits the provided DataFrame into three lists containing Boolean, Numerical, and Categorical column names.\n",
    "    Converts floats with trailing zeros into integers and replaces NaN values with 0 for integers, 0.00 for floats.\n",
    "\n",
    "    Parameters:\n",
    "    features_list (list): List of column names to be checked.\n",
    "    df (pd.DataFrame): The input DataFrame to split.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing three lists (boolean_cols, numerical_cols, categorical_cols).\n",
    "    \"\"\"\n",
    "    boolean_cols = []\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "\n",
    "    # Define acceptable boolean values\n",
    "    acceptable_boolean_values = {0, 1, True, False, 0.0, 1.0}\n",
    "\n",
    "    for col in features_list:\n",
    "        # Treat each column explicitly as a Series\n",
    "        column_series = df[col]\n",
    "\n",
    "        # Handle cases where columns might be interpreted incorrectly\n",
    "        if pd.api.types.is_bool_dtype(column_series) or all(column_series.dropna().isin(acceptable_boolean_values)):\n",
    "            boolean_cols.append(col)\n",
    "        elif pd.api.types.is_numeric_dtype(column_series):\n",
    "            # Check for floats with trailing zeros\n",
    "            if column_series.dtype == 'float64':\n",
    "                # Check if all float values are equivalent to integers\n",
    "                if all(column_series.dropna() == column_series.dropna().astype(int)):\n",
    "                    df[col] = column_series.fillna(0).astype(int)  # Replace NaNs with 0 and convert to int\n",
    "                else:\n",
    "                    df[col] = column_series.fillna(0.00)  # Replace NaNs with 0.00 for floats\n",
    "                numerical_cols.append(col)\n",
    "            else:\n",
    "                df[col] = column_series.fillna(0)  # Replace NaNs with 0 for integers\n",
    "                numerical_cols.append(col)\n",
    "        else:\n",
    "            categorical_cols.append(col)\n",
    "    \n",
    "    # Print a summary of the count of columns in each list\n",
    "    print(f\"Summary of column counts:\")\n",
    "    print(f\"boolean_list contains {len(boolean_cols)} values\")\n",
    "    print(f\"numerical_list contains {len(numerical_cols)} values\")\n",
    "    print(f\"categorical_list contains {len(categorical_cols)} values\")\n",
    "    print(f\"The feature list we'll be working with contains {df_dropped[new_features].shape} rows and columns.\")\n",
    "\n",
    "    return boolean_cols, numerical_cols, categorical_cols\n",
    "\n",
    "# Instructions:\n",
    "# this calls the split_data_frame function above create three lists to capture the sorting outputs in. \n",
    "# These will later be used to pull some graphs to evaluate the data and what possible transformations we've missed.\n",
    "# boolean_list, numerical_list, categorical_list = split_data_frame(new_features, df_dropped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which columns are in df_dropped\n",
    "missing_columns = [col for col in new_features if col not in df_dropped.columns]\n",
    "\n",
    "print(\"Missing columns:\", missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this calls the split_data_frame function above create three lists to capture the sorting outputs in. \n",
    "# These will later be used to pull some graphs to evaluate the data and what possible transformations we've missed.\n",
    "# Our function parameters are list and the latest version of our data frame in this caes new_features and df_dropped accordingly.\n",
    "boolean_list, numerical_list, categorical_list = split_data_frame(new_features, df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[categorical_list].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[numerical_list].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[boolean_list].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collowing three sections are the code snippets we'll use to analyse our data (Numerical, Boolean, Categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_numeric_columns(numeric_cols, dataframe):\n",
    "    \"\"\"\n",
    "    Analyze and visualize numeric columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    numeric_cols (list): List of numeric column names to analyze.\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    for column in numeric_cols:\n",
    "        print(f\"\\nSummary Statistics and Analysis for Numeric Column: {column}\")\n",
    "\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if column not in dataframe.columns:\n",
    "            print(f\" '{column}' is not found in the DataFrame. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Ensure the column is converted to numeric\n",
    "        dataframe.loc[:, column] = pd.to_numeric(dataframe[column], errors='coerce')\n",
    "\n",
    "        # Drop NaN values to ensure we have numeric data for analysis\n",
    "        numeric_data = dataframe[column].dropna()\n",
    "\n",
    "        if numeric_data.empty:\n",
    "            print(f\"No valid numeric data available for column: {column}. Skipping...\")\n",
    "            continue  # Skip the column if there's no valid data\n",
    "\n",
    "        # Calculate z-scores\n",
    "        z_scores = (numeric_data - numeric_data.mean()) / numeric_data.std()\n",
    "\n",
    "        # Calculate IQR\n",
    "        Q1 = numeric_data.quantile(0.25)\n",
    "        Q3 = numeric_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Identify outliers based on z-scores (z > 3 or z < -3)\n",
    "        outliers_z = numeric_data[(z_scores > 3) | (z_scores < -3)]\n",
    "\n",
    "        # Identify outliers based on IQR\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers_iqr = numeric_data[(numeric_data < lower_bound) | (numeric_data > upper_bound)]\n",
    "\n",
    "        # Create a figure for the distribution and box plot\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "\n",
    "        # Distribution plot (histogram)\n",
    "        sns.histplot(numeric_data, kde=True, bins=10, ax=axs[0])\n",
    "        axs[0].set_title(f'Distribution of {column}')\n",
    "        axs[0].set_xlabel(column)\n",
    "        axs[0].set_ylabel('Frequency')\n",
    "\n",
    "        # Box plot for outlier detection\n",
    "        sns.boxplot(x=numeric_data, ax=axs[1])\n",
    "        axs[1].set_title(f'Box Plot for {column} (Outlier Detection)')\n",
    "        axs[1].set_xlabel(column)\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Summary Statistics\n",
    "        print(f\"\\nSummary Statistics for Numeric  '{column}':\")\n",
    "        print(numeric_data.describe())\n",
    "\n",
    "        # Interquartile Range (IQR)\n",
    "        print(f\"\\nInterquartile Range (IQR): {IQR:.4f}\")\n",
    "        print(f\"Lower Bound for Outliers (IQR method): {lower_bound:.4f}\")\n",
    "        print(f\"Upper Bound for Outliers (IQR method): {upper_bound:.4f}\")\n",
    "        print(f\"Number of Outliers (IQR method): {len(outliers_iqr)}\")\n",
    "\n",
    "        # Display outliers based on IQR\n",
    "        #if not outliers_iqr.empty:\n",
    "        #    print(\"\\nOutliers detected using the IQR method:\")\n",
    "        #    print(outliers_iqr)\n",
    "        #else:\n",
    "        #    print(\"\\nNo outliers detected using the IQR method.\")\n",
    "\n",
    "        # Z-scores\n",
    "        print(\"\\nZ-score Summary:\")\n",
    "        print(z_scores.describe())\n",
    "\n",
    "        print(f\"\\nNumber of Outliers (Z-score method): {len(outliers_z)}\")\n",
    "\n",
    "        # Display outliers based on Z-scores\n",
    "        #if not outliers_z.empty:\n",
    "        #    print(\"\\nOutliers detected using the Z-score method:\")\n",
    "        #    print(outliers_z)\n",
    "        #else:\n",
    "        #    print(\"\\nNo outliers detected using the Z-score method.\")\n",
    "\n",
    "        # Skewness\n",
    "        skewness_value = skew(numeric_data)\n",
    "        print(f\"\\nSkewness: {skewness_value:.4f}\")\n",
    "\n",
    "        # Normality Tests\n",
    "        print(\"\\nNormality Tests:\")\n",
    "        # D'Agostino's K^2 Test\n",
    "        k2_stat, k2_p = normaltest(numeric_data)\n",
    "        print(f\"D'Agostino's K^2 Test: Statistic={k2_stat:.4f}, p-value={k2_p:.4f}\")\n",
    "\n",
    "        # Shapiro-Wilk Test\n",
    "        shapiro_stat, shapiro_p = shapiro(numeric_data)\n",
    "        print(f\"Shapiro-Wilk Test: Statistic={shapiro_stat:.4f}, p-value={shapiro_p:.4f}\")\n",
    "\n",
    "        # Anderson-Darling Test\n",
    "        anderson_result = anderson(numeric_data)\n",
    "        print(f\"Anderson-Darling Test: Statistic={anderson_result.statistic:.4f}\")\n",
    "        #for i in range(len(anderson_result.critical_values)):\n",
    "        #    sl, cv = anderson_result.significance_level[i], anderson_result.critical_values[i]\n",
    "        #    if anderson_result.statistic < cv:\n",
    "        #        result = \"Accept\"\n",
    "        #    else:\n",
    "        #        result = \"Reject\"\n",
    "        #    print(f\"At {sl}% significance level, critical value: {cv:.4f}, {result} the null hypothesis of normality\")\n",
    "\n",
    "        # Kolmogorov-Smirnov Test against normal distribution\n",
    "        # ks_stat, ks_p = kstest(numeric_data, 'norm', args=(numeric_data.mean(), numeric_data.std()))\n",
    "        # print(f\"Kolmogorov-Smirnov Test: Statistic={ks_stat:.4f}, p-value={ks_p:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boolean Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_boolean_columns(boolean_cols, dataframe):\n",
    "    \"\"\"\n",
    "    Analyze and visualize boolean columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    boolean_cols (list): List of boolean column names to analyze.\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    for column in boolean_cols:\n",
    "        print(f\"\\nSummary Statistics and Analysis for Boolean Column: {column}\")\n",
    "\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if column not in dataframe.columns:\n",
    "            print(f\"'{column}' is not found in the DataFrame. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Cast the column to boolean in case it contains 1/0 or other non-boolean values\n",
    "        dataframe[column] = dataframe[column].astype(bool)\n",
    "\n",
    "        # Prepare boolean counts\n",
    "        boolean_counts = dataframe[column].value_counts()\n",
    "\n",
    "        # Create a figure with two subplots\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(FIG_WIDTH, FIG_HEIGHT)) # Configured at the beginning of the file for image consistancy\n",
    "\n",
    "        # Bar plot using 'x' parameter\n",
    "        sns.countplot(\n",
    "            x=column,\n",
    "            data=dataframe,\n",
    "            ax=axs[0]\n",
    "        )\n",
    "        axs[0].set_title(f'Boolean Distribution for {column}')\n",
    "        axs[0].set_xlabel(column)\n",
    "        axs[0].set_ylabel('Count')\n",
    "\n",
    "        # Rotate x-axis labels\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # Pie chart on the right subplot\n",
    "        boolean_counts.plot.pie(\n",
    "            autopct='%1.1f%%',\n",
    "            ax=axs[1],\n",
    "            startangle=90\n",
    "        )\n",
    "        axs[1].set_title(f'Proportion of True/False for {column}')\n",
    "        axs[1].set_ylabel('')\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Display summary statistics for boolean data\n",
    "        print(f\"\\nSummary for Boolean '{column}':\")\n",
    "        true_count = boolean_counts.get(True, 0)\n",
    "        false_count = boolean_counts.get(False, 0)\n",
    "        total_count = true_count + false_count\n",
    "        true_percentage = (true_count / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "        print(f\"Count of True: {true_count}\")\n",
    "        print(f\"Count of False: {false_count}\")\n",
    "        print(f\"Percentage of True: {true_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_columns(categorical_cols, dataframe):\n",
    "    \"\"\"\n",
    "    Analyze and visualize categorical columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    categorical_cols (list): List of categorical column names to analyze.\n",
    "    dataframe (pd.DataFrame): The DataFrame containing the data.\n",
    "    \"\"\"\n",
    "    for column in categorical_cols:\n",
    "        print(f\"\\nSummary Statistics and Analysis for Categorical Column: {column}\")\n",
    "\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if column not in dataframe.columns:\n",
    "            print(f\" '{column}' is not found in the DataFrame. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Prepare category counts and percentages\n",
    "        category_counts = dataframe[column].value_counts()\n",
    "        category_percentages = dataframe[column].value_counts(normalize=True) * 100\n",
    "\n",
    "        # Display bar plot and pie chart\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "\n",
    "        # Bar plot using 'x' parameter\n",
    "        sns.countplot(\n",
    "            x=column,\n",
    "            data=dataframe,\n",
    "            ax=axs[0]\n",
    "        )\n",
    "        axs[0].set_title(f'Frequency Distribution for {column} (Categorical Data)')\n",
    "        axs[0].set_xlabel(column)\n",
    "        axs[0].set_ylabel('Frequency')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Pie chart on the right subplot\n",
    "        category_counts.plot.pie(\n",
    "            autopct='%1.1f%%',\n",
    "            ax=axs[1],\n",
    "            title=f'Proportion of Categories for {column}',\n",
    "            startangle=90\n",
    "        )\n",
    "        axs[1].set_ylabel('')\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nFrequency Table for '{column}':\")\n",
    "        freq_table = pd.DataFrame({'Count': category_counts, 'Percentage': category_percentages})\n",
    "        print(freq_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_features) # issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with our analysis of the Categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List to store features that contain any numerical values (even within mixed text)\n",
    "categorical_list_with_numerical_values = []\n",
    "\n",
    "# Iterate through each column in categorical_list\n",
    "for column in categorical_list:\n",
    "    # Check if the column contains any values with numerical characters\n",
    "    if df_dropped[column].apply(lambda x: bool(re.search(r'\\d', str(x)))).any():\n",
    "        categorical_list_with_numerical_values.append(column)\n",
    "        print(f\" '{column}' contains numerical values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[categorical_list_with_numerical_values].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore our data to avoid creating graphs for rows that contain unique values. To do this we'll run a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_uniqueness(df, columns):\n",
    "    \"\"\"\n",
    "    Analyzes the number of unique values in categorical columns and \n",
    "    helps identify suitable candidates for visualization.\n",
    "\n",
    "    Args:\n",
    "        df: The Pandas DataFrame.\n",
    "        columns: A list of column names to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are column names and values are \n",
    "        the number of unique values in each column. Also, \n",
    "        prints a summary to assist visualizing the results via bar graphs.\n",
    "    \"\"\"\n",
    "    uniqueness_counts = {}\n",
    "    for col in columns:\n",
    "        if not pd.api.types.is_object_dtype(df[col]):  # checks for object type; adjust if needed\n",
    "            continue  # Skip analysis for non-categorical columns\n",
    "        \n",
    "        unique_count = df[col].nunique()  # use nunique for direct count of unique values\n",
    "        uniqueness_counts[col] = unique_count\n",
    "\n",
    "    # Sort the dictionary by unique counts in descending order\n",
    "    sorted_uniqueness_counts = dict(sorted(uniqueness_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Print sorted output with unique counts\n",
    "    print(\"Sorted Unique Value Counts:\")\n",
    "    for col, unique_count in sorted_uniqueness_counts.items():\n",
    "        print(f\"Number of unique values in '{col}': {unique_count}\")\n",
    "\n",
    "    # Create a bar graph to visualize the number of unique values, sorted from high to low\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(sorted_uniqueness_counts.keys(), sorted_uniqueness_counts.values())\n",
    "    plt.xlabel(\"Categorical Columns\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Unique Values\", fontsize=14)\n",
    "    plt.title(\"Unique Value Counts per Categorical Column (Sorted)\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)  # Rotate x-axis labels for readability\n",
    "    plt.tight_layout()  # Adjust subplot parameters for a tight layout\n",
    "    plt.show()\n",
    "\n",
    "    return sorted_uniqueness_counts\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#categorical_uniqueness = analyze_categorical_uniqueness(df_dropped, categorical_list_with_numerical_values)\n",
    "#columns_for_visualization = [col for col, count in categorical_uniqueness.items() if count <= 15] # Example: Use only below or equal to 10\n",
    "#print(f\"\\nSuggested columns for visualization (<= 15 unique values): {columns_for_visualization}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_uniqueness = analyze_categorical_uniqueness(df, categorical_list_with_numerical_values)\n",
    "\n",
    "columns_for_visualization = [col for col, count in categorical_uniqueness.items() if count <= 10] # Example: Use only below or equal to 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straight away we can see `int_rate` (584),`emp_title` (40049), `issue_d` (159),`title` (3454), `revol_util` (1088), `last_pymnt_d` (147), `earliest_cr_line` (667), `sec_app_earliest_cr_line` (506), `last_credit_pull` (137). `zip_code` (878) stand out, we'll remove these as graphically representing these won't produce any meaningful insight. However, we'll see if we can reduce them into logical groupings later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_list = ['int_rate', 'zip_code', 'emp_title', 'issue_d', 'title', 'revol_util', 'last_pymnt_d', 'earliest_cr_line', 'sec_app_earliest_cr_line', 'last_credit_pull_d']\n",
    "\n",
    "# Filter the categorical columns to exclude specified ones\n",
    "filtered_categorical_columns = [col for col in categorical_list_with_numerical_values if col not in exclude_list]\n",
    "\n",
    "# Analyze the filtered categorical columns\n",
    "analyze_categorical_columns(filtered_categorical_columns, df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Object and Float Features\n",
    "\n",
    "We have identified the following columns that require conversion or encoding:\n",
    "\n",
    "#### Actions Post-Analysis\n",
    "\n",
    "##### Convert String to Integer\n",
    "- **term**: Extract numerical part and convert to integer (keep `36` and `60`).\n",
    "- **emp_length**: Extract numerical years and convert to integer.\n",
    "\n",
    "##### Convert String to Float\n",
    "- **int_rate**: Convert to float after removing any non-numeric characters.\n",
    "- **revol_util**: Convert to float after removing the \"%\" symbol.\n",
    "\n",
    "##### Encode Categorical Values\n",
    "- **sub_grade**: Use as is or encode if necessary; consider dropping **grade** if redundant.\n",
    "- **loan_status**: Group or encode based on loan status levels.\n",
    "- **hardship_loan_status**: Analyze and group similar hardship statuses if logical.\n",
    "\n",
    "##### Convert to Date/Time Format\n",
    "- **issue_d**: Convert to date/time for chronological analysis.\n",
    "- **earliest_cr_line**: Convert to date/time to track the earliest credit history.\n",
    "- **last_pymnt_d**: Convert to date/time; create separate year and month columns.\n",
    "- **next_pymnt_d**: Convert to date/time; add year and month columns.\n",
    "- **last_credit_pull_d**: Convert to date/time for recent credit activity insights.\n",
    "- **sec_app_earliest_cr_line**: Convert to date/time for secondary applicants credit history.\n",
    "- **hardship_start_date**: Convert to date/time; add year and month columns.\n",
    "- **hardship_end_date**: Convert to date/time; add year and month columns.\n",
    "- **payment_plan_start_date**: Convert to date/time; add year and month columns.\n",
    "\n",
    "##### Remove Non-Analytical or Irrelevant Columns\n",
    "- **emp_title**: Not relevant for numerical analysis; remove.\n",
    "- **url**: Non-analytical; remove as it doesnt contribute to analysis.\n",
    "\n",
    "##### Evaluate for Categorical Consistency\n",
    "- **zip_code**: Analyze the first few digits if relevant to extract location-based insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Columns that need to be converted from string to integer\n",
    "string_columns_to_convert_int = ['term', 'emp_length']  # features to replace with int values\n",
    "\n",
    "# Convert each specified column to an integer in a new column\n",
    "for column in string_columns_to_convert_int:\n",
    "    # Extract numerical part and convert to integer\n",
    "    df_dropped[f\"{column}_kn\"] = df_dropped[column].apply(lambda x: int(re.search(r'\\d+', str(x)).group()) if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[['term_kn','emp_length_kn']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['term_kn', 'emp_length_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = string_columns_to_convert_int\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"Both columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the columns that need to be converted from string to float\n",
    "string_columns_to_convert_float = ['int_rate', 'revol_util']  # features to replace with float values\n",
    "\n",
    "# Convert each specified column to a float in a new column as percentage (e.g., 80% becomes 0.80)\n",
    "for column in string_columns_to_convert_float:\n",
    "    # Extract numerical part, convert to float, and divide by 100\n",
    "    df_dropped[f\"{column}_kn\"] = df_dropped[column].apply(\n",
    "        lambda x: float(re.search(r'\\d+', str(x)).group()) / 100 if pd.notnull(x) else None\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['int_rate_kn'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['int_rate_kn', 'revol_util_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = ['int_rate_kn', 'revol_util_kn']\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"Both columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert date time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[['issue_d', 'earliest_cr_line']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of date columns to convert\n",
    "date_columns = ['issue_d', 'earliest_cr_line']\n",
    "\n",
    "# Convert the columns to datetime format\n",
    "for col in date_columns:\n",
    "    # Convert to datetime using the format '%b-%y', with errors coerced to NaT\n",
    "    df_dropped[col] = pd.to_datetime(df_dropped[col], format='%b-%y', errors='coerce')\n",
    "\n",
    "# Extracting date features for each date column\n",
    "for col in date_columns:\n",
    "    # Define new column names for year and month\n",
    "    year_col = f'{col}_year_kn'\n",
    "    month_col = f'{col}_month_kn'\n",
    "       \n",
    "    # Extract year and month, convert to integer with support for NaNs\n",
    "    df_dropped[year_col] = df_dropped[col].dt.year.astype('Int64').fillna(0)  # Extract year\n",
    "    df_dropped[month_col] = df_dropped[col].dt.month.astype('Int64').fillna(0)  # Extract month\n",
    "    \n",
    "    # Add the new column names to the existing 'new_features' list\n",
    "    new_features.extend([year_col, month_col])\n",
    "    \n",
    "    # Print the names of the newly created columns\n",
    "    print(f\"New columns created: {year_col}, {month_col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[['issue_d', 'earliest_cr_line', 'issue_d_year_kn', 'issue_d_month_kn', 'earliest_cr_line_year_kn', 'earliest_cr_line_month_kn']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['issue_d_year_kn', 'issue_d_month_kn','earliest_cr_line_year_kn', 'earliest_cr_line_month_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = ['issue_d_year_kn', 'issue_d_month_kn','earliest_cr_line_year_kn', 'earliest_cr_line_month_kn']\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"Both columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of columns in df_dropped\n",
    "df_dropped_columns = set(df_dropped.columns)\n",
    "\n",
    "# Convert new_features to a set for comparison\n",
    "new_features_set = set(new_features)\n",
    "\n",
    "# Find columns in new_features that are missing in df_dropped\n",
    "missing_in_df_dropped = new_features_set - df_dropped_columns\n",
    "\n",
    "# Find columns in df_dropped that are not in new_features (if relevant)\n",
    "extra_in_df_dropped = df_dropped_columns - new_features_set\n",
    "\n",
    "# Print the results\n",
    "print(\"Features in new_features missing from df_dropped:\", missing_in_df_dropped)\n",
    "print(\"Columns in df_dropped not listed in new_features:\", extra_in_df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Non Analytical or Irrelevent columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm tempted to leave these in as it could be a good indicator of good or bad payment trends but won't have the time to do it for now we'll remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove\n",
    "columns_to_remove = ['zip_code', 'emp_title']\n",
    "\n",
    "# Remove specified columns from new_features if they exist\n",
    "new_features = [col for col in new_features if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sense check\n",
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating logical groups "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **hardship_reason**: Analyse and group to simplify analysis (import from df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logical groupings for 'hardship_reason'\n",
    "hardship_reason_groupings = {\n",
    "    'INCOMECURT': 'Income Loss',\n",
    "    'UNEMPLOYED': 'Income Loss',\n",
    "    'UNEMPLOYMENT': 'Income Loss',\n",
    "    'INCOME_CURTAILMENT': 'Income Loss',\n",
    "    'REDCDHOURS': 'Income Loss',\n",
    "    'REDUCED_HOURS': 'Income Loss',\n",
    "    'FURLOUGH': 'Income Loss',\n",
    "    'MEDICAL': 'Health Issues',\n",
    "    'DISABILITY': 'Health Issues',\n",
    "    'NATURAL_DISASTER': 'External Events',\n",
    "    'NATDISAST': 'External Events',\n",
    "    'FINANCIAL': 'Financial Strain',\n",
    "    'EXCESSIVE_OBLIGATIONS': 'Financial Strain',\n",
    "    'EXCESSOBLI': 'Financial Strain',\n",
    "    'DIVORCE': 'Family Circumstances',\n",
    "    'FAMILY_DEATH': 'Family Circumstances',\n",
    "    'DEATH': 'Family Circumstances'\n",
    "}\n",
    "\n",
    "# Apply the grouping to the 'hardship_reason' column\n",
    "df_dropped['hardship_reason_grouped_kn'] = df['hardship_reason'].replace(hardship_reason_groupings) # adding this back into our data set\n",
    "\n",
    "# Verify the groupings\n",
    "print(df_dropped['hardship_reason_grouped_kn'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logical groupings for 'loan_status'\n",
    "loan_status_groupings = {\n",
    "    'Fully Paid': 'Completed',\n",
    "    'Current': 'In Progress',\n",
    "    'Charged Off': 'Defaulted',\n",
    "    'Late (31-120 days)': 'Late',\n",
    "    'In Grace Period': 'Late',\n",
    "    'Late (16-30 days)': 'Late',\n",
    "    'Does not meet the credit policy. Status:Fully Paid': 'Completed',\n",
    "    'Issued': 'In Progress',\n",
    "    'Does not meet the credit policy. Status:Charged Off': 'Defaulted',\n",
    "    'Default': 'Defaulted'\n",
    "}\n",
    "\n",
    "# Apply the grouping to the 'loan_status' column\n",
    "df_dropped['loan_status_grouped_kn'] = df['loan_status'].replace(loan_status_groupings)\n",
    "\n",
    "# Verify the groupings\n",
    "print(df_dropped['loan_status_grouped_kn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Any', 'Other', and 'none' with a unified value 'Other'\n",
    "df_dropped['home_ownership_grouped_kn'] = df_dropped['home_ownership'].replace(['ANY', 'OTHER', 'NONE'], 'OTHER')\n",
    "\n",
    "# Display the updated DataFrame to verify\n",
    "df_dropped['home_ownership_grouped_kn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.extend(['hardship_reason_grouped_kn', 'loan_status_grouped_kn','home_ownership_grouped_kn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of columns to be removed\n",
    "# these are all the columns we've altered. These can remain in the data frame but should be removed from our feature list\n",
    "columns_to_remove = ['hardship_reason', 'loan_status','home_ownership']\n",
    "\n",
    "# Remove specified columns from new_features if they exist\n",
    "new_features = [col for col in new_features if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the specified columns are in new_features\n",
    "columns_to_check = ['hardship_reason_grouped_kn', 'loan_status_grouped_kn','home_ownership_grouped_kn']\n",
    "missing_columns = [col for col in columns_to_check if col not in new_features]\n",
    "\n",
    "# Display results\n",
    "if not missing_columns:\n",
    "    print(\"Both columns are in new_features.\")\n",
    "else:\n",
    "    print(f\"The following columns are missing from new_features: {missing_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dropped.shape)\n",
    "print(len(new_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our list function again on the udpated data frame and scroll through to update our feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_boolean_columns(boolean_list, df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to check for missing values\n",
    "columns_to_check = [\n",
    "    'earliest_cr_line_missing', 'fico_range_high_missing', \n",
    "    'fico_range_low_missing', 'last_fico_range_high_missing', \n",
    "    'last_fico_range_low_missing', 'pub_rec_missing', \n",
    "    'pub_rec_bankruptcies_missing', 'annual_inc_missing', \n",
    "    'inq_last_6mths_missing'\n",
    "]\n",
    "\n",
    "# Check for missing values in each specified column and display the count\n",
    "for column in columns_to_check:\n",
    "    missing_count = df[column].isnull().sum() # checking against original data if 0 we can remove these from our ML feature list\n",
    "    print(f\"Missing values in '{column}': {missing_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing our output I've decided to drop the following values as their representation within the data set is not meaningful enought to be any statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original list of columns to remove\n",
    "columns_to_remove = [\n",
    "    'earliest_cr_line_missing_clean_kn', 'fico_range_high_missing_clean_kn', \n",
    "    'fico_range_low_missing_clean_kn', 'last_fico_range_high_missing_clean_kn', \n",
    "    'last_fico_range_low_missing_clean_kn', 'pub_rec_missing_clean_kn', \n",
    "    'pub_rec_bankruptcies_missing_clean_kn', 'annual_inc_missing_clean_kn', \n",
    "    'inq_last_6mths_missing_clean_kn'\n",
    "]\n",
    "\n",
    "# Create a new list by excluding the columns in columns_to_remove\n",
    "new_features_updated = [col for col in new_features if col not in columns_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['loan_status_grouped_kn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_numeric_columns(numerical_list, df_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finised our EDA let's check some of the categorical points with our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of purpose and loan_status\n",
    "comparison_loan_status_purpose = pd.crosstab(df_dropped['loan_status_grouped_kn'], df_dropped['purpose'])\n",
    "\n",
    "# Display the result\n",
    "comparison_loan_status_purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of purpose and loan_status\n",
    "comparison_loan_status_ver_status = pd.crosstab(df_dropped['loan_status_grouped_kn'], df_dropped['verification_status'])\n",
    "\n",
    "# Display the result\n",
    "comparison_loan_status_ver_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting view, as the the Not Verified, and Source Verified represent both roughly 15-20% of the charged off loans for each status but Verified accounts for roughly 23% of the total. I was expecting a lot of the Charged Off in the Not Verified `verification_status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of purpose and loan_status\n",
    "comparison_loan_status_addr_state = pd.crosstab(df_dropped['loan_status_grouped_kn'], df_dropped['addr_state'])\n",
    "\n",
    "# Display the result\n",
    "comparison_loan_status_addr_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group data by 'loan_status' and 'addr_state' and count occurrences\n",
    "grouped_data = df_dropped.groupby(['addr_state', 'loan_status_grouped_kn']).size().unstack()\n",
    "\n",
    "# Sort the data by the total count of loan statuses in descending order\n",
    "grouped_data = grouped_data.loc[grouped_data.sum(axis=1).sort_values(ascending=False).index]\n",
    "\n",
    "# Plot the bar chart\n",
    "grouped_data.plot(kind='bar', stacked=True, figsize=(10, 7))\n",
    "\n",
    "# Adding labels and title\n",
    "plt.title('Loan Status by Address State')\n",
    "plt.xlabel('Address State')\n",
    "plt.ylabel('Count of Loan Status')\n",
    "plt.legend(title='Loan Status')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Observations\n",
    "- **Top States**: The states with the highest loan counts include **CA (California)**, **TX (Texas)**, **NY (New York)**, and **FL (Florida)**. These states exhibit a high volume of loans, likely due to their larger populations and economic activities.\n",
    "- **Completed Loans**: The **Completed** loan status (blue segment) constitutes a significant portion in most states, suggesting a high rate of loan completion across regions.\n",
    "- **In Progress Loans**: The **In Progress** loan status (green segment) appears prominently in states with higher loan volumes, indicating ongoing loan activities.\n",
    "- **Defaulted and Late Loans**: **Defaulted** (orange) and **Late** (red) loans make up smaller portions of the overall loan distribution. However, states with higher loan counts (e.g., CA, TX, NY) also show relatively higher counts in these categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep `addr_state` in our feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional feature could be the ratio of Charged Off by Fully Paid by state. We'll leave this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post EDA Analysis data transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def transform_features(dataframe, bool_features, num_features, cat_features, prefix_suffix='_clean_kn'):\n",
    "    \"\"\"\n",
    "    Transforms features in the dataframe based on their data type and specified feature lists.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dataframe : pd.DataFrame\n",
    "        The DataFrame containing the data to be transformed.\n",
    "    bool_features : list\n",
    "        A list of column names representing boolean features.\n",
    "    num_features : list\n",
    "        A list of column names representing numerical features.\n",
    "    cat_features : list\n",
    "        A list of column names representing categorical features.\n",
    "    prefix_suffix : str\n",
    "        The suffix to append to the prefix of each column during one-hot encoding.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_transformed : pd.DataFrame\n",
    "        The DataFrame with transformed features.\n",
    "    \"\"\"\n",
    "    # Copy the original DataFrame to avoid modifying it\n",
    "    df_transformed = dataframe.copy()\n",
    "    \n",
    "    # Process boolean features\n",
    "    for feature in bool_features:\n",
    "        if feature in df_transformed.columns:\n",
    "            print(f\"Processing boolean feature: {feature}\")\n",
    "            # Ensure the feature is boolean or convert to boolean\n",
    "            df_transformed[feature] = df_transformed[feature].astype(bool)\n",
    "            \n",
    "            # Convert boolean to integer (0 and 1)\n",
    "            df_transformed[feature] = df_transformed[feature].astype(int)\n",
    "            \n",
    "            # Scale the feature using StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            df_transformed[feature] = scaler.fit_transform(df_transformed[[feature]])\n",
    "        else:\n",
    "            print(f\"Boolean feature '{feature}' not found in DataFrame. Skipping...\")\n",
    "\n",
    "    # Process numerical features\n",
    "    for feature in num_features:\n",
    "        if feature in df_transformed.columns:\n",
    "            print(f\"Processing numerical feature: {feature}\")\n",
    "            # Handle missing values by imputing with 0\n",
    "            df_transformed[feature] = df_transformed[feature].fillna(0)\n",
    "            \n",
    "            # Check for skewness and apply log transformation if necessary\n",
    "            skewness = df_transformed[feature].skew()\n",
    "            if skewness > 1 or skewness < -1:\n",
    "                # Apply log transformation to reduce skewness\n",
    "                feature_min = df_transformed[feature].min()\n",
    "                if feature_min <= 0:\n",
    "                    df_transformed[feature] = np.log1p(df_transformed[feature] - feature_min + 1)\n",
    "                else:\n",
    "                    df_transformed[feature] = np.log1p(df_transformed[feature])\n",
    "            \n",
    "            # Scale the feature using StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            df_transformed[feature] = scaler.fit_transform(df_transformed[[feature]])\n",
    "        else:\n",
    "            print(f\"Numerical feature '{feature}' not found in DataFrame. Skipping...\")\n",
    "\n",
    "    # Process categorical features (One-Hot Encoding)\n",
    "    if cat_features:\n",
    "        print(f\"Processing categorical features: {cat_features}\")\n",
    "        \n",
    "        # Generate one-hot encoded columns for the specified categorical columns\n",
    "        dummies = pd.get_dummies(\n",
    "            df_transformed[cat_features],\n",
    "            prefix=[f\"{col}{prefix_suffix}\" for col in cat_features]\n",
    "        )\n",
    "        \n",
    "        # Use pd.concat to concatenate the new dummy columns with the original DataFrame\n",
    "        df_transformed = pd.concat([df_transformed, dummies], axis=1)\n",
    "        \n",
    "        # Drop the original categorical columns from the DataFrame\n",
    "        df_transformed.drop(columns=cat_features, inplace=True)\n",
    "\n",
    "    print(\"Transformation complete\")\n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped_ml_features = df_dropped.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode our categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this calls the split_data_frame function above create three lists to capture the sorting outputs in. \n",
    "# These will later be used to pull some graphs to evaluate the data and what possible transformations we've missed.\n",
    "# Our function parameters are list and the latest version of our data frame in this caes new_features and df_dropped accordingly.\n",
    "boolean_list, numerical_list, categorical_list = split_data_frame(new_features, df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def apply_one_hot_encoding(df, features):\n",
    "    \"\"\"\n",
    "    Applies One-Hot Encoding to specified features in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The original DataFrame.\n",
    "    features (list): List of column names to apply One-Hot Encoding on.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with One-Hot Encoded features.\n",
    "    \"\"\"\n",
    "    # Apply One-Hot Encoding\n",
    "    df_encoded = pd.get_dummies(df, columns=features, drop_first=True)\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `df` is your DataFrame and `features_to_encode` is the list of categorical features to encode\n",
    "features_to_encode = ['application_type', 'initial_list_status']\n",
    "df_encoded = apply_one_hot_encoding(df, features_to_encode)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that need to convert 'Y' to 1 and 'N' to 0\n",
    "columns_to_convert_binary = ['application_type','initial_list_status','verification_status', 'home_ownership_kn']  # Add any additional columns here\n",
    "\n",
    "# Initialize an empty list for new feature names\n",
    "new_features = []\n",
    "\n",
    "# Loop through each specified column\n",
    "for column in columns_to_convert_binary:\n",
    "    # Create a new column with '_kn' suffix and map 'Y' to 1 and 'N' to 0\n",
    "    df_dropped[f\"{column}_kn\"] = df_dropped[column].map({'Y': 1, 'N': 0})\n",
    "\n",
    "    # Append the new column name to new_features if not already present\n",
    "    new_col_name = f\"{column}_kn\"\n",
    "    if new_col_name not in new_features:\n",
    "        new_features.append(new_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped_ml_features = df_dropped_ml_features.drop('loan_default', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped[ignore_features].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to ignore\n",
    "ignore_features = [\n",
    "    'addr_state', 'application_type', 'earliest_cr_line', 'emp_length', 'emp_title', \n",
    "    'grade', 'home_ownership', 'initial_list_status', 'int_rate', 'issue_d', \n",
    "    'loan_status', 'purpose', 'revol_util', 'sub_grade', 'term', 'title', \n",
    "    'verification_status', 'zip_code', 'loan_status_grouped_kn'\n",
    "]\n",
    "\n",
    "# Filter each list to exclude the ignored features\n",
    "filtered_boolean_list = [feature for feature in boolean_list if feature not in ignore_features]\n",
    "filtered_numerical_list = [feature for feature in numerical_list if feature not in ignore_features]\n",
    "filtered_categorical_list = [feature for feature in categorical_list if feature not in ignore_features]\n",
    "\n",
    "# Now pass the filtered lists to the transform_features function\n",
    "df_transformed = transform_features(df_dropped, filtered_boolean_list, filtered_numerical_list, filtered_categorical_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouble shooting. We didn't remove the original columns after encoding. This has been flagged as an error. The following code displays, and removes them from the feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped_ml_features[['hardship_flag', 'verification_status', 'purpose', 'home_ownership']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_from_features = ['hardship_flag', 'verification_status', 'purpose', 'home_ownership']\n",
    "\n",
    "# Loop through each column in remove_from_features\n",
    "for col in remove_from_features:\n",
    "    if col in new_features:  # Check if the column exists in new_features\n",
    "        new_features.remove(col)  # Remove the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed[new_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troubleshooting the new_features list. During the encoding process I forgot to remove the original columns. This caused string to float conversion errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['home_ownership', 'purpose', 'hardship_flag', 'verification_status', 'sec_app_earliest_cr_line', 'next_pymnt_d', 'addr_state', 'emp_title']\n",
    "\n",
    "# Dropping values using remove\n",
    "for value in columns_to_drop:\n",
    "    while value in new_features:\n",
    "        new_features.remove(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assign your dataframes\n",
    "X = df_transformed[df_dropped_ml_features]  # Use the output from our encoded data\n",
    "Y = df_dropped['loan_status_grouped_kn']\n",
    "\n",
    "# Optional: Encode target variable if necessary\n",
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.5, random_state=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=50),\n",
    "    'Random Forest': RandomForestClassifier(random_state=50, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=50, n_estimators=100),\n",
    "    #'SVM': SVC(probability=True, random_state=50),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_dropped[new_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Function to evaluate a single model\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test, save_path=\"\"):\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Check if the model provides probability estimates\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    else:\n",
    "        y_pred_proba = None\n",
    "        roc_auc = None\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    else:\n",
    "        print(\"ROC-AUC: Not available (model does not support probability estimates)\")\n",
    "    print(f\"Cross-Validation Mean Accuracy: {cv_mean:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    # Plot ROC curve if probabilities are available\n",
    "    if y_pred_proba is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.title(f'ROC Curve - {name}')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{name} does not provide probability estimates; ROC curve cannot be plotted.\")\n",
    "    \n",
    "    # Create a results dictionary\n",
    "    results = {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'CV Mean Accuracy': cv_mean\n",
    "    }\n",
    "    \n",
    "    # Save the model and its results\n",
    "    if save_path:\n",
    "        model_save_path = f\"{save_path}/{name}_model.pkl\"\n",
    "        results_save_path = f\"{save_path}/{name}_results.pkl\"\n",
    "        \n",
    "        # Save the model\n",
    "        joblib.dump(model, model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "        \n",
    "        # Save the results\n",
    "        joblib.dump(results, results_save_path)\n",
    "        print(f\"Results saved to {results_save_path}\")\n",
    "    \n",
    "    # Return the results dictionary\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've extracted, created, cleaned, and encoded our features. Time to run the machine learning models. I've tweaked the code to run several machine learning models since we're focusing on regression type models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, model in models.items():\n",
    "    result = evaluate_model(name, model, X_train, X_test, y_train, y_test, save_path=\"C:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\JupyterNotes\\\\ELVTR\\\\Assignments\\\\Assignment2\\\\models\\\\\")\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model Run Conclusion:\n",
    "\n",
    "## Summary of Results:\n",
    "\n",
    "- **Best Recall**: Naive Bayes, though its low precision and high false-positive make it my third choice.\n",
    "- **Balanced Models**: Random Forest (#1) and Gradient Boosting (#2) offer the best balance between precision, recall, and AUC. I'd suggest working with the features to improve the scoring of these models.\n",
    "\n",
    "### Detail:\n",
    "\n",
    "#### 1. Logistic Regression:\n",
    "\n",
    "The Logistic Regression model achieves a good accuracy and AUC score, indicating some predictive capability. \n",
    "\n",
    "However, the low recall indicates it struggles to capture true positive loan defaults.\n",
    "\n",
    "#### 2. Decision Tree:\n",
    "\n",
    "The Decision Tree model performs lower than Logistic Regression in terms of AUC.\n",
    "\n",
    "But it has a better recall, implying it is better at identifying loan defaults compared to Logistic Regression.\n",
    "\n",
    "#### 3. Random Forest:\n",
    "\n",
    "Random Forest achieves the highest accuracy and AUC score, but with a lower recall. \n",
    "\n",
    "It is more balanced than Logistic Regression (both Precision and Accuracy in the 80s) but scores worse than the Decision Tree in identifying true positives (45 DT compared to 27 RF).\n",
    "\n",
    "#### 4. Gradient Boosting:\n",
    "\n",
    "Similar to Random Forest, Gradient Boosting has high accuracy and AUC but lower recall. \n",
    "\n",
    "#### 5. K-Nearest Neighbors (KNN):\n",
    "\n",
    "KNN has lower performance compared to the previous models, particularly struggling with recall and AUC.\n",
    "\n",
    "We can safely elminate this model.\n",
    "\n",
    "#### 6. Naive Bayes:\n",
    "\n",
    "Naive Bayes has a high recall, but very poor accuracy and precision. \n",
    "\n",
    "It identifies nearly all defaults, but with many false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing imbalance to improve our model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "We noticed earlier that the data set was largely skewed. Leading to an imbalence in our data set. Let's try and account for this using SMOTE and see if this improves our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Reduce k_neighbors to handle the small minority class\n",
    "smote = SMOTE(random_state=50, k_neighbors=3)\n",
    "\n",
    "# Resample the training data\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for name, model in models.items():\n",
    "    result = evaluate_model(name, model, X_train, X_test, y_train, y_test, save_path=\"C:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\JupyterNotes\\\\ELVTR\\\\Assignments\\\\Assignment2\\\\models\\\\smote\\\\\")\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model Run Conclusion:\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "Overall reduced precision resulted in drastically better ROC results at the sacrifice of precision (avg. 0.20 pts).\n",
    "\n",
    "After applying SMOTE, the performance of most models improved in terms of recall, particularly:\n",
    "- Random Forest: improvements overall besides Precision which dropped a few points. ROC score improved from 0.27 to 0.43 equal to a 60% improvement.\n",
    "- Gradient Boosting: improvements overall besides Precision, Accuracy, and as a consequence ROC-AUC. ROC score improved from 0.28 to 0.49 equal to a 75% improvement.\n",
    "and Logistic Regression: improvements overall but suffered by a decrease in accuracy, and precision. This model improved its ROC score the most going from 0.09 to 0.63 after SMOTE equal to a 600% improvement.\n",
    "\n",
    "### Recommendations:\n",
    "1. **Random Forest** remains a top choice due to its overall balanced performance, strong ROC-AUC (0.8412), and improved recall after SMOTE.\n",
    "2. **Gradient Boosting** is another strong option, with a good F1-score and high ROC-AUC (0.8155), performing well across metrics.\n",
    "\n",
    "These two models are the ones I'd leverage be the primary algorithms selected for predicting loan defaults after applying SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cost of being wrong\n",
    "\n",
    "In attempt to understand the cost of error I've used a sample number of 1000 predictions. The question I'm asking myself is the following:\n",
    "\n",
    "How much is the cost of the time spent researching, execution, and closing false negatives, and false positives?\n",
    "\n",
    "I'll use Precision and Recall to calculate this. We'll also substitue a few numbers to simulate cost in minutes.\n",
    "\n",
    "We have the following information:\n",
    "\n",
    "- **Gradient Boosting:**\n",
    "    - Precsion (after SMOTE) 0.5523\n",
    "    - Recall (after SMOTE) 0.4991\n",
    "\n",
    "- **Random Forest:**\n",
    "    - Precsion (after SMOTE) 0.6833\n",
    "    - Recall (after SMOTE) 0.4325\n",
    "\n",
    "Expected number of defaults = Sum of defaults / Total data set =  12431 / 63689 = 0.1951 * 100 = 20%\n",
    "\n",
    "Using 1000 applicants or current loans as a base figure.\n",
    "\n",
    "1000 * 0.20 = 800 estimated non defaults, leaving us with 200 actual defaults estimated.\n",
    "\n",
    "- **Gradient Boosting:**\n",
    "    - False Positives = Precsion (after SMOTE) = (1 - 0.5523) * 800 =  358\n",
    "    - False Negatives = Recall (after SMOTE) = (1 - 0.4991) * 200 = 100\n",
    "    - Total of incorrect predictions 458\n",
    "\n",
    "- **Random Forest:**\n",
    "    - False Positives = Precsion (after SMOTE) = (1 - 0.6833) * 800 =  253\n",
    "    - False Negatives = Recall (after SMOTE) = (1 - 0.4325) * 200 = 114\n",
    "    - Total of incorrect predictions 367\n",
    "\n",
    "The cost of for each approach can be calculated by multiplying our results against the cost of a False Positive, and the cost of a False Negative.\n",
    "\n",
    "Let's assume the cost of a False Positive (revenue loss maybe denying a loan) equates to 2500, and the cost of a False Negtive (loss from a customer defaulting) equates to 5600.\n",
    "\n",
    "**Gradient Boosting** = (358 * 2500) + (100 * 5600) = 1,455,000\n",
    "**Random Forest** = (253 * 2500) + (114 * 5600) = 1,270,900\n",
    "\n",
    "In the above scenario with these very subjective numbers we'd be best opting for the Random Forest which has a lower cost of being wrong.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
