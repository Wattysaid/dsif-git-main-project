{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - Clean Code Version\n",
    "\n",
    "This notebook demonstrates a comprehensive machine learning pipeline, including data preprocessing, feature engineering, model training, and evaluation for a binary classification task (Predicting Loan Status as 'Paid Loan' or 'Defaulted Loan'). Below is an outline of each key section:\n",
    "\n",
    "1. **Library Imports**: Loads all essential libraries for data handling, visualization, model training, evaluation, and saving.\n",
    "\n",
    "2. **Data Loading and Cleaning**: Reads the dataset, standardizes column names, and applies initial data quality checks for missing and infinite values.\n",
    "\n",
    "3. **Data Preprocessing**:\n",
    "   - Categorical features are filled and encoded using binary encoding.\n",
    "   - Missing values in numerical features are imputed.\n",
    "   - Missing data indicators are created for further analysis.\n",
    "\n",
    "4. **Feature Engineering**:\n",
    "   - Calculates Variance Inflation Factor (VIF) to identify and remove features with high multicollinearity.\n",
    "   - Applies Recursive Feature Elimination (RFE) with Logistic Regression and RandomForest for feature selection.\n",
    "\n",
    "5. **Model Evaluation Functions**:\n",
    "   - Trains multiple models and evaluates them using accuracy, precision, recall, F1-score, ROC-AUC, and cross-validation accuracy.\n",
    "\n",
    "6. **Results Summary and Visualization**:\n",
    "   - Summarizes results in a DataFrame and saves them to a CSV file.\n",
    "   - Plots a comparison of performance metrics across models for quick assessment.\n",
    "\n",
    "7. **Model Saving and Reloading**:\n",
    "    - Saves all trained models to disk for future use.\n",
    "    - Demonstrates reloading saved models and making predictions to validate accuracy.\n",
    "\n",
    "8. **SHAP Explanations for Model Interpretability**:\n",
    "   - Generates SHAP values for model explanations to understand feature importance.\n",
    "\n",
    "This pipeline is designed to handle binary classification problems, supports multiple models, and provides detailed performance analysis and visualization for decision-making.\n",
    "\n",
    "# Let the Fun Begin\n",
    "\n",
    "**Below code Block Explanation**: This block imports essential libraries required for data handling, encoding, visualization, machine learning models, feature selection, and evaluation metrics. Grouping imports helps keep the code organized, and importing them all at once avoids repetitive imports later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Import Required Libraries ===\n",
    "\n",
    "# Data Manipulation and Preprocessing\n",
    "import pandas as pd         # Core data manipulation library\n",
    "import numpy as np          # Mathematical operations\n",
    "import category_encoders as ce  # Encoding categorical variables\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import normaltest, shapiro, anderson, kstest, skew\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  # For multicollinearity checks (VIF)\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt         # Basic plotting\n",
    "import seaborn as sns                   # Advanced static visualizations with themes\n",
    "import plotly.express as px             # Interactive plots\n",
    "import missingno as msno                # Visualizing missing data\n",
    "\n",
    "# Data Preprocessing and Encoding\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize  # Scaling and label encoding\n",
    "from sklearn.model_selection import train_test_split, cross_val_score           # Data splitting and cross-validation\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression           # Linear models for classification and regression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor          # Decision Tree models\n",
    "from sklearn.ensemble import (                                                 \n",
    "    RandomForestClassifier, RandomForestRegressor,                              # Random Forest models\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor                       # Gradient Boosting models\n",
    ")\n",
    "from sklearn.svm import SVC, SVR                                               # Support Vector Machine for classification and regression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor        # K-Nearest Neighbors for classification and regression\n",
    "from sklearn.naive_bayes import GaussianNB                                     # Naive Bayes for classification\n",
    "from sklearn.cluster import KMeans                                             # KMeans clustering\n",
    "from sklearn.decomposition import PCA                                          # Principal Component Analysis (PCA) for dimensionality reduction\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier         # Multi-class classification strategies\n",
    "\n",
    "# Advanced Machine Learning Models\n",
    "from xgboost import XGBClassifier         # Extreme Gradient Boosting\n",
    "from lightgbm import LGBMClassifier       # Light Gradient Boosting Machine\n",
    "from catboost import CatBoostClassifier   # CatBoost Gradient Boosting\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import RFE  # Recursive Feature Elimination (RFE) for feature selection\n",
    "\n",
    "# Model Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,      # Basic classification metrics\n",
    "    roc_auc_score, confusion_matrix,                             # Advanced metrics and confusion matrix\n",
    "    roc_curve, precision_recall_curve, average_precision_score    # Curve metrics for model evaluation\n",
    ")\n",
    "\n",
    "# Deep Learning with TensorFlow/Keras\n",
    "from tensorflow.keras.models import Sequential          # Sequential model setup in Keras\n",
    "from tensorflow.keras.layers import Dense               # Dense layers for neural networks\n",
    "\n",
    "# Utilities\n",
    "from tqdm import tqdm       # Progress bar for loops\n",
    "import joblib               # For saving/loading models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Clean Data\n",
    "df = pd.read_csv(\"c:\\\\Users\\\\kiera\\\\OneDrive\\\\Documents\\\\GitHub\\\\dsif-git-main-project\\\\elvtr_main_project\\\\data\\\\1-raw\\\\lending-club-2007-2020Q3\\\\Loan_status_2007-2020Q3-100ksample.csv\")\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "print(\"Cleaned headers:\", df.columns.tolist())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration and Display Settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "sns.set_theme(style='whitegrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [\n",
    "    'acc_now_delinq', 'acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc', \n",
    "    'annual_inc_joint', 'application_type', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', \n",
    "    'chargeoff_within_12_mths', 'collections_12_mths_ex_med', 'delinq_2yrs', 'delinq_amnt', \n",
    "    'dti', 'dti_joint', 'earliest_cr_line', 'emp_length', 'emp_title', \n",
    "    'fico_range_high', 'fico_range_low', 'funded_amnt', 'funded_amnt_inv', 'grade', \n",
    "    'home_ownership', 'il_util', 'initial_list_status', 'inq_fi', 'inq_last_12m', \n",
    "    'inq_last_6mths', 'installment', 'int_rate', 'issue_d', 'loan_amnt', 'loan_status', \n",
    "    'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', \n",
    "    'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'mths_since_last_delinq', \n",
    "    'mths_since_last_major_derog', 'mths_since_last_record', 'mths_since_rcnt_il', \n",
    "    'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq', \n",
    "    'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', \n",
    "    'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', \n",
    "    'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', \n",
    "    'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'open_acc', 'open_acc_6m', 'open_il_12m', \n",
    "    'open_il_24m', 'open_act_il', 'open_rv_12m', 'open_rv_24m', 'out_prncp', \n",
    "    'out_prncp_inv', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'policy_code', 'pub_rec', \n",
    "    'pub_rec_bankruptcies', 'purpose', 'pymnt_plan', 'revol_bal', 'revol_util', \n",
    "    'sub_grade', 'tax_liens', 'term', 'title', 'tot_coll_amt', 'tot_cur_bal', \n",
    "    'tot_hi_cred_lim', 'total_acc', 'total_bal_ex_mort', 'total_bal_il', 'total_bc_limit', \n",
    "    'total_cu_tl', 'total_il_high_credit_limit', 'total_pymnt', 'total_pymnt_inv', \n",
    "    'total_rec_int', 'total_rec_late_fee', 'total_rec_prncp', 'total_rev_hi_lim', \n",
    "    'verification_status', 'zip_code'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block filters our data frame (df) to the feature_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our data frame is comprised of (rows, cols): {df[feature_list].shape}\")\n",
    "df[feature_list].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: The below code creates a bar chart for us to evaluate our loan_status data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the value counts for loan status\n",
    "loan_status_counts = df['loan_status'].value_counts()\n",
    "\n",
    "# Plot with matplotlib\n",
    "plt.figure(figsize=(7, 5))  # width=500/100 and height=350/100 for similar sizing in inches\n",
    "loan_status_counts.plot(kind='bar')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(\"Loan Status Counts\")\n",
    "plt.xlabel(\"Loan Status\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explained: This block creates a logical grouping for Paid Loans and Defaulted loans. It then further loops through the data frame and retains only the rows that contain Paid Loans and Defaulted Loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logical groupings for 'loan_status'\n",
    "loan_status_groupings = {\n",
    "    'Fully Paid': 'Paid Loan',\n",
    "    'Does not meet the credit policy. Status:Fully Paid': 'Paid Loan',\n",
    "    'Charged Off': 'Defaulted Loan',\n",
    "    'Does not meet the credit policy. Status:Charged Off': 'Defaulted Loan',\n",
    "    'Default': 'Defaulted Loan'\n",
    "}\n",
    "\n",
    "# Apply the grouping to the 'loan_status' column\n",
    "df['loan_status_grouped_kn'] = df['loan_status'].replace(loan_status_groupings)\n",
    "\n",
    "# Retain only rows with 'Paid Loan' or 'Defaulted Loan'\n",
    "df = df[df['loan_status_grouped_kn'].isin(['Paid Loan', 'Defaulted Loan'])]\n",
    "\n",
    "# Verify the groupings\n",
    "print(df['loan_status_grouped_kn'].value_counts())\n",
    "\n",
    "# Newly created columns\n",
    "new_columns = ['loan_status_grouped_kn']  # Replace with columns name\n",
    "\n",
    "# Add new columns to feature_list if they're not already in the list\n",
    "feature_list.extend([col for col in new_columns if col not in feature_list])\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block visualizes the distribution of loan amounts for each loan status category using Kernel Density Estimation (KDE) plots. It iterates over the unique values of loan_status_grouped_kn and creates a filled KDE plot for each category (Paid Loan or Defaulted Loan). The resulting graph allows for a comparison of loan amount distributions between different loan statuses, providing insights into any potential differences in loan amount trends across the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for status in df['loan_status_grouped_kn'].unique():\n",
    "    sns.kdeplot(df[df['loan_status_grouped_kn'] == status]['loan_amnt'], label=status, fill=True)\n",
    "plt.title('Distribution of Loan Amount by Loan Status')\n",
    "plt.xlabel('Loan Amount')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(title=\"Loan Status\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the loan_status against employment length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_length'].info()\n",
    "df['emp_length'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block processes the emp_length column, converting it to numeric values and handling missing data. It then creates a crosstab of emp_length_cleaned and loan_status_grouped_kn, calculates the percentage distribution of loan statuses for each employment length, and formats these percentages to two decimal places with a percentage sign. The resulting table (emp_length_percentage) shows the loan status distribution across different employment lengths, sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values in emp_length with None and convert to integer values\n",
    "df['emp_length_cleaned'] = df['emp_length'].replace({\n",
    "    '10+ years': 10,\n",
    "    '9 years': 9, '8 years': 8, '7 years': 7, '6 years': 6, '5 years': 5,\n",
    "    '4 years': 4, '3 years': 3, '2 years': 2, '1 year': 1, '< 1 year': 0,\n",
    "    'n/a': None  # Assuming 'n/a' represents missing values\n",
    "}).astype('Int64')  # Use 'Int64' for integer with support for NaN\n",
    "\n",
    "# Drop any NaN values in emp_length_cleaned if necessary\n",
    "df = df.dropna(subset=['emp_length_cleaned'])\n",
    "\n",
    "# Create a crosstab of loan_status_grouped_kn and emp_length_cleaned\n",
    "emp_length_counts = pd.crosstab(df['emp_length_cleaned'], df['loan_status_grouped_kn'])\n",
    "\n",
    "# Calculate the percentage for each loan status within each employment length year\n",
    "emp_length_percentage = emp_length_counts.div(emp_length_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# Sort the index of emp_length_percentage in descending order\n",
    "emp_length_percentage = emp_length_percentage.sort_index(ascending=False)\n",
    "\n",
    "# Format each column to two decimal places with a % sign using map\n",
    "for col in emp_length_percentage.columns:\n",
    "    emp_length_percentage[col] = emp_length_percentage[col].map(lambda x: f\"{x:.2f} %\")\n",
    "\n",
    "# Display the resulting table\n",
    "emp_length_percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've finished our EDA let's check some of the categorical points with our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newly created columns\n",
    "new_columns = ['emp_length_cleaned']  # Replace with columns name\n",
    "\n",
    "# Add new columns to feature_list if they're not already in the list\n",
    "feature_list.extend([col for col in new_columns if col not in feature_list])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block generates a cross-tabulation between loan_status_grouped_kn (loan status) and purpose. It uses pd.crosstab() to calculate the number of loans for each combination of loan status and loan purpose, resulting in a summary table (comparison_loan_status_purpose). This table provides insight into how different loan purposes relate to loan outcomes, helping to identify patterns between the purpose of the loan and its status (e.g., \"Paid Loan\" or \"Defaulted Loan\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of purpose and loan_status\n",
    "comparison_loan_status_purpose = pd.crosstab(df['loan_status_grouped_kn'], df['purpose'])\n",
    "\n",
    "# Display the result\n",
    "comparison_loan_status_purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block sorts loan purposes by total loan counts across all statuses in descending order. It then creates a stacked bar chart to visualize the distribution of loan statuses for each purpose (comparison_loan_status_purpose_sorted). Labels and a title are added for clarity, and x-axis labels are rotated for readability. The stacked chart helps highlight how each loan status (e.g., \"Paid Loan\" or \"Defaulted Loan\") contributes to the total loan count for each purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by total loan counts across all statuses for each purpose (descending order)\n",
    "sorted_data = comparison_loan_status_purpose.sum(axis=0).sort_values(ascending=False)\n",
    "sorted_columns = sorted_data.index\n",
    "comparison_loan_status_purpose_sorted = comparison_loan_status_purpose[sorted_columns]\n",
    "\n",
    "# Plot a stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot each loan status as a stacked segment\n",
    "comparison_loan_status_purpose_sorted.T.plot(kind='bar', stacked=True, ax=ax)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Purpose')\n",
    "ax.set_ylabel('Number of Loans')\n",
    "ax.set_title('Distribution of Loan Statuses by Loan Purpose')\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Loan Status')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block generates a cross-tabulation between loan_status_grouped_kn (loan status) and verification_status. Using pd.crosstab(), it calculates the count of loans for each combination of loan status and verification status, resulting in a summary table (comparison_loan_status_ver_status). This table helps in understanding the distribution of different loan statuses (e.g., \"Paid Loan\" or \"Defaulted Loan\") across various verification statuses, providing insights into how verification affects loan outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of purpose and loan_status\n",
    "comparison_loan_status_ver_status = pd.crosstab(df['loan_status_grouped_kn'], df['verification_status'])\n",
    "\n",
    "# Display the result\n",
    "comparison_loan_status_ver_status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block visualizes the distribution of loan statuses (loan_status_grouped_kn) across different verification statuses using a stacked bar chart. It first transposes the comparison_loan_status_ver_status DataFrame for easier plotting of the different loan statuses as stacked segments for each verification status. The chart is labeled with appropriate axis labels (Verification Status and Number of Loans) and a title (Distribution of Loan Statuses by Verification Status). The x-axis labels are rotated for better readability, and tight_layout() ensures that all elements fit properly within the figure. A legend is included to indicate the different loan statuses in the stacked bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Transpose the DataFrame for easier plotting and plot each loan status as a stacked segment\n",
    "comparison_loan_status_ver_status.T.plot(kind='bar', stacked=True, ax=ax)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Verification Status')\n",
    "ax.set_ylabel('Number of Loans')\n",
    "ax.set_title('Distribution of Loan Statuses by Verification Status')\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.legend(title='Loan Status')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block creates a cross-tabulation between loan_status_grouped_kn (loan status) and addr_state (state). It uses pd.crosstab() to count the number of loans for each combination of loan status and state, providing a summary table that shows how loan statuses (such as \"Paid Loan\" or \"Defaulted Loan\") are distributed across different states. The resulting DataFrame, comparison_loan_status_addr_state, helps in understanding patterns or variations in loan outcomes by geographic location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation of purpose and loan_status\n",
    "comparison_loan_status_addr_state = pd.crosstab(df['loan_status_grouped_kn'], df['addr_state'])\n",
    "\n",
    "# Display the result\n",
    "comparison_loan_status_addr_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block visualizes the number of defaulted loans by state. It first filters the data to only include loans with a Defaulted Loan status and sorts the states in descending order based on the number of defaulted loans. A bar plot is then generated to display this information, using salmon-colored bars for better visual appeal. The plot includes labels for the x-axis (State) and y-axis (Number of Defaulted Loans), as well as a title. The x-axis labels are rotated for readability, and tight_layout() is applied to ensure the plot elements fit well within the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to only include the 'Defaulted' loan status\n",
    "defaulted_by_state = comparison_loan_status_addr_state.loc['Defaulted Loan']\n",
    "\n",
    "# Sort the data by the number of defaulted loans in descending order\n",
    "defaulted_by_state_sorted = defaulted_by_state.sort_values(ascending=False)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "defaulted_by_state_sorted.plot(kind='bar', color='salmon', edgecolor='black')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel(\"State\")\n",
    "plt.ylabel(\"Number of Defaulted Loans\")\n",
    "plt.title(\"Number of Defaulted Loans by State (Sorted)\")\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Code Block Explanation: This block calculates and displays the percentage of loans that defaulted for each state. It first calculates the total number of loans for each state and then extracts the number of loans that defaulted. The percentage of defaulted loans is calculated by dividing the number of defaulted loans by the total number of loans for each state, multiplying by 100. Finally, a DataFrame is created to combine the total loans, defaulted loans, and percentage of defaulted loans for easy viewing, sorted by the percentage of defaulted loans in descending order to highlight states with the highest default rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of loans for each state\n",
    "total_loans_by_state = comparison_loan_status_addr_state.sum(axis=0)\n",
    "\n",
    "# Extract the number of defaulted loans for each state\n",
    "defaulted_loans_by_state = comparison_loan_status_addr_state.loc['Defaulted Loan']\n",
    "\n",
    "# Calculate the percentage of defaulted loans\n",
    "defaulted_percentage_by_state = (defaulted_loans_by_state / total_loans_by_state) * 100\n",
    "\n",
    "# Combine into a DataFrame for easy viewing\n",
    "defaulted_percentage_df = pd.DataFrame({\n",
    "    'Total Loans': total_loans_by_state,\n",
    "    'Defaulted Loans': defaulted_loans_by_state,\n",
    "    '% Defaulted': defaulted_percentage_by_state\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "defaulted_percentage_df = defaulted_percentage_df.sort_values(by='% Defaulted', ascending=False)\n",
    "defaulted_percentage_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split for Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This function splits the provided DataFrame into three lists containing Boolean, Numerical, and Categorical column names. It also handles data type conversions and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data split for Analysis\n",
    "def split_data_frame(features_list, df):\n",
    "    \"\"\"\n",
    "    Splits the provided DataFrame into three lists containing Boolean, Numerical, and Categorical column names.\n",
    "    Converts floats with trailing zeros into integers and replaces NaN values with 0 for integers, 0.00 for floats.\n",
    "\n",
    "    Parameters:\n",
    "    features_list (list): List of column names to be checked.\n",
    "    df (pd.DataFrame): The input DataFrame to split.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing three lists (boolean_cols, numerical_cols, categorical_cols).\n",
    "    \"\"\"\n",
    "    boolean_cols = []\n",
    "    numerical_cols = []\n",
    "    categorical_cols = []\n",
    "\n",
    "    # Define acceptable boolean values\n",
    "    acceptable_boolean_values = {0, 1, True, False, 0.0, 1.0}\n",
    "\n",
    "    for col in features_list:\n",
    "        # Treat each column explicitly as a Series\n",
    "        column_series = df[col]\n",
    "\n",
    "        # Handle cases where columns might be interpreted incorrectly\n",
    "        if pd.api.types.is_bool_dtype(column_series) or all(column_series.dropna().isin(acceptable_boolean_values)):\n",
    "            boolean_cols.append(col)\n",
    "        elif pd.api.types.is_numeric_dtype(column_series):\n",
    "            # Check for floats with trailing zeros\n",
    "            if column_series.dtype == 'float64':\n",
    "                # Check if all float values are equivalent to integers\n",
    "                if all(column_series.dropna() == column_series.dropna().astype(int)):\n",
    "                    df[col] = column_series.fillna(0).astype(int)  # Replace NaNs with 0 and convert to int\n",
    "                else:\n",
    "                    df[col] = column_series.fillna(0.00)  # Replace NaNs with 0.00 for floats\n",
    "                numerical_cols.append(col)\n",
    "            else:\n",
    "                df[col] = column_series.fillna(0)  # Replace NaNs with 0 for integers\n",
    "                numerical_cols.append(col)\n",
    "        else:\n",
    "            categorical_cols.append(col)\n",
    "    \n",
    "    # Print a summary of the count of columns in each list\n",
    "    print(f\"Summary of column counts:\")\n",
    "    print(f\"boolean_list contains {len(boolean_cols)} values\")\n",
    "    print(f\"numerical_list contains {len(numerical_cols)} values\")\n",
    "    print(f\"categorical_list contains {len(categorical_cols)} values\")\n",
    "    print(f\"The data frame we'll continue our analysis with contains (rows, cols) {df[feature_list].shape} rows and columns.\")\n",
    "\n",
    "    return boolean_cols, numerical_cols, categorical_cols\n",
    "\n",
    "# Split the data\n",
    "boolean_list, numerical_list, categorical_list = split_data_frame(feature_list, df)\n",
    "df_clean = df[feature_list].copy()\n",
    "df_clean.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: These lines display the first few entries of each type of column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[boolean_list].head()\n",
    "df_clean[numerical_list].head()\n",
    "df_clean[categorical_list].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This block identifies columns with missing values and sorts them in descending order by count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Missing Values\n",
    "nan_list = df_clean.isna().sum()\n",
    "if nan_list.sum() == 0:\n",
    "    print(\"No column has NaN values\")\n",
    "else:\n",
    "    print(\"Columns with NaN values (sorted high to low):\")\n",
    "    print(nan_list[nan_list > 0].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This block generates visualizations of missing data using custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_missing_value_analysis(df, feature_list, target_column='loan_status_grouped_kn'):\n",
    "    # Check if there are any missing values in the specified columns\n",
    "    missing_cols = [col for col in feature_list if df[col].isnull().any()]\n",
    "    if not missing_cols:\n",
    "        print(\"There are no null values to analyse. Implying that NaN values have been transformed (0, mean, median, mode, etc.).\")\n",
    "        return\n",
    "\n",
    "    # Initialize dictionaries to store results for plotting\n",
    "    missing_dict = {}\n",
    "    not_missing_dict = {}\n",
    "\n",
    "    # Function to collect percentages for missing and non-missing data\n",
    "    def missing_value_analysis(column):\n",
    "        missing = df[df[column].isnull()][target_column].value_counts(normalize=True) * 100\n",
    "        not_missing = df[df[column].notnull()][target_column].value_counts(normalize=True) * 100\n",
    "        missing_dict[column] = missing\n",
    "        not_missing_dict[column] = not_missing\n",
    "\n",
    "    # Apply the function for all columns in missing_cols\n",
    "    for col in missing_cols:\n",
    "        missing_value_analysis(col)\n",
    "\n",
    "    # Create DataFrames for heatmaps\n",
    "    missing_df = pd.DataFrame(missing_dict).fillna(0)  # Fill NaN with 0 for heatmap display\n",
    "    not_missing_df = pd.DataFrame(not_missing_dict).fillna(0)\n",
    "\n",
    "    # Plotting heatmaps one below the other\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12, 16), gridspec_kw={'height_ratios': [1, 1]})  # Adjust aspect ratio\n",
    "\n",
    "    # Heatmap for missing data\n",
    "    sns.heatmap(missing_df, annot=False, cmap=\"Blues\", ax=ax[0], cbar_kws={\"shrink\": .75})\n",
    "    ax[0].set_title('Percentage of Loan Status for Missing Data')\n",
    "    ax[0].tick_params(axis='x', rotation=90, labelsize=10)  # Rotate x-axis labels for readability\n",
    "    ax[0].tick_params(axis='y', labelsize=10)  # Adjust y-axis label size\n",
    "\n",
    "    # Heatmap for non-missing data\n",
    "    sns.heatmap(not_missing_df, annot=False, cmap=\"Greens\", ax=ax[1], cbar_kws={\"shrink\": .75})\n",
    "    ax[1].set_title('Percentage of Loan Status for Non-Missing Data')\n",
    "    ax[1].tick_params(axis='x', rotation=90, labelsize=10)  # Rotate x-axis labels for readability\n",
    "    ax[1].tick_params(axis='y', labelsize=10)  # Adjust y-axis label size\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot missing value analysis for categorical and numerical columns\n",
    "plot_missing_value_analysis(df_clean, categorical_list, target_column='loan_status_grouped_kn')\n",
    "plot_missing_value_analysis(df_clean, numerical_list, target_column='loan_status_grouped_kn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This block visualizes missing data using the missingno library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "def visualize_missing_data(df, feature_list):\n",
    "    \"\"\"\n",
    "    Visualize missing data in specified columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to analyze.\n",
    "    - feature_list (list): List of columns to check for missing values.\n",
    "\n",
    "    Returns:\n",
    "    - None. Displays visualizations of missing data if present.\n",
    "    \"\"\"\n",
    "    # Identify columns from feature_list with missing values\n",
    "    missing_values = df[feature_list].isnull().sum()\n",
    "    missing_cols = missing_values[missing_values > 0].index\n",
    "\n",
    "    # Check if there are any missing values to display\n",
    "    if not missing_cols.empty:\n",
    "        print(\"Categorical Data Missing Values\\n\")\n",
    "        \n",
    "        # Filter DataFrame to include only columns with missing values\n",
    "        missing_values_graph = df[missing_cols]\n",
    "        \n",
    "        # Visualize the missing data using the missingno library\n",
    "        msno.matrix(missing_values_graph)\n",
    "        msno.bar(missing_values_graph)\n",
    "        msno.heatmap(missing_values_graph)\n",
    "    else:\n",
    "        print(\"No columns with missing values found in the specified feature list.\")\n",
    "\n",
    "# Visualize missing data for categorical columns\n",
    "visualize_missing_data(df_clean, categorical_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This block generates binary indicators for missing values across columns, adding additional columns to flag where data was missing. These indicators can sometimes be useful as features, helping models understand data patterns related to missingness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_indicators(df, feature_list):\n",
    "    \"\"\"\n",
    "    Adds missing indicators for specified categorical columns in feature_list.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The original DataFrame.\n",
    "    - feature_list (list): List of column names to check for missing values and create indicators.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The original DataFrame with added missing indicators for specified columns.\n",
    "    \"\"\"\n",
    "    # Create an empty DataFrame to store missing indicators\n",
    "    missing_indicators = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Loop through specified columns in feature_list\n",
    "    for col in feature_list:\n",
    "        # Check if the column exists in df and is categorical\n",
    "        if col in df.columns and df[col].dtype == 'object':\n",
    "            # Create missing indicator for the categorical column\n",
    "            missing_indicators[f\"{col}_missing\"] = df[col].isna().astype(int)\n",
    "\n",
    "    # Concatenate the original DataFrame with the missing indicators\n",
    "    df_with_indicators = pd.concat([df, missing_indicators], axis=1)\n",
    "\n",
    "    return df_with_indicators\n",
    "\n",
    "# Add missing indicators\n",
    "df_with_missing_indicators = add_missing_indicators(df_clean, categorical_list)\n",
    "print(df_with_missing_indicators.shape)\n",
    "\n",
    "# Updating df_clean\n",
    "df_clean = df_with_missing_indicators.copy()\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This block preprocesses categorical variables by filling missing values with \"NaN\" and applying binary encoding. The target column (loan_status_grouped_kn) is excluded from encoding to avoid unintended transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import category_encoders as ce\n",
    "\n",
    "def preprocess_categorical_variables(df, categorical_columns, target_column='loan_status_grouped_kn'):\n",
    "    \"\"\"\n",
    "    Preprocess categorical variables by filling missing values and encoding them.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to preprocess.\n",
    "    - categorical_columns (list): List of categorical columns to process.\n",
    "    - target_column (str): The target column to exclude from encoding (default is 'loan_status_grouped_kn').\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Encoded DataFrame after preprocessing.\n",
    "    - tuple: Shape of the encoded DataFrame.\n",
    "    - list: Updated feature_list.\n",
    "    - dict: Mapping of original to newly created columns.\n",
    "    \"\"\"\n",
    "    # Initialize a mapping dictionary for original to newly created columns\n",
    "    column_mapping = {}\n",
    "\n",
    "    # Filter categorical columns, excluding the target column\n",
    "    categorical_columns = [col for col in categorical_columns if col != target_column]\n",
    "    \n",
    "    # Fill missing values with \"NaN\" in specified categorical columns\n",
    "    with tqdm(total=len(categorical_columns), desc=\"Filling missing values\", leave=True) as pbar_fill:\n",
    "        for col in categorical_columns:\n",
    "            df[col] = df[col].fillna(\"NaN\")\n",
    "            pbar_fill.update(1)  # Update the progress bar for each column\n",
    "\n",
    "    # Apply Binary Encoding to the specified categorical columns\n",
    "    with tqdm(total=1, desc=\"Applying Binary Encoding\", leave=True) as pbar_encode:\n",
    "        binary_encoder = ce.BinaryEncoder(cols=categorical_columns, drop_invariant=True)\n",
    "        X_encoded = binary_encoder.fit_transform(df.drop(columns=[target_column]))\n",
    "        pbar_encode.update(1)  # Complete progress for encoding\n",
    "\n",
    "    # Update feature_list\n",
    "    with tqdm(total=1, desc=\"Updating feature_list\", leave=True) as pbar_feature_list:\n",
    "        global feature_list  # Assuming feature_list is defined globally\n",
    "        new_columns = X_encoded.columns.difference(df.columns).tolist()  # Get newly created columns\n",
    "        feature_list = [col for col in feature_list if col not in categorical_columns] + new_columns\n",
    "        pbar_feature_list.update(1)  # Complete progress for feature list update\n",
    "\n",
    "    # Create mapping for future reference\n",
    "    with tqdm(total=len(categorical_columns), desc=\"Creating column mapping\", leave=True) as pbar_mapping:\n",
    "        for col in categorical_columns:\n",
    "            new_col_names = [nc for nc in X_encoded.columns if nc.startswith(f\"{col}_\")]\n",
    "            column_mapping[col] = new_col_names\n",
    "            pbar_mapping.update(1)  # Update the progress bar for each mapping entry\n",
    "\n",
    "    print(f\"DataFrame shape after categorical preprocessing: {X_encoded.shape}\")\n",
    "    \n",
    "    return X_encoded, X_encoded.shape, feature_list, column_mapping\n",
    "\n",
    "# Preprocess categorical variables\n",
    "X_encoded, encoded_shape, updated_feature_list, column_mapping = preprocess_categorical_variables(df_clean, categorical_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This code fills any remaining missing values in numerical columns with zero, ensuring there are no NaN values in the dataset, which might disrupt model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_numerical_variables(df, numerical_columns):\n",
    "    \"\"\"\n",
    "    Preprocess numerical variables by filling missing values with 0.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to preprocess.\n",
    "    - numerical_columns (list): List of numerical columns to process.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame after preprocessing numerical variables.\n",
    "    - tuple: Shape of the processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Filter to include only numerical columns in the provided list\n",
    "    numerical_columns = df.select_dtypes(include=['number']).columns.intersection(numerical_columns)\n",
    "    \n",
    "    # Fill missing values with 0 in the specified numerical columns\n",
    "    for col in tqdm(numerical_columns, desc=\"Imputing missing numerical values\"):\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    print(f\"DataFrame shape after numerical preprocessing: {df.shape}\")\n",
    "    \n",
    "    return df, df.shape\n",
    "\n",
    "# Preprocess numerical variables\n",
    "df_processed, processed_shape = preprocess_numerical_variables(X_encoded, numerical_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: Assigning df_processed back to X_encoded to maintain consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This function checks the DataFrame for infinite values, which can disrupt calculations and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Infinite Values\n",
    "def check_infinity(df):\n",
    "    try:\n",
    "        # Apply np.isinf only to numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number])\n",
    "        infinite_mask = np.isinf(numeric_cols)\n",
    "        infinite_list = infinite_mask.sum()\n",
    "        if infinite_list.sum() == 0:\n",
    "            print(\"No column has infinite values\")\n",
    "        else:\n",
    "            print(\"Columns with infinite values:\")\n",
    "            print(infinite_list[infinite_list > 0].sort_values(ascending=False))\n",
    "    except Exception as e:\n",
    "        # Identify the columns that may be causing the error\n",
    "        problematic_cols = []\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                np.isinf(df[col])\n",
    "            except TypeError:\n",
    "                problematic_cols.append(col)\n",
    "        \n",
    "        print(f\"An error occurred while checking for infinite values: {e}\")\n",
    "        if problematic_cols:\n",
    "            print(f\"The following columns may be causing the issue due to incompatible types: {problematic_cols}\")\n",
    "\n",
    "check_infinity(X_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This block calculates the Variance Inflation Factor (VIF) for each feature to assess multicollinearity. High VIF values indicate high correlation among predictors, which can affect model stability and interpretability. This information guides the removal of redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.head().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with 'Int64' dtype\n",
    "int64_cols = X_encoded.select_dtypes(include='Int64').columns\n",
    "print(\"Columns with Int64 dtype:\", int64_cols)\n",
    "\n",
    "# Convert 'Int64' columns to 'float64'\n",
    "X_encoded[int64_cols] = X_encoded[int64_cols].astype('float64')\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "X_numeric = X_encoded.select_dtypes(include=[np.number])\n",
    "\n",
    "# Handle missing and infinite values\n",
    "if X_numeric.isnull().values.any():\n",
    "    print(\"NaN values detected. Filling NaNs with zeros.\")\n",
    "    X_numeric = X_numeric.fillna(0)\n",
    "if np.isinf(X_numeric.values).any():\n",
    "    print(\"Infinite values detected. Replacing infinite values with zeros.\")\n",
    "    X_numeric = X_numeric.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Function to calculate VIF with Progress Bar\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    \n",
    "    # Calculate VIF\n",
    "    vif_data[\"VIF\"] = [\n",
    "        variance_inflation_factor(X.values, i) for i in tqdm(range(X.shape[1]), desc=\"Calculating VIF\")\n",
    "    ]\n",
    "    return vif_data\n",
    "\n",
    "# Now use the function\n",
    "vif_data_original = calculate_vif(X_numeric)\n",
    "\n",
    "# Remove features with high VIF\n",
    "high_vif_features = vif_data_original[vif_data_original[\"VIF\"] > 5][\"feature\"].tolist()\n",
    "X_encoded = X_encoded.drop(columns=high_vif_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: Now, define X and y Variables and ensure they are aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that X_encoded and df_clean are aligned\n",
    "assert len(X_encoded) == len(df_clean), \"Mismatch between X_encoded and df_clean, ensure they are aligned before splitting.\"\n",
    "\n",
    "# Define X and y Variables\n",
    "X = X_encoded\n",
    "y = df_clean['loan_status_grouped_kn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: Split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: Encode the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Target Variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: Perform Recursive Feature Elimination (RFE) using Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Standardize the features to aid in convergence\n",
    "print(\"Step 1: Standardizing features to improve model convergence...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "print(\"Features standardized successfully.\\n\")\n",
    "\n",
    "# Step 2: Setting up Logistic Regression and RFE\n",
    "print(\"Step 2: Setting up Logistic Regression model with RFE...\")\n",
    "model = LogisticRegression(max_iter=5000, random_state=42)  # Reduced max_iter to 5000 for efficiency\n",
    "n_features_to_select = 48  # Adjust based on your requirement or try fewer features for faster performance\n",
    "rfe = RFE(estimator=model, n_features_to_select=n_features_to_select)\n",
    "print(f\"Configured RFE to select the top {n_features_to_select} features.\\n\")\n",
    "\n",
    "# Step 3: Running RFE with a progress bar\n",
    "print(\"Step 3: Running Recursive Feature Elimination (RFE)...\\n\")\n",
    "for i in tqdm(range(1), desc=\"RFE Progress\"):\n",
    "    rfe.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Step 4: Extracting selected features\n",
    "selected_features = X_train.columns[rfe.support_]\n",
    "print(\"\\nStep 4: Displaying selected features.\")\n",
    "print(\"Features selected by RFE:\\n\", selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: Update X_train and X_test to include only selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update X_train and X_test with selected features\n",
    "X_train = X_train[selected_features]\n",
    "X_test = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: Models we'll run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define a dictionary of models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=5000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(probability=True, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(silent=True, random_state=42)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: Evaluating models by training and calculating performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in tqdm(models.items(), desc=\"Evaluating models\"):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train_encoded)\n",
    "    trained_models[name] = model\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "    precision = precision_score(y_test_encoded, y_pred, average='binary', pos_label=label_encoder.transform(['Defaulted Loan'])[0])\n",
    "    recall = recall_score(y_test_encoded, y_pred, average='binary', pos_label=label_encoder.transform(['Defaulted Loan'])[0])\n",
    "    f1 = f1_score(y_test_encoded, y_pred, average='binary', pos_label=label_encoder.transform(['Defaulted Loan'])[0])\n",
    "    roc_auc = roc_auc_score(y_test_encoded, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X, label_encoder.transform(y), cv=5)\n",
    "    cv_mean_accuracy = cv_scores.mean()\n",
    "    \n",
    "    # Append results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Cross-Validation Mean Accuracy': cv_mean_accuracy\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This block converts the list of evaluation results into a DataFrame for a clear summary and displays it. It optionally saves the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results list to a DataFrame for better visualization and analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nEvaluation Results Summary:\")\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, save results to a CSV file\n",
    "results_df.to_csv(\"model_evaluation_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This block generates a line plot to compare different performance metrics across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `results_df` is the DataFrame created by the model evaluation function\n",
    "# Display results as a table for easy comparison\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "display(results_df)  # If running in a Jupyter notebook, this will display a nice formatted table\n",
    "\n",
    "# Visualize Performance Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Cross-Validation Mean Accuracy']\n",
    "\n",
    "# Plot each metric for all models\n",
    "for metric in metrics:\n",
    "    plt.plot(results_df['Model'], results_df[metric], marker='o', label=metric)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: This block saves each model to disk, enabling easy reuse without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save All Models for Future Use\n",
    "for name, model in trained_models.items():\n",
    "    joblib.dump(model, f\"{name}_final_model.pkl\")\n",
    "print(\"\\nAll models have been saved for future use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code Block Explanation: SHAP Explanations for Model Interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SHAP Explanations for Model Interpretability ===\n",
    "import shap\n",
    "\n",
    "# Initialize SHAP Explainer for each model\n",
    "def explain_model_with_shap(model, X_sample, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Generates SHAP explanations for the given model and dataset sample.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    model : estimator object\n",
    "        The trained model to explain.\n",
    "    X_sample : DataFrame\n",
    "        A sample of the dataset to generate SHAP values for.\n",
    "    model_name : str\n",
    "        Name of the model for display in plots.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating SHAP explanations for {model_name}...\")\n",
    "    \n",
    "    # Use appropriate explainer based on model type\n",
    "    if isinstance(model, (RandomForestClassifier, GradientBoostingClassifier, XGBClassifier, LGBMClassifier, CatBoostClassifier)):\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "    else:\n",
    "        explainer = shap.KernelExplainer(model.predict, X_sample)\n",
    "\n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "    # Plot feature importance summary\n",
    "    shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", show=True)\n",
    "    \n",
    "    # Detailed summary plot with individual SHAP values per feature and instance\n",
    "    shap.summary_plot(shap_values, X_sample, show=True)\n",
    "    \n",
    "    # Note: For large datasets, force plots and dependence plots may be resource-intensive\n",
    "\n",
    "# Choose a subset of data to explain (e.g., a random sample of 100 rows)\n",
    "X_sample = X_test.sample(100, random_state=42)\n",
    "\n",
    "# Loop to explain all models in the trained_models dictionary\n",
    "for name, model in trained_models.items():\n",
    "    explain_model_with_shap(model, X_sample, model_name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
