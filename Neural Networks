## ðŸ§  Neural Network Challenger Model: Simple Architecture

For this neural network experiment, I opted for a simpler model architecture to evaluate whether a more streamlined network could achieve comparable performance. This model acts as a challenger by testing if fewer layers and neurons can still capture the predictive patterns in our data with high accuracy.

### Model Architecture

- **Layer 1**: Dense layer with 16 neurons and ReLU activation. This layer takes the scaled input features and applies non-linear transformations to capture underlying patterns.
- **Layer 2**: Dense layer with 8 neurons and ReLU activation, refining the representations learned in the first layer.
- **Output Layer**: Dense layer with 1 neuron and sigmoid activation to output a probability score, indicating the likelihood of default.

### Training and Validation Results

- **Training Accuracy**: The model achieved a final training accuracy of **98.56%** with a low loss of **0.0538**.
- **Validation Accuracy**: The validation accuracy reached **98.51%** by the end of training, with a similarly low validation loss of **0.0566**.

### Training Curve Analysis

The training and validation accuracy curves indicate strong model performance with minimal signs of overfitting. Both curves converge closely, suggesting that the model generalises well to unseen data. The validation accuracy improves rapidly in the first few epochs, plateauing around epoch 4 as it nears maximum performance.

This experiment shows that even a relatively simple neural network architecture can achieve impressive predictive accuracy on this dataset. However, further testing and comparison with more complex models are necessary to confirm if this simpler model maintains robust performance across different data splits.

